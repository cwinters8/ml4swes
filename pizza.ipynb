{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59506e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and define required functions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sea\n",
    "\n",
    "def predict(X, w):\n",
    "  return X * w\n",
    "\n",
    "def loss(X, Y, w):\n",
    "  return np.average((predict(X, w) - Y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4babf168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from pizza.txt\n",
    "X, Y = np.loadtxt(\"pizza.txt\", skiprows=1, unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88d2acc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.9586546146667\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Guess a weight and calculate the average loss\n",
    "weight = 2.1211\n",
    "error = loss(X, Y, weight)\n",
    "print(error)\n",
    "\n",
    "# Plot the data\n",
    "sea.set_theme()\n",
    "plt.axis([0, 50, 0, 50])\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.xlabel(\"Reservations\", fontsize=14)\n",
    "plt.ylabel(\"Pizzas\", fontsize=14)\n",
    "plt.plot(X, Y, \"ro\")\n",
    "plt.plot(X, predict(X, weight))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "63d1bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "# calculate the correct (or closest possible?) weight from the values in X and Y\n",
    "\n",
    "# iterations = max number of iterations to determine the weight\n",
    "# lr = learning rate (step size)\n",
    "def train(X, Y, iterations, lr) -> float:\n",
    "  # starting weight\n",
    "  w = 0\n",
    "  for i in range(iterations):\n",
    "    current_loss = loss(X, Y, w)\n",
    "    print(\"Iteration %4d => Weight: %.6f => Loss: %.6f\" % (i, w, current_loss))\n",
    "\n",
    "    # does adding a step to w reduce the loss?\n",
    "    if loss(X, Y, w + lr) < current_loss:\n",
    "      w += lr\n",
    "    # does subtracting a step from w reduce the loss?\n",
    "    elif loss(X, Y, w - lr) < current_loss:\n",
    "      w -= lr\n",
    "    # if a step in neither direction doesn't improve the loss, return the closest weight we can\n",
    "    else:\n",
    "      return w\n",
    "  \n",
    "  raise Exception(\"Couldn't converge within %4d iterations\" % iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7d463944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 => Weight: 0.000000 => Loss: 812.866667\n",
      "Iteration    1 => Weight: 0.001000 => Loss: 812.060085\n",
      "Iteration    2 => Weight: 0.002000 => Loss: 811.253942\n",
      "Iteration    3 => Weight: 0.003000 => Loss: 810.448236\n",
      "Iteration    4 => Weight: 0.004000 => Loss: 809.642967\n",
      "Iteration    5 => Weight: 0.005000 => Loss: 808.838137\n",
      "Iteration    6 => Weight: 0.006000 => Loss: 808.033743\n",
      "Iteration    7 => Weight: 0.007000 => Loss: 807.229788\n",
      "Iteration    8 => Weight: 0.008000 => Loss: 806.426270\n",
      "Iteration    9 => Weight: 0.009000 => Loss: 805.623189\n",
      "Iteration   10 => Weight: 0.010000 => Loss: 804.820547\n",
      "Iteration   11 => Weight: 0.011000 => Loss: 804.018341\n",
      "Iteration   12 => Weight: 0.012000 => Loss: 803.216574\n",
      "Iteration   13 => Weight: 0.013000 => Loss: 802.415244\n",
      "Iteration   14 => Weight: 0.014000 => Loss: 801.614351\n",
      "Iteration   15 => Weight: 0.015000 => Loss: 800.813897\n",
      "Iteration   16 => Weight: 0.016000 => Loss: 800.013879\n",
      "Iteration   17 => Weight: 0.017000 => Loss: 799.214300\n",
      "Iteration   18 => Weight: 0.018000 => Loss: 798.415158\n",
      "Iteration   19 => Weight: 0.019000 => Loss: 797.616453\n",
      "Iteration   20 => Weight: 0.020000 => Loss: 796.818187\n",
      "Iteration   21 => Weight: 0.021000 => Loss: 796.020357\n",
      "Iteration   22 => Weight: 0.022000 => Loss: 795.222966\n",
      "Iteration   23 => Weight: 0.023000 => Loss: 794.426012\n",
      "Iteration   24 => Weight: 0.024000 => Loss: 793.629495\n",
      "Iteration   25 => Weight: 0.025000 => Loss: 792.833417\n",
      "Iteration   26 => Weight: 0.026000 => Loss: 792.037775\n",
      "Iteration   27 => Weight: 0.027000 => Loss: 791.242572\n",
      "Iteration   28 => Weight: 0.028000 => Loss: 790.447806\n",
      "Iteration   29 => Weight: 0.029000 => Loss: 789.653477\n",
      "Iteration   30 => Weight: 0.030000 => Loss: 788.859587\n",
      "Iteration   31 => Weight: 0.031000 => Loss: 788.066133\n",
      "Iteration   32 => Weight: 0.032000 => Loss: 787.273118\n",
      "Iteration   33 => Weight: 0.033000 => Loss: 786.480540\n",
      "Iteration   34 => Weight: 0.034000 => Loss: 785.688399\n",
      "Iteration   35 => Weight: 0.035000 => Loss: 784.896697\n",
      "Iteration   36 => Weight: 0.036000 => Loss: 784.105431\n",
      "Iteration   37 => Weight: 0.037000 => Loss: 783.314604\n",
      "Iteration   38 => Weight: 0.038000 => Loss: 782.524214\n",
      "Iteration   39 => Weight: 0.039000 => Loss: 781.734261\n",
      "Iteration   40 => Weight: 0.040000 => Loss: 780.944747\n",
      "Iteration   41 => Weight: 0.041000 => Loss: 780.155669\n",
      "Iteration   42 => Weight: 0.042000 => Loss: 779.367030\n",
      "Iteration   43 => Weight: 0.043000 => Loss: 778.578828\n",
      "Iteration   44 => Weight: 0.044000 => Loss: 777.791063\n",
      "Iteration   45 => Weight: 0.045000 => Loss: 777.003737\n",
      "Iteration   46 => Weight: 0.046000 => Loss: 776.216847\n",
      "Iteration   47 => Weight: 0.047000 => Loss: 775.430396\n",
      "Iteration   48 => Weight: 0.048000 => Loss: 774.644382\n",
      "Iteration   49 => Weight: 0.049000 => Loss: 773.858805\n",
      "Iteration   50 => Weight: 0.050000 => Loss: 773.073667\n",
      "Iteration   51 => Weight: 0.051000 => Loss: 772.288965\n",
      "Iteration   52 => Weight: 0.052000 => Loss: 771.504702\n",
      "Iteration   53 => Weight: 0.053000 => Loss: 770.720876\n",
      "Iteration   54 => Weight: 0.054000 => Loss: 769.937487\n",
      "Iteration   55 => Weight: 0.055000 => Loss: 769.154537\n",
      "Iteration   56 => Weight: 0.056000 => Loss: 768.372023\n",
      "Iteration   57 => Weight: 0.057000 => Loss: 767.589948\n",
      "Iteration   58 => Weight: 0.058000 => Loss: 766.808310\n",
      "Iteration   59 => Weight: 0.059000 => Loss: 766.027109\n",
      "Iteration   60 => Weight: 0.060000 => Loss: 765.246347\n",
      "Iteration   61 => Weight: 0.061000 => Loss: 764.466021\n",
      "Iteration   62 => Weight: 0.062000 => Loss: 763.686134\n",
      "Iteration   63 => Weight: 0.063000 => Loss: 762.906684\n",
      "Iteration   64 => Weight: 0.064000 => Loss: 762.127671\n",
      "Iteration   65 => Weight: 0.065000 => Loss: 761.349097\n",
      "Iteration   66 => Weight: 0.066000 => Loss: 760.570959\n",
      "Iteration   67 => Weight: 0.067000 => Loss: 759.793260\n",
      "Iteration   68 => Weight: 0.068000 => Loss: 759.015998\n",
      "Iteration   69 => Weight: 0.069000 => Loss: 758.239173\n",
      "Iteration   70 => Weight: 0.070000 => Loss: 757.462787\n",
      "Iteration   71 => Weight: 0.071000 => Loss: 756.686837\n",
      "Iteration   72 => Weight: 0.072000 => Loss: 755.911326\n",
      "Iteration   73 => Weight: 0.073000 => Loss: 755.136252\n",
      "Iteration   74 => Weight: 0.074000 => Loss: 754.361615\n",
      "Iteration   75 => Weight: 0.075000 => Loss: 753.587417\n",
      "Iteration   76 => Weight: 0.076000 => Loss: 752.813655\n",
      "Iteration   77 => Weight: 0.077000 => Loss: 752.040332\n",
      "Iteration   78 => Weight: 0.078000 => Loss: 751.267446\n",
      "Iteration   79 => Weight: 0.079000 => Loss: 750.494997\n",
      "Iteration   80 => Weight: 0.080000 => Loss: 749.722987\n",
      "Iteration   81 => Weight: 0.081000 => Loss: 748.951413\n",
      "Iteration   82 => Weight: 0.082000 => Loss: 748.180278\n",
      "Iteration   83 => Weight: 0.083000 => Loss: 747.409580\n",
      "Iteration   84 => Weight: 0.084000 => Loss: 746.639319\n",
      "Iteration   85 => Weight: 0.085000 => Loss: 745.869497\n",
      "Iteration   86 => Weight: 0.086000 => Loss: 745.100111\n",
      "Iteration   87 => Weight: 0.087000 => Loss: 744.331164\n",
      "Iteration   88 => Weight: 0.088000 => Loss: 743.562654\n",
      "Iteration   89 => Weight: 0.089000 => Loss: 742.794581\n",
      "Iteration   90 => Weight: 0.090000 => Loss: 742.026947\n",
      "Iteration   91 => Weight: 0.091000 => Loss: 741.259749\n",
      "Iteration   92 => Weight: 0.092000 => Loss: 740.492990\n",
      "Iteration   93 => Weight: 0.093000 => Loss: 739.726668\n",
      "Iteration   94 => Weight: 0.094000 => Loss: 738.960783\n",
      "Iteration   95 => Weight: 0.095000 => Loss: 738.195337\n",
      "Iteration   96 => Weight: 0.096000 => Loss: 737.430327\n",
      "Iteration   97 => Weight: 0.097000 => Loss: 736.665756\n",
      "Iteration   98 => Weight: 0.098000 => Loss: 735.901622\n",
      "Iteration   99 => Weight: 0.099000 => Loss: 735.137925\n",
      "Iteration  100 => Weight: 0.100000 => Loss: 734.374667\n",
      "Iteration  101 => Weight: 0.101000 => Loss: 733.611845\n",
      "Iteration  102 => Weight: 0.102000 => Loss: 732.849462\n",
      "Iteration  103 => Weight: 0.103000 => Loss: 732.087516\n",
      "Iteration  104 => Weight: 0.104000 => Loss: 731.326007\n",
      "Iteration  105 => Weight: 0.105000 => Loss: 730.564937\n",
      "Iteration  106 => Weight: 0.106000 => Loss: 729.804303\n",
      "Iteration  107 => Weight: 0.107000 => Loss: 729.044108\n",
      "Iteration  108 => Weight: 0.108000 => Loss: 728.284350\n",
      "Iteration  109 => Weight: 0.109000 => Loss: 727.525029\n",
      "Iteration  110 => Weight: 0.110000 => Loss: 726.766147\n",
      "Iteration  111 => Weight: 0.111000 => Loss: 726.007701\n",
      "Iteration  112 => Weight: 0.112000 => Loss: 725.249694\n",
      "Iteration  113 => Weight: 0.113000 => Loss: 724.492124\n",
      "Iteration  114 => Weight: 0.114000 => Loss: 723.734991\n",
      "Iteration  115 => Weight: 0.115000 => Loss: 722.978297\n",
      "Iteration  116 => Weight: 0.116000 => Loss: 722.222039\n",
      "Iteration  117 => Weight: 0.117000 => Loss: 721.466220\n",
      "Iteration  118 => Weight: 0.118000 => Loss: 720.710838\n",
      "Iteration  119 => Weight: 0.119000 => Loss: 719.955893\n",
      "Iteration  120 => Weight: 0.120000 => Loss: 719.201387\n",
      "Iteration  121 => Weight: 0.121000 => Loss: 718.447317\n",
      "Iteration  122 => Weight: 0.122000 => Loss: 717.693686\n",
      "Iteration  123 => Weight: 0.123000 => Loss: 716.940492\n",
      "Iteration  124 => Weight: 0.124000 => Loss: 716.187735\n",
      "Iteration  125 => Weight: 0.125000 => Loss: 715.435417\n",
      "Iteration  126 => Weight: 0.126000 => Loss: 714.683535\n",
      "Iteration  127 => Weight: 0.127000 => Loss: 713.932092\n",
      "Iteration  128 => Weight: 0.128000 => Loss: 713.181086\n",
      "Iteration  129 => Weight: 0.129000 => Loss: 712.430517\n",
      "Iteration  130 => Weight: 0.130000 => Loss: 711.680387\n",
      "Iteration  131 => Weight: 0.131000 => Loss: 710.930693\n",
      "Iteration  132 => Weight: 0.132000 => Loss: 710.181438\n",
      "Iteration  133 => Weight: 0.133000 => Loss: 709.432620\n",
      "Iteration  134 => Weight: 0.134000 => Loss: 708.684239\n",
      "Iteration  135 => Weight: 0.135000 => Loss: 707.936297\n",
      "Iteration  136 => Weight: 0.136000 => Loss: 707.188791\n",
      "Iteration  137 => Weight: 0.137000 => Loss: 706.441724\n",
      "Iteration  138 => Weight: 0.138000 => Loss: 705.695094\n",
      "Iteration  139 => Weight: 0.139000 => Loss: 704.948901\n",
      "Iteration  140 => Weight: 0.140000 => Loss: 704.203147\n",
      "Iteration  141 => Weight: 0.141000 => Loss: 703.457829\n",
      "Iteration  142 => Weight: 0.142000 => Loss: 702.712950\n",
      "Iteration  143 => Weight: 0.143000 => Loss: 701.968508\n",
      "Iteration  144 => Weight: 0.144000 => Loss: 701.224503\n",
      "Iteration  145 => Weight: 0.145000 => Loss: 700.480937\n",
      "Iteration  146 => Weight: 0.146000 => Loss: 699.737807\n",
      "Iteration  147 => Weight: 0.147000 => Loss: 698.995116\n",
      "Iteration  148 => Weight: 0.148000 => Loss: 698.252862\n",
      "Iteration  149 => Weight: 0.149000 => Loss: 697.511045\n",
      "Iteration  150 => Weight: 0.150000 => Loss: 696.769667\n",
      "Iteration  151 => Weight: 0.151000 => Loss: 696.028725\n",
      "Iteration  152 => Weight: 0.152000 => Loss: 695.288222\n",
      "Iteration  153 => Weight: 0.153000 => Loss: 694.548156\n",
      "Iteration  154 => Weight: 0.154000 => Loss: 693.808527\n",
      "Iteration  155 => Weight: 0.155000 => Loss: 693.069337\n",
      "Iteration  156 => Weight: 0.156000 => Loss: 692.330583\n",
      "Iteration  157 => Weight: 0.157000 => Loss: 691.592268\n",
      "Iteration  158 => Weight: 0.158000 => Loss: 690.854390\n",
      "Iteration  159 => Weight: 0.159000 => Loss: 690.116949\n",
      "Iteration  160 => Weight: 0.160000 => Loss: 689.379947\n",
      "Iteration  161 => Weight: 0.161000 => Loss: 688.643381\n",
      "Iteration  162 => Weight: 0.162000 => Loss: 687.907254\n",
      "Iteration  163 => Weight: 0.163000 => Loss: 687.171564\n",
      "Iteration  164 => Weight: 0.164000 => Loss: 686.436311\n",
      "Iteration  165 => Weight: 0.165000 => Loss: 685.701497\n",
      "Iteration  166 => Weight: 0.166000 => Loss: 684.967119\n",
      "Iteration  167 => Weight: 0.167000 => Loss: 684.233180\n",
      "Iteration  168 => Weight: 0.168000 => Loss: 683.499678\n",
      "Iteration  169 => Weight: 0.169000 => Loss: 682.766613\n",
      "Iteration  170 => Weight: 0.170000 => Loss: 682.033987\n",
      "Iteration  171 => Weight: 0.171000 => Loss: 681.301797\n",
      "Iteration  172 => Weight: 0.172000 => Loss: 680.570046\n",
      "Iteration  173 => Weight: 0.173000 => Loss: 679.838732\n",
      "Iteration  174 => Weight: 0.174000 => Loss: 679.107855\n",
      "Iteration  175 => Weight: 0.175000 => Loss: 678.377417\n",
      "Iteration  176 => Weight: 0.176000 => Loss: 677.647415\n",
      "Iteration  177 => Weight: 0.177000 => Loss: 676.917852\n",
      "Iteration  178 => Weight: 0.178000 => Loss: 676.188726\n",
      "Iteration  179 => Weight: 0.179000 => Loss: 675.460037\n",
      "Iteration  180 => Weight: 0.180000 => Loss: 674.731787\n",
      "Iteration  181 => Weight: 0.181000 => Loss: 674.003973\n",
      "Iteration  182 => Weight: 0.182000 => Loss: 673.276598\n",
      "Iteration  183 => Weight: 0.183000 => Loss: 672.549660\n",
      "Iteration  184 => Weight: 0.184000 => Loss: 671.823159\n",
      "Iteration  185 => Weight: 0.185000 => Loss: 671.097097\n",
      "Iteration  186 => Weight: 0.186000 => Loss: 670.371471\n",
      "Iteration  187 => Weight: 0.187000 => Loss: 669.646284\n",
      "Iteration  188 => Weight: 0.188000 => Loss: 668.921534\n",
      "Iteration  189 => Weight: 0.189000 => Loss: 668.197221\n",
      "Iteration  190 => Weight: 0.190000 => Loss: 667.473347\n",
      "Iteration  191 => Weight: 0.191000 => Loss: 666.749909\n",
      "Iteration  192 => Weight: 0.192000 => Loss: 666.026910\n",
      "Iteration  193 => Weight: 0.193000 => Loss: 665.304348\n",
      "Iteration  194 => Weight: 0.194000 => Loss: 664.582223\n",
      "Iteration  195 => Weight: 0.195000 => Loss: 663.860537\n",
      "Iteration  196 => Weight: 0.196000 => Loss: 663.139287\n",
      "Iteration  197 => Weight: 0.197000 => Loss: 662.418476\n",
      "Iteration  198 => Weight: 0.198000 => Loss: 661.698102\n",
      "Iteration  199 => Weight: 0.199000 => Loss: 660.978165\n",
      "Iteration  200 => Weight: 0.200000 => Loss: 660.258667\n",
      "Iteration  201 => Weight: 0.201000 => Loss: 659.539605\n",
      "Iteration  202 => Weight: 0.202000 => Loss: 658.820982\n",
      "Iteration  203 => Weight: 0.203000 => Loss: 658.102796\n",
      "Iteration  204 => Weight: 0.204000 => Loss: 657.385047\n",
      "Iteration  205 => Weight: 0.205000 => Loss: 656.667737\n",
      "Iteration  206 => Weight: 0.206000 => Loss: 655.950863\n",
      "Iteration  207 => Weight: 0.207000 => Loss: 655.234428\n",
      "Iteration  208 => Weight: 0.208000 => Loss: 654.518430\n",
      "Iteration  209 => Weight: 0.209000 => Loss: 653.802869\n",
      "Iteration  210 => Weight: 0.210000 => Loss: 653.087747\n",
      "Iteration  211 => Weight: 0.211000 => Loss: 652.373061\n",
      "Iteration  212 => Weight: 0.212000 => Loss: 651.658814\n",
      "Iteration  213 => Weight: 0.213000 => Loss: 650.945004\n",
      "Iteration  214 => Weight: 0.214000 => Loss: 650.231631\n",
      "Iteration  215 => Weight: 0.215000 => Loss: 649.518697\n",
      "Iteration  216 => Weight: 0.216000 => Loss: 648.806199\n",
      "Iteration  217 => Weight: 0.217000 => Loss: 648.094140\n",
      "Iteration  218 => Weight: 0.218000 => Loss: 647.382518\n",
      "Iteration  219 => Weight: 0.219000 => Loss: 646.671333\n",
      "Iteration  220 => Weight: 0.220000 => Loss: 645.960587\n",
      "Iteration  221 => Weight: 0.221000 => Loss: 645.250277\n",
      "Iteration  222 => Weight: 0.222000 => Loss: 644.540406\n",
      "Iteration  223 => Weight: 0.223000 => Loss: 643.830972\n",
      "Iteration  224 => Weight: 0.224000 => Loss: 643.121975\n",
      "Iteration  225 => Weight: 0.225000 => Loss: 642.413417\n",
      "Iteration  226 => Weight: 0.226000 => Loss: 641.705295\n",
      "Iteration  227 => Weight: 0.227000 => Loss: 640.997612\n",
      "Iteration  228 => Weight: 0.228000 => Loss: 640.290366\n",
      "Iteration  229 => Weight: 0.229000 => Loss: 639.583557\n",
      "Iteration  230 => Weight: 0.230000 => Loss: 638.877187\n",
      "Iteration  231 => Weight: 0.231000 => Loss: 638.171253\n",
      "Iteration  232 => Weight: 0.232000 => Loss: 637.465758\n",
      "Iteration  233 => Weight: 0.233000 => Loss: 636.760700\n",
      "Iteration  234 => Weight: 0.234000 => Loss: 636.056079\n",
      "Iteration  235 => Weight: 0.235000 => Loss: 635.351897\n",
      "Iteration  236 => Weight: 0.236000 => Loss: 634.648151\n",
      "Iteration  237 => Weight: 0.237000 => Loss: 633.944844\n",
      "Iteration  238 => Weight: 0.238000 => Loss: 633.241974\n",
      "Iteration  239 => Weight: 0.239000 => Loss: 632.539541\n",
      "Iteration  240 => Weight: 0.240000 => Loss: 631.837547\n",
      "Iteration  241 => Weight: 0.241000 => Loss: 631.135989\n",
      "Iteration  242 => Weight: 0.242000 => Loss: 630.434870\n",
      "Iteration  243 => Weight: 0.243000 => Loss: 629.734188\n",
      "Iteration  244 => Weight: 0.244000 => Loss: 629.033943\n",
      "Iteration  245 => Weight: 0.245000 => Loss: 628.334137\n",
      "Iteration  246 => Weight: 0.246000 => Loss: 627.634767\n",
      "Iteration  247 => Weight: 0.247000 => Loss: 626.935836\n",
      "Iteration  248 => Weight: 0.248000 => Loss: 626.237342\n",
      "Iteration  249 => Weight: 0.249000 => Loss: 625.539285\n",
      "Iteration  250 => Weight: 0.250000 => Loss: 624.841667\n",
      "Iteration  251 => Weight: 0.251000 => Loss: 624.144485\n",
      "Iteration  252 => Weight: 0.252000 => Loss: 623.447742\n",
      "Iteration  253 => Weight: 0.253000 => Loss: 622.751436\n",
      "Iteration  254 => Weight: 0.254000 => Loss: 622.055567\n",
      "Iteration  255 => Weight: 0.255000 => Loss: 621.360137\n",
      "Iteration  256 => Weight: 0.256000 => Loss: 620.665143\n",
      "Iteration  257 => Weight: 0.257000 => Loss: 619.970588\n",
      "Iteration  258 => Weight: 0.258000 => Loss: 619.276470\n",
      "Iteration  259 => Weight: 0.259000 => Loss: 618.582789\n",
      "Iteration  260 => Weight: 0.260000 => Loss: 617.889547\n",
      "Iteration  261 => Weight: 0.261000 => Loss: 617.196741\n",
      "Iteration  262 => Weight: 0.262000 => Loss: 616.504374\n",
      "Iteration  263 => Weight: 0.263000 => Loss: 615.812444\n",
      "Iteration  264 => Weight: 0.264000 => Loss: 615.120951\n",
      "Iteration  265 => Weight: 0.265000 => Loss: 614.429897\n",
      "Iteration  266 => Weight: 0.266000 => Loss: 613.739279\n",
      "Iteration  267 => Weight: 0.267000 => Loss: 613.049100\n",
      "Iteration  268 => Weight: 0.268000 => Loss: 612.359358\n",
      "Iteration  269 => Weight: 0.269000 => Loss: 611.670053\n",
      "Iteration  270 => Weight: 0.270000 => Loss: 610.981187\n",
      "Iteration  271 => Weight: 0.271000 => Loss: 610.292757\n",
      "Iteration  272 => Weight: 0.272000 => Loss: 609.604766\n",
      "Iteration  273 => Weight: 0.273000 => Loss: 608.917212\n",
      "Iteration  274 => Weight: 0.274000 => Loss: 608.230095\n",
      "Iteration  275 => Weight: 0.275000 => Loss: 607.543417\n",
      "Iteration  276 => Weight: 0.276000 => Loss: 606.857175\n",
      "Iteration  277 => Weight: 0.277000 => Loss: 606.171372\n",
      "Iteration  278 => Weight: 0.278000 => Loss: 605.486006\n",
      "Iteration  279 => Weight: 0.279000 => Loss: 604.801077\n",
      "Iteration  280 => Weight: 0.280000 => Loss: 604.116587\n",
      "Iteration  281 => Weight: 0.281000 => Loss: 603.432533\n",
      "Iteration  282 => Weight: 0.282000 => Loss: 602.748918\n",
      "Iteration  283 => Weight: 0.283000 => Loss: 602.065740\n",
      "Iteration  284 => Weight: 0.284000 => Loss: 601.382999\n",
      "Iteration  285 => Weight: 0.285000 => Loss: 600.700697\n",
      "Iteration  286 => Weight: 0.286000 => Loss: 600.018831\n",
      "Iteration  287 => Weight: 0.287000 => Loss: 599.337404\n",
      "Iteration  288 => Weight: 0.288000 => Loss: 598.656414\n",
      "Iteration  289 => Weight: 0.289000 => Loss: 597.975861\n",
      "Iteration  290 => Weight: 0.290000 => Loss: 597.295747\n",
      "Iteration  291 => Weight: 0.291000 => Loss: 596.616069\n",
      "Iteration  292 => Weight: 0.292000 => Loss: 595.936830\n",
      "Iteration  293 => Weight: 0.293000 => Loss: 595.258028\n",
      "Iteration  294 => Weight: 0.294000 => Loss: 594.579663\n",
      "Iteration  295 => Weight: 0.295000 => Loss: 593.901737\n",
      "Iteration  296 => Weight: 0.296000 => Loss: 593.224247\n",
      "Iteration  297 => Weight: 0.297000 => Loss: 592.547196\n",
      "Iteration  298 => Weight: 0.298000 => Loss: 591.870582\n",
      "Iteration  299 => Weight: 0.299000 => Loss: 591.194405\n",
      "Iteration  300 => Weight: 0.300000 => Loss: 590.518667\n",
      "Iteration  301 => Weight: 0.301000 => Loss: 589.843365\n",
      "Iteration  302 => Weight: 0.302000 => Loss: 589.168502\n",
      "Iteration  303 => Weight: 0.303000 => Loss: 588.494076\n",
      "Iteration  304 => Weight: 0.304000 => Loss: 587.820087\n",
      "Iteration  305 => Weight: 0.305000 => Loss: 587.146537\n",
      "Iteration  306 => Weight: 0.306000 => Loss: 586.473423\n",
      "Iteration  307 => Weight: 0.307000 => Loss: 585.800748\n",
      "Iteration  308 => Weight: 0.308000 => Loss: 585.128510\n",
      "Iteration  309 => Weight: 0.309000 => Loss: 584.456709\n",
      "Iteration  310 => Weight: 0.310000 => Loss: 583.785347\n",
      "Iteration  311 => Weight: 0.311000 => Loss: 583.114421\n",
      "Iteration  312 => Weight: 0.312000 => Loss: 582.443934\n",
      "Iteration  313 => Weight: 0.313000 => Loss: 581.773884\n",
      "Iteration  314 => Weight: 0.314000 => Loss: 581.104271\n",
      "Iteration  315 => Weight: 0.315000 => Loss: 580.435097\n",
      "Iteration  316 => Weight: 0.316000 => Loss: 579.766359\n",
      "Iteration  317 => Weight: 0.317000 => Loss: 579.098060\n",
      "Iteration  318 => Weight: 0.318000 => Loss: 578.430198\n",
      "Iteration  319 => Weight: 0.319000 => Loss: 577.762773\n",
      "Iteration  320 => Weight: 0.320000 => Loss: 577.095787\n",
      "Iteration  321 => Weight: 0.321000 => Loss: 576.429237\n",
      "Iteration  322 => Weight: 0.322000 => Loss: 575.763126\n",
      "Iteration  323 => Weight: 0.323000 => Loss: 575.097452\n",
      "Iteration  324 => Weight: 0.324000 => Loss: 574.432215\n",
      "Iteration  325 => Weight: 0.325000 => Loss: 573.767417\n",
      "Iteration  326 => Weight: 0.326000 => Loss: 573.103055\n",
      "Iteration  327 => Weight: 0.327000 => Loss: 572.439132\n",
      "Iteration  328 => Weight: 0.328000 => Loss: 571.775646\n",
      "Iteration  329 => Weight: 0.329000 => Loss: 571.112597\n",
      "Iteration  330 => Weight: 0.330000 => Loss: 570.449987\n",
      "Iteration  331 => Weight: 0.331000 => Loss: 569.787813\n",
      "Iteration  332 => Weight: 0.332000 => Loss: 569.126078\n",
      "Iteration  333 => Weight: 0.333000 => Loss: 568.464780\n",
      "Iteration  334 => Weight: 0.334000 => Loss: 567.803919\n",
      "Iteration  335 => Weight: 0.335000 => Loss: 567.143497\n",
      "Iteration  336 => Weight: 0.336000 => Loss: 566.483511\n",
      "Iteration  337 => Weight: 0.337000 => Loss: 565.823964\n",
      "Iteration  338 => Weight: 0.338000 => Loss: 565.164854\n",
      "Iteration  339 => Weight: 0.339000 => Loss: 564.506181\n",
      "Iteration  340 => Weight: 0.340000 => Loss: 563.847947\n",
      "Iteration  341 => Weight: 0.341000 => Loss: 563.190149\n",
      "Iteration  342 => Weight: 0.342000 => Loss: 562.532790\n",
      "Iteration  343 => Weight: 0.343000 => Loss: 561.875868\n",
      "Iteration  344 => Weight: 0.344000 => Loss: 561.219383\n",
      "Iteration  345 => Weight: 0.345000 => Loss: 560.563337\n",
      "Iteration  346 => Weight: 0.346000 => Loss: 559.907727\n",
      "Iteration  347 => Weight: 0.347000 => Loss: 559.252556\n",
      "Iteration  348 => Weight: 0.348000 => Loss: 558.597822\n",
      "Iteration  349 => Weight: 0.349000 => Loss: 557.943525\n",
      "Iteration  350 => Weight: 0.350000 => Loss: 557.289667\n",
      "Iteration  351 => Weight: 0.351000 => Loss: 556.636245\n",
      "Iteration  352 => Weight: 0.352000 => Loss: 555.983262\n",
      "Iteration  353 => Weight: 0.353000 => Loss: 555.330716\n",
      "Iteration  354 => Weight: 0.354000 => Loss: 554.678607\n",
      "Iteration  355 => Weight: 0.355000 => Loss: 554.026937\n",
      "Iteration  356 => Weight: 0.356000 => Loss: 553.375703\n",
      "Iteration  357 => Weight: 0.357000 => Loss: 552.724908\n",
      "Iteration  358 => Weight: 0.358000 => Loss: 552.074550\n",
      "Iteration  359 => Weight: 0.359000 => Loss: 551.424629\n",
      "Iteration  360 => Weight: 0.360000 => Loss: 550.775147\n",
      "Iteration  361 => Weight: 0.361000 => Loss: 550.126101\n",
      "Iteration  362 => Weight: 0.362000 => Loss: 549.477494\n",
      "Iteration  363 => Weight: 0.363000 => Loss: 548.829324\n",
      "Iteration  364 => Weight: 0.364000 => Loss: 548.181591\n",
      "Iteration  365 => Weight: 0.365000 => Loss: 547.534297\n",
      "Iteration  366 => Weight: 0.366000 => Loss: 546.887439\n",
      "Iteration  367 => Weight: 0.367000 => Loss: 546.241020\n",
      "Iteration  368 => Weight: 0.368000 => Loss: 545.595038\n",
      "Iteration  369 => Weight: 0.369000 => Loss: 544.949493\n",
      "Iteration  370 => Weight: 0.370000 => Loss: 544.304387\n",
      "Iteration  371 => Weight: 0.371000 => Loss: 543.659717\n",
      "Iteration  372 => Weight: 0.372000 => Loss: 543.015486\n",
      "Iteration  373 => Weight: 0.373000 => Loss: 542.371692\n",
      "Iteration  374 => Weight: 0.374000 => Loss: 541.728335\n",
      "Iteration  375 => Weight: 0.375000 => Loss: 541.085417\n",
      "Iteration  376 => Weight: 0.376000 => Loss: 540.442935\n",
      "Iteration  377 => Weight: 0.377000 => Loss: 539.800892\n",
      "Iteration  378 => Weight: 0.378000 => Loss: 539.159286\n",
      "Iteration  379 => Weight: 0.379000 => Loss: 538.518117\n",
      "Iteration  380 => Weight: 0.380000 => Loss: 537.877387\n",
      "Iteration  381 => Weight: 0.381000 => Loss: 537.237093\n",
      "Iteration  382 => Weight: 0.382000 => Loss: 536.597238\n",
      "Iteration  383 => Weight: 0.383000 => Loss: 535.957820\n",
      "Iteration  384 => Weight: 0.384000 => Loss: 535.318839\n",
      "Iteration  385 => Weight: 0.385000 => Loss: 534.680297\n",
      "Iteration  386 => Weight: 0.386000 => Loss: 534.042191\n",
      "Iteration  387 => Weight: 0.387000 => Loss: 533.404524\n",
      "Iteration  388 => Weight: 0.388000 => Loss: 532.767294\n",
      "Iteration  389 => Weight: 0.389000 => Loss: 532.130501\n",
      "Iteration  390 => Weight: 0.390000 => Loss: 531.494147\n",
      "Iteration  391 => Weight: 0.391000 => Loss: 530.858229\n",
      "Iteration  392 => Weight: 0.392000 => Loss: 530.222750\n",
      "Iteration  393 => Weight: 0.393000 => Loss: 529.587708\n",
      "Iteration  394 => Weight: 0.394000 => Loss: 528.953103\n",
      "Iteration  395 => Weight: 0.395000 => Loss: 528.318937\n",
      "Iteration  396 => Weight: 0.396000 => Loss: 527.685207\n",
      "Iteration  397 => Weight: 0.397000 => Loss: 527.051916\n",
      "Iteration  398 => Weight: 0.398000 => Loss: 526.419062\n",
      "Iteration  399 => Weight: 0.399000 => Loss: 525.786645\n",
      "Iteration  400 => Weight: 0.400000 => Loss: 525.154667\n",
      "Iteration  401 => Weight: 0.401000 => Loss: 524.523125\n",
      "Iteration  402 => Weight: 0.402000 => Loss: 523.892022\n",
      "Iteration  403 => Weight: 0.403000 => Loss: 523.261356\n",
      "Iteration  404 => Weight: 0.404000 => Loss: 522.631127\n",
      "Iteration  405 => Weight: 0.405000 => Loss: 522.001337\n",
      "Iteration  406 => Weight: 0.406000 => Loss: 521.371983\n",
      "Iteration  407 => Weight: 0.407000 => Loss: 520.743068\n",
      "Iteration  408 => Weight: 0.408000 => Loss: 520.114590\n",
      "Iteration  409 => Weight: 0.409000 => Loss: 519.486549\n",
      "Iteration  410 => Weight: 0.410000 => Loss: 518.858947\n",
      "Iteration  411 => Weight: 0.411000 => Loss: 518.231781\n",
      "Iteration  412 => Weight: 0.412000 => Loss: 517.605054\n",
      "Iteration  413 => Weight: 0.413000 => Loss: 516.978764\n",
      "Iteration  414 => Weight: 0.414000 => Loss: 516.352911\n",
      "Iteration  415 => Weight: 0.415000 => Loss: 515.727497\n",
      "Iteration  416 => Weight: 0.416000 => Loss: 515.102519\n",
      "Iteration  417 => Weight: 0.417000 => Loss: 514.477980\n",
      "Iteration  418 => Weight: 0.418000 => Loss: 513.853878\n",
      "Iteration  419 => Weight: 0.419000 => Loss: 513.230213\n",
      "Iteration  420 => Weight: 0.420000 => Loss: 512.606987\n",
      "Iteration  421 => Weight: 0.421000 => Loss: 511.984197\n",
      "Iteration  422 => Weight: 0.422000 => Loss: 511.361846\n",
      "Iteration  423 => Weight: 0.423000 => Loss: 510.739932\n",
      "Iteration  424 => Weight: 0.424000 => Loss: 510.118455\n",
      "Iteration  425 => Weight: 0.425000 => Loss: 509.497417\n",
      "Iteration  426 => Weight: 0.426000 => Loss: 508.876815\n",
      "Iteration  427 => Weight: 0.427000 => Loss: 508.256652\n",
      "Iteration  428 => Weight: 0.428000 => Loss: 507.636926\n",
      "Iteration  429 => Weight: 0.429000 => Loss: 507.017637\n",
      "Iteration  430 => Weight: 0.430000 => Loss: 506.398787\n",
      "Iteration  431 => Weight: 0.431000 => Loss: 505.780373\n",
      "Iteration  432 => Weight: 0.432000 => Loss: 505.162398\n",
      "Iteration  433 => Weight: 0.433000 => Loss: 504.544860\n",
      "Iteration  434 => Weight: 0.434000 => Loss: 503.927759\n",
      "Iteration  435 => Weight: 0.435000 => Loss: 503.311097\n",
      "Iteration  436 => Weight: 0.436000 => Loss: 502.694871\n",
      "Iteration  437 => Weight: 0.437000 => Loss: 502.079084\n",
      "Iteration  438 => Weight: 0.438000 => Loss: 501.463734\n",
      "Iteration  439 => Weight: 0.439000 => Loss: 500.848821\n",
      "Iteration  440 => Weight: 0.440000 => Loss: 500.234347\n",
      "Iteration  441 => Weight: 0.441000 => Loss: 499.620309\n",
      "Iteration  442 => Weight: 0.442000 => Loss: 499.006710\n",
      "Iteration  443 => Weight: 0.443000 => Loss: 498.393548\n",
      "Iteration  444 => Weight: 0.444000 => Loss: 497.780823\n",
      "Iteration  445 => Weight: 0.445000 => Loss: 497.168537\n",
      "Iteration  446 => Weight: 0.446000 => Loss: 496.556687\n",
      "Iteration  447 => Weight: 0.447000 => Loss: 495.945276\n",
      "Iteration  448 => Weight: 0.448000 => Loss: 495.334302\n",
      "Iteration  449 => Weight: 0.449000 => Loss: 494.723765\n",
      "Iteration  450 => Weight: 0.450000 => Loss: 494.113667\n",
      "Iteration  451 => Weight: 0.451000 => Loss: 493.504005\n",
      "Iteration  452 => Weight: 0.452000 => Loss: 492.894782\n",
      "Iteration  453 => Weight: 0.453000 => Loss: 492.285996\n",
      "Iteration  454 => Weight: 0.454000 => Loss: 491.677647\n",
      "Iteration  455 => Weight: 0.455000 => Loss: 491.069737\n",
      "Iteration  456 => Weight: 0.456000 => Loss: 490.462263\n",
      "Iteration  457 => Weight: 0.457000 => Loss: 489.855228\n",
      "Iteration  458 => Weight: 0.458000 => Loss: 489.248630\n",
      "Iteration  459 => Weight: 0.459000 => Loss: 488.642469\n",
      "Iteration  460 => Weight: 0.460000 => Loss: 488.036747\n",
      "Iteration  461 => Weight: 0.461000 => Loss: 487.431461\n",
      "Iteration  462 => Weight: 0.462000 => Loss: 486.826614\n",
      "Iteration  463 => Weight: 0.463000 => Loss: 486.222204\n",
      "Iteration  464 => Weight: 0.464000 => Loss: 485.618231\n",
      "Iteration  465 => Weight: 0.465000 => Loss: 485.014697\n",
      "Iteration  466 => Weight: 0.466000 => Loss: 484.411599\n",
      "Iteration  467 => Weight: 0.467000 => Loss: 483.808940\n",
      "Iteration  468 => Weight: 0.468000 => Loss: 483.206718\n",
      "Iteration  469 => Weight: 0.469000 => Loss: 482.604933\n",
      "Iteration  470 => Weight: 0.470000 => Loss: 482.003587\n",
      "Iteration  471 => Weight: 0.471000 => Loss: 481.402677\n",
      "Iteration  472 => Weight: 0.472000 => Loss: 480.802206\n",
      "Iteration  473 => Weight: 0.473000 => Loss: 480.202172\n",
      "Iteration  474 => Weight: 0.474000 => Loss: 479.602575\n",
      "Iteration  475 => Weight: 0.475000 => Loss: 479.003417\n",
      "Iteration  476 => Weight: 0.476000 => Loss: 478.404695\n",
      "Iteration  477 => Weight: 0.477000 => Loss: 477.806412\n",
      "Iteration  478 => Weight: 0.478000 => Loss: 477.208566\n",
      "Iteration  479 => Weight: 0.479000 => Loss: 476.611157\n",
      "Iteration  480 => Weight: 0.480000 => Loss: 476.014187\n",
      "Iteration  481 => Weight: 0.481000 => Loss: 475.417653\n",
      "Iteration  482 => Weight: 0.482000 => Loss: 474.821558\n",
      "Iteration  483 => Weight: 0.483000 => Loss: 474.225900\n",
      "Iteration  484 => Weight: 0.484000 => Loss: 473.630679\n",
      "Iteration  485 => Weight: 0.485000 => Loss: 473.035897\n",
      "Iteration  486 => Weight: 0.486000 => Loss: 472.441551\n",
      "Iteration  487 => Weight: 0.487000 => Loss: 471.847644\n",
      "Iteration  488 => Weight: 0.488000 => Loss: 471.254174\n",
      "Iteration  489 => Weight: 0.489000 => Loss: 470.661141\n",
      "Iteration  490 => Weight: 0.490000 => Loss: 470.068547\n",
      "Iteration  491 => Weight: 0.491000 => Loss: 469.476389\n",
      "Iteration  492 => Weight: 0.492000 => Loss: 468.884670\n",
      "Iteration  493 => Weight: 0.493000 => Loss: 468.293388\n",
      "Iteration  494 => Weight: 0.494000 => Loss: 467.702543\n",
      "Iteration  495 => Weight: 0.495000 => Loss: 467.112137\n",
      "Iteration  496 => Weight: 0.496000 => Loss: 466.522167\n",
      "Iteration  497 => Weight: 0.497000 => Loss: 465.932636\n",
      "Iteration  498 => Weight: 0.498000 => Loss: 465.343542\n",
      "Iteration  499 => Weight: 0.499000 => Loss: 464.754885\n",
      "Iteration  500 => Weight: 0.500000 => Loss: 464.166667\n",
      "Iteration  501 => Weight: 0.501000 => Loss: 463.578885\n",
      "Iteration  502 => Weight: 0.502000 => Loss: 462.991542\n",
      "Iteration  503 => Weight: 0.503000 => Loss: 462.404636\n",
      "Iteration  504 => Weight: 0.504000 => Loss: 461.818167\n",
      "Iteration  505 => Weight: 0.505000 => Loss: 461.232137\n",
      "Iteration  506 => Weight: 0.506000 => Loss: 460.646543\n",
      "Iteration  507 => Weight: 0.507000 => Loss: 460.061388\n",
      "Iteration  508 => Weight: 0.508000 => Loss: 459.476670\n",
      "Iteration  509 => Weight: 0.509000 => Loss: 458.892389\n",
      "Iteration  510 => Weight: 0.510000 => Loss: 458.308547\n",
      "Iteration  511 => Weight: 0.511000 => Loss: 457.725141\n",
      "Iteration  512 => Weight: 0.512000 => Loss: 457.142174\n",
      "Iteration  513 => Weight: 0.513000 => Loss: 456.559644\n",
      "Iteration  514 => Weight: 0.514000 => Loss: 455.977551\n",
      "Iteration  515 => Weight: 0.515000 => Loss: 455.395897\n",
      "Iteration  516 => Weight: 0.516000 => Loss: 454.814679\n",
      "Iteration  517 => Weight: 0.517000 => Loss: 454.233900\n",
      "Iteration  518 => Weight: 0.518000 => Loss: 453.653558\n",
      "Iteration  519 => Weight: 0.519000 => Loss: 453.073653\n",
      "Iteration  520 => Weight: 0.520000 => Loss: 452.494187\n",
      "Iteration  521 => Weight: 0.521000 => Loss: 451.915157\n",
      "Iteration  522 => Weight: 0.522000 => Loss: 451.336566\n",
      "Iteration  523 => Weight: 0.523000 => Loss: 450.758412\n",
      "Iteration  524 => Weight: 0.524000 => Loss: 450.180695\n",
      "Iteration  525 => Weight: 0.525000 => Loss: 449.603417\n",
      "Iteration  526 => Weight: 0.526000 => Loss: 449.026575\n",
      "Iteration  527 => Weight: 0.527000 => Loss: 448.450172\n",
      "Iteration  528 => Weight: 0.528000 => Loss: 447.874206\n",
      "Iteration  529 => Weight: 0.529000 => Loss: 447.298677\n",
      "Iteration  530 => Weight: 0.530000 => Loss: 446.723587\n",
      "Iteration  531 => Weight: 0.531000 => Loss: 446.148933\n",
      "Iteration  532 => Weight: 0.532000 => Loss: 445.574718\n",
      "Iteration  533 => Weight: 0.533000 => Loss: 445.000940\n",
      "Iteration  534 => Weight: 0.534000 => Loss: 444.427599\n",
      "Iteration  535 => Weight: 0.535000 => Loss: 443.854697\n",
      "Iteration  536 => Weight: 0.536000 => Loss: 443.282231\n",
      "Iteration  537 => Weight: 0.537000 => Loss: 442.710204\n",
      "Iteration  538 => Weight: 0.538000 => Loss: 442.138614\n",
      "Iteration  539 => Weight: 0.539000 => Loss: 441.567461\n",
      "Iteration  540 => Weight: 0.540000 => Loss: 440.996747\n",
      "Iteration  541 => Weight: 0.541000 => Loss: 440.426469\n",
      "Iteration  542 => Weight: 0.542000 => Loss: 439.856630\n",
      "Iteration  543 => Weight: 0.543000 => Loss: 439.287228\n",
      "Iteration  544 => Weight: 0.544000 => Loss: 438.718263\n",
      "Iteration  545 => Weight: 0.545000 => Loss: 438.149737\n",
      "Iteration  546 => Weight: 0.546000 => Loss: 437.581647\n",
      "Iteration  547 => Weight: 0.547000 => Loss: 437.013996\n",
      "Iteration  548 => Weight: 0.548000 => Loss: 436.446782\n",
      "Iteration  549 => Weight: 0.549000 => Loss: 435.880005\n",
      "Iteration  550 => Weight: 0.550000 => Loss: 435.313667\n",
      "Iteration  551 => Weight: 0.551000 => Loss: 434.747765\n",
      "Iteration  552 => Weight: 0.552000 => Loss: 434.182302\n",
      "Iteration  553 => Weight: 0.553000 => Loss: 433.617276\n",
      "Iteration  554 => Weight: 0.554000 => Loss: 433.052687\n",
      "Iteration  555 => Weight: 0.555000 => Loss: 432.488537\n",
      "Iteration  556 => Weight: 0.556000 => Loss: 431.924823\n",
      "Iteration  557 => Weight: 0.557000 => Loss: 431.361548\n",
      "Iteration  558 => Weight: 0.558000 => Loss: 430.798710\n",
      "Iteration  559 => Weight: 0.559000 => Loss: 430.236309\n",
      "Iteration  560 => Weight: 0.560000 => Loss: 429.674347\n",
      "Iteration  561 => Weight: 0.561000 => Loss: 429.112821\n",
      "Iteration  562 => Weight: 0.562000 => Loss: 428.551734\n",
      "Iteration  563 => Weight: 0.563000 => Loss: 427.991084\n",
      "Iteration  564 => Weight: 0.564000 => Loss: 427.430871\n",
      "Iteration  565 => Weight: 0.565000 => Loss: 426.871097\n",
      "Iteration  566 => Weight: 0.566000 => Loss: 426.311759\n",
      "Iteration  567 => Weight: 0.567000 => Loss: 425.752860\n",
      "Iteration  568 => Weight: 0.568000 => Loss: 425.194398\n",
      "Iteration  569 => Weight: 0.569000 => Loss: 424.636373\n",
      "Iteration  570 => Weight: 0.570000 => Loss: 424.078787\n",
      "Iteration  571 => Weight: 0.571000 => Loss: 423.521637\n",
      "Iteration  572 => Weight: 0.572000 => Loss: 422.964926\n",
      "Iteration  573 => Weight: 0.573000 => Loss: 422.408652\n",
      "Iteration  574 => Weight: 0.574000 => Loss: 421.852815\n",
      "Iteration  575 => Weight: 0.575000 => Loss: 421.297417\n",
      "Iteration  576 => Weight: 0.576000 => Loss: 420.742455\n",
      "Iteration  577 => Weight: 0.577000 => Loss: 420.187932\n",
      "Iteration  578 => Weight: 0.578000 => Loss: 419.633846\n",
      "Iteration  579 => Weight: 0.579000 => Loss: 419.080197\n",
      "Iteration  580 => Weight: 0.580000 => Loss: 418.526987\n",
      "Iteration  581 => Weight: 0.581000 => Loss: 417.974213\n",
      "Iteration  582 => Weight: 0.582000 => Loss: 417.421878\n",
      "Iteration  583 => Weight: 0.583000 => Loss: 416.869980\n",
      "Iteration  584 => Weight: 0.584000 => Loss: 416.318519\n",
      "Iteration  585 => Weight: 0.585000 => Loss: 415.767497\n",
      "Iteration  586 => Weight: 0.586000 => Loss: 415.216911\n",
      "Iteration  587 => Weight: 0.587000 => Loss: 414.666764\n",
      "Iteration  588 => Weight: 0.588000 => Loss: 414.117054\n",
      "Iteration  589 => Weight: 0.589000 => Loss: 413.567781\n",
      "Iteration  590 => Weight: 0.590000 => Loss: 413.018947\n",
      "Iteration  591 => Weight: 0.591000 => Loss: 412.470549\n",
      "Iteration  592 => Weight: 0.592000 => Loss: 411.922590\n",
      "Iteration  593 => Weight: 0.593000 => Loss: 411.375068\n",
      "Iteration  594 => Weight: 0.594000 => Loss: 410.827983\n",
      "Iteration  595 => Weight: 0.595000 => Loss: 410.281337\n",
      "Iteration  596 => Weight: 0.596000 => Loss: 409.735127\n",
      "Iteration  597 => Weight: 0.597000 => Loss: 409.189356\n",
      "Iteration  598 => Weight: 0.598000 => Loss: 408.644022\n",
      "Iteration  599 => Weight: 0.599000 => Loss: 408.099125\n",
      "Iteration  600 => Weight: 0.600000 => Loss: 407.554667\n",
      "Iteration  601 => Weight: 0.601000 => Loss: 407.010645\n",
      "Iteration  602 => Weight: 0.602000 => Loss: 406.467062\n",
      "Iteration  603 => Weight: 0.603000 => Loss: 405.923916\n",
      "Iteration  604 => Weight: 0.604000 => Loss: 405.381207\n",
      "Iteration  605 => Weight: 0.605000 => Loss: 404.838937\n",
      "Iteration  606 => Weight: 0.606000 => Loss: 404.297103\n",
      "Iteration  607 => Weight: 0.607000 => Loss: 403.755708\n",
      "Iteration  608 => Weight: 0.608000 => Loss: 403.214750\n",
      "Iteration  609 => Weight: 0.609000 => Loss: 402.674229\n",
      "Iteration  610 => Weight: 0.610000 => Loss: 402.134147\n",
      "Iteration  611 => Weight: 0.611000 => Loss: 401.594501\n",
      "Iteration  612 => Weight: 0.612000 => Loss: 401.055294\n",
      "Iteration  613 => Weight: 0.613000 => Loss: 400.516524\n",
      "Iteration  614 => Weight: 0.614000 => Loss: 399.978191\n",
      "Iteration  615 => Weight: 0.615000 => Loss: 399.440297\n",
      "Iteration  616 => Weight: 0.616000 => Loss: 398.902839\n",
      "Iteration  617 => Weight: 0.617000 => Loss: 398.365820\n",
      "Iteration  618 => Weight: 0.618000 => Loss: 397.829238\n",
      "Iteration  619 => Weight: 0.619000 => Loss: 397.293093\n",
      "Iteration  620 => Weight: 0.620000 => Loss: 396.757387\n",
      "Iteration  621 => Weight: 0.621000 => Loss: 396.222117\n",
      "Iteration  622 => Weight: 0.622000 => Loss: 395.687286\n",
      "Iteration  623 => Weight: 0.623000 => Loss: 395.152892\n",
      "Iteration  624 => Weight: 0.624000 => Loss: 394.618935\n",
      "Iteration  625 => Weight: 0.625000 => Loss: 394.085417\n",
      "Iteration  626 => Weight: 0.626000 => Loss: 393.552335\n",
      "Iteration  627 => Weight: 0.627000 => Loss: 393.019692\n",
      "Iteration  628 => Weight: 0.628000 => Loss: 392.487486\n",
      "Iteration  629 => Weight: 0.629000 => Loss: 391.955717\n",
      "Iteration  630 => Weight: 0.630000 => Loss: 391.424387\n",
      "Iteration  631 => Weight: 0.631000 => Loss: 390.893493\n",
      "Iteration  632 => Weight: 0.632000 => Loss: 390.363038\n",
      "Iteration  633 => Weight: 0.633000 => Loss: 389.833020\n",
      "Iteration  634 => Weight: 0.634000 => Loss: 389.303439\n",
      "Iteration  635 => Weight: 0.635000 => Loss: 388.774297\n",
      "Iteration  636 => Weight: 0.636000 => Loss: 388.245591\n",
      "Iteration  637 => Weight: 0.637000 => Loss: 387.717324\n",
      "Iteration  638 => Weight: 0.638000 => Loss: 387.189494\n",
      "Iteration  639 => Weight: 0.639000 => Loss: 386.662101\n",
      "Iteration  640 => Weight: 0.640000 => Loss: 386.135147\n",
      "Iteration  641 => Weight: 0.641000 => Loss: 385.608629\n",
      "Iteration  642 => Weight: 0.642000 => Loss: 385.082550\n",
      "Iteration  643 => Weight: 0.643000 => Loss: 384.556908\n",
      "Iteration  644 => Weight: 0.644000 => Loss: 384.031703\n",
      "Iteration  645 => Weight: 0.645000 => Loss: 383.506937\n",
      "Iteration  646 => Weight: 0.646000 => Loss: 382.982607\n",
      "Iteration  647 => Weight: 0.647000 => Loss: 382.458716\n",
      "Iteration  648 => Weight: 0.648000 => Loss: 381.935262\n",
      "Iteration  649 => Weight: 0.649000 => Loss: 381.412245\n",
      "Iteration  650 => Weight: 0.650000 => Loss: 380.889667\n",
      "Iteration  651 => Weight: 0.651000 => Loss: 380.367525\n",
      "Iteration  652 => Weight: 0.652000 => Loss: 379.845822\n",
      "Iteration  653 => Weight: 0.653000 => Loss: 379.324556\n",
      "Iteration  654 => Weight: 0.654000 => Loss: 378.803727\n",
      "Iteration  655 => Weight: 0.655000 => Loss: 378.283337\n",
      "Iteration  656 => Weight: 0.656000 => Loss: 377.763383\n",
      "Iteration  657 => Weight: 0.657000 => Loss: 377.243868\n",
      "Iteration  658 => Weight: 0.658000 => Loss: 376.724790\n",
      "Iteration  659 => Weight: 0.659000 => Loss: 376.206149\n",
      "Iteration  660 => Weight: 0.660000 => Loss: 375.687947\n",
      "Iteration  661 => Weight: 0.661000 => Loss: 375.170181\n",
      "Iteration  662 => Weight: 0.662000 => Loss: 374.652854\n",
      "Iteration  663 => Weight: 0.663000 => Loss: 374.135964\n",
      "Iteration  664 => Weight: 0.664000 => Loss: 373.619511\n",
      "Iteration  665 => Weight: 0.665000 => Loss: 373.103497\n",
      "Iteration  666 => Weight: 0.666000 => Loss: 372.587919\n",
      "Iteration  667 => Weight: 0.667000 => Loss: 372.072780\n",
      "Iteration  668 => Weight: 0.668000 => Loss: 371.558078\n",
      "Iteration  669 => Weight: 0.669000 => Loss: 371.043813\n",
      "Iteration  670 => Weight: 0.670000 => Loss: 370.529987\n",
      "Iteration  671 => Weight: 0.671000 => Loss: 370.016597\n",
      "Iteration  672 => Weight: 0.672000 => Loss: 369.503646\n",
      "Iteration  673 => Weight: 0.673000 => Loss: 368.991132\n",
      "Iteration  674 => Weight: 0.674000 => Loss: 368.479055\n",
      "Iteration  675 => Weight: 0.675000 => Loss: 367.967417\n",
      "Iteration  676 => Weight: 0.676000 => Loss: 367.456215\n",
      "Iteration  677 => Weight: 0.677000 => Loss: 366.945452\n",
      "Iteration  678 => Weight: 0.678000 => Loss: 366.435126\n",
      "Iteration  679 => Weight: 0.679000 => Loss: 365.925237\n",
      "Iteration  680 => Weight: 0.680000 => Loss: 365.415787\n",
      "Iteration  681 => Weight: 0.681000 => Loss: 364.906773\n",
      "Iteration  682 => Weight: 0.682000 => Loss: 364.398198\n",
      "Iteration  683 => Weight: 0.683000 => Loss: 363.890060\n",
      "Iteration  684 => Weight: 0.684000 => Loss: 363.382359\n",
      "Iteration  685 => Weight: 0.685000 => Loss: 362.875097\n",
      "Iteration  686 => Weight: 0.686000 => Loss: 362.368271\n",
      "Iteration  687 => Weight: 0.687000 => Loss: 361.861884\n",
      "Iteration  688 => Weight: 0.688000 => Loss: 361.355934\n",
      "Iteration  689 => Weight: 0.689000 => Loss: 360.850421\n",
      "Iteration  690 => Weight: 0.690000 => Loss: 360.345347\n",
      "Iteration  691 => Weight: 0.691000 => Loss: 359.840709\n",
      "Iteration  692 => Weight: 0.692000 => Loss: 359.336510\n",
      "Iteration  693 => Weight: 0.693000 => Loss: 358.832748\n",
      "Iteration  694 => Weight: 0.694000 => Loss: 358.329423\n",
      "Iteration  695 => Weight: 0.695000 => Loss: 357.826537\n",
      "Iteration  696 => Weight: 0.696000 => Loss: 357.324087\n",
      "Iteration  697 => Weight: 0.697000 => Loss: 356.822076\n",
      "Iteration  698 => Weight: 0.698000 => Loss: 356.320502\n",
      "Iteration  699 => Weight: 0.699000 => Loss: 355.819365\n",
      "Iteration  700 => Weight: 0.700000 => Loss: 355.318667\n",
      "Iteration  701 => Weight: 0.701000 => Loss: 354.818405\n",
      "Iteration  702 => Weight: 0.702000 => Loss: 354.318582\n",
      "Iteration  703 => Weight: 0.703000 => Loss: 353.819196\n",
      "Iteration  704 => Weight: 0.704000 => Loss: 353.320247\n",
      "Iteration  705 => Weight: 0.705000 => Loss: 352.821737\n",
      "Iteration  706 => Weight: 0.706000 => Loss: 352.323663\n",
      "Iteration  707 => Weight: 0.707000 => Loss: 351.826028\n",
      "Iteration  708 => Weight: 0.708000 => Loss: 351.328830\n",
      "Iteration  709 => Weight: 0.709000 => Loss: 350.832069\n",
      "Iteration  710 => Weight: 0.710000 => Loss: 350.335747\n",
      "Iteration  711 => Weight: 0.711000 => Loss: 349.839861\n",
      "Iteration  712 => Weight: 0.712000 => Loss: 349.344414\n",
      "Iteration  713 => Weight: 0.713000 => Loss: 348.849404\n",
      "Iteration  714 => Weight: 0.714000 => Loss: 348.354831\n",
      "Iteration  715 => Weight: 0.715000 => Loss: 347.860697\n",
      "Iteration  716 => Weight: 0.716000 => Loss: 347.366999\n",
      "Iteration  717 => Weight: 0.717000 => Loss: 346.873740\n",
      "Iteration  718 => Weight: 0.718000 => Loss: 346.380918\n",
      "Iteration  719 => Weight: 0.719000 => Loss: 345.888533\n",
      "Iteration  720 => Weight: 0.720000 => Loss: 345.396587\n",
      "Iteration  721 => Weight: 0.721000 => Loss: 344.905077\n",
      "Iteration  722 => Weight: 0.722000 => Loss: 344.414006\n",
      "Iteration  723 => Weight: 0.723000 => Loss: 343.923372\n",
      "Iteration  724 => Weight: 0.724000 => Loss: 343.433175\n",
      "Iteration  725 => Weight: 0.725000 => Loss: 342.943417\n",
      "Iteration  726 => Weight: 0.726000 => Loss: 342.454095\n",
      "Iteration  727 => Weight: 0.727000 => Loss: 341.965212\n",
      "Iteration  728 => Weight: 0.728000 => Loss: 341.476766\n",
      "Iteration  729 => Weight: 0.729000 => Loss: 340.988757\n",
      "Iteration  730 => Weight: 0.730000 => Loss: 340.501187\n",
      "Iteration  731 => Weight: 0.731000 => Loss: 340.014053\n",
      "Iteration  732 => Weight: 0.732000 => Loss: 339.527358\n",
      "Iteration  733 => Weight: 0.733000 => Loss: 339.041100\n",
      "Iteration  734 => Weight: 0.734000 => Loss: 338.555279\n",
      "Iteration  735 => Weight: 0.735000 => Loss: 338.069897\n",
      "Iteration  736 => Weight: 0.736000 => Loss: 337.584951\n",
      "Iteration  737 => Weight: 0.737000 => Loss: 337.100444\n",
      "Iteration  738 => Weight: 0.738000 => Loss: 336.616374\n",
      "Iteration  739 => Weight: 0.739000 => Loss: 336.132741\n",
      "Iteration  740 => Weight: 0.740000 => Loss: 335.649547\n",
      "Iteration  741 => Weight: 0.741000 => Loss: 335.166789\n",
      "Iteration  742 => Weight: 0.742000 => Loss: 334.684470\n",
      "Iteration  743 => Weight: 0.743000 => Loss: 334.202588\n",
      "Iteration  744 => Weight: 0.744000 => Loss: 333.721143\n",
      "Iteration  745 => Weight: 0.745000 => Loss: 333.240137\n",
      "Iteration  746 => Weight: 0.746000 => Loss: 332.759567\n",
      "Iteration  747 => Weight: 0.747000 => Loss: 332.279436\n",
      "Iteration  748 => Weight: 0.748000 => Loss: 331.799742\n",
      "Iteration  749 => Weight: 0.749000 => Loss: 331.320485\n",
      "Iteration  750 => Weight: 0.750000 => Loss: 330.841667\n",
      "Iteration  751 => Weight: 0.751000 => Loss: 330.363285\n",
      "Iteration  752 => Weight: 0.752000 => Loss: 329.885342\n",
      "Iteration  753 => Weight: 0.753000 => Loss: 329.407836\n",
      "Iteration  754 => Weight: 0.754000 => Loss: 328.930767\n",
      "Iteration  755 => Weight: 0.755000 => Loss: 328.454137\n",
      "Iteration  756 => Weight: 0.756000 => Loss: 327.977943\n",
      "Iteration  757 => Weight: 0.757000 => Loss: 327.502188\n",
      "Iteration  758 => Weight: 0.758000 => Loss: 327.026870\n",
      "Iteration  759 => Weight: 0.759000 => Loss: 326.551989\n",
      "Iteration  760 => Weight: 0.760000 => Loss: 326.077547\n",
      "Iteration  761 => Weight: 0.761000 => Loss: 325.603541\n",
      "Iteration  762 => Weight: 0.762000 => Loss: 325.129974\n",
      "Iteration  763 => Weight: 0.763000 => Loss: 324.656844\n",
      "Iteration  764 => Weight: 0.764000 => Loss: 324.184151\n",
      "Iteration  765 => Weight: 0.765000 => Loss: 323.711897\n",
      "Iteration  766 => Weight: 0.766000 => Loss: 323.240079\n",
      "Iteration  767 => Weight: 0.767000 => Loss: 322.768700\n",
      "Iteration  768 => Weight: 0.768000 => Loss: 322.297758\n",
      "Iteration  769 => Weight: 0.769000 => Loss: 321.827253\n",
      "Iteration  770 => Weight: 0.770000 => Loss: 321.357187\n",
      "Iteration  771 => Weight: 0.771000 => Loss: 320.887557\n",
      "Iteration  772 => Weight: 0.772000 => Loss: 320.418366\n",
      "Iteration  773 => Weight: 0.773000 => Loss: 319.949612\n",
      "Iteration  774 => Weight: 0.774000 => Loss: 319.481295\n",
      "Iteration  775 => Weight: 0.775000 => Loss: 319.013417\n",
      "Iteration  776 => Weight: 0.776000 => Loss: 318.545975\n",
      "Iteration  777 => Weight: 0.777000 => Loss: 318.078972\n",
      "Iteration  778 => Weight: 0.778000 => Loss: 317.612406\n",
      "Iteration  779 => Weight: 0.779000 => Loss: 317.146277\n",
      "Iteration  780 => Weight: 0.780000 => Loss: 316.680587\n",
      "Iteration  781 => Weight: 0.781000 => Loss: 316.215333\n",
      "Iteration  782 => Weight: 0.782000 => Loss: 315.750518\n",
      "Iteration  783 => Weight: 0.783000 => Loss: 315.286140\n",
      "Iteration  784 => Weight: 0.784000 => Loss: 314.822199\n",
      "Iteration  785 => Weight: 0.785000 => Loss: 314.358697\n",
      "Iteration  786 => Weight: 0.786000 => Loss: 313.895631\n",
      "Iteration  787 => Weight: 0.787000 => Loss: 313.433004\n",
      "Iteration  788 => Weight: 0.788000 => Loss: 312.970814\n",
      "Iteration  789 => Weight: 0.789000 => Loss: 312.509061\n",
      "Iteration  790 => Weight: 0.790000 => Loss: 312.047747\n",
      "Iteration  791 => Weight: 0.791000 => Loss: 311.586869\n",
      "Iteration  792 => Weight: 0.792000 => Loss: 311.126430\n",
      "Iteration  793 => Weight: 0.793000 => Loss: 310.666428\n",
      "Iteration  794 => Weight: 0.794000 => Loss: 310.206863\n",
      "Iteration  795 => Weight: 0.795000 => Loss: 309.747737\n",
      "Iteration  796 => Weight: 0.796000 => Loss: 309.289047\n",
      "Iteration  797 => Weight: 0.797000 => Loss: 308.830796\n",
      "Iteration  798 => Weight: 0.798000 => Loss: 308.372982\n",
      "Iteration  799 => Weight: 0.799000 => Loss: 307.915605\n",
      "Iteration  800 => Weight: 0.800000 => Loss: 307.458667\n",
      "Iteration  801 => Weight: 0.801000 => Loss: 307.002165\n",
      "Iteration  802 => Weight: 0.802000 => Loss: 306.546102\n",
      "Iteration  803 => Weight: 0.803000 => Loss: 306.090476\n",
      "Iteration  804 => Weight: 0.804000 => Loss: 305.635287\n",
      "Iteration  805 => Weight: 0.805000 => Loss: 305.180537\n",
      "Iteration  806 => Weight: 0.806000 => Loss: 304.726223\n",
      "Iteration  807 => Weight: 0.807000 => Loss: 304.272348\n",
      "Iteration  808 => Weight: 0.808000 => Loss: 303.818910\n",
      "Iteration  809 => Weight: 0.809000 => Loss: 303.365909\n",
      "Iteration  810 => Weight: 0.810000 => Loss: 302.913347\n",
      "Iteration  811 => Weight: 0.811000 => Loss: 302.461221\n",
      "Iteration  812 => Weight: 0.812000 => Loss: 302.009534\n",
      "Iteration  813 => Weight: 0.813000 => Loss: 301.558284\n",
      "Iteration  814 => Weight: 0.814000 => Loss: 301.107471\n",
      "Iteration  815 => Weight: 0.815000 => Loss: 300.657097\n",
      "Iteration  816 => Weight: 0.816000 => Loss: 300.207159\n",
      "Iteration  817 => Weight: 0.817000 => Loss: 299.757660\n",
      "Iteration  818 => Weight: 0.818000 => Loss: 299.308598\n",
      "Iteration  819 => Weight: 0.819000 => Loss: 298.859973\n",
      "Iteration  820 => Weight: 0.820000 => Loss: 298.411787\n",
      "Iteration  821 => Weight: 0.821000 => Loss: 297.964037\n",
      "Iteration  822 => Weight: 0.822000 => Loss: 297.516726\n",
      "Iteration  823 => Weight: 0.823000 => Loss: 297.069852\n",
      "Iteration  824 => Weight: 0.824000 => Loss: 296.623415\n",
      "Iteration  825 => Weight: 0.825000 => Loss: 296.177417\n",
      "Iteration  826 => Weight: 0.826000 => Loss: 295.731855\n",
      "Iteration  827 => Weight: 0.827000 => Loss: 295.286732\n",
      "Iteration  828 => Weight: 0.828000 => Loss: 294.842046\n",
      "Iteration  829 => Weight: 0.829000 => Loss: 294.397797\n",
      "Iteration  830 => Weight: 0.830000 => Loss: 293.953987\n",
      "Iteration  831 => Weight: 0.831000 => Loss: 293.510613\n",
      "Iteration  832 => Weight: 0.832000 => Loss: 293.067678\n",
      "Iteration  833 => Weight: 0.833000 => Loss: 292.625180\n",
      "Iteration  834 => Weight: 0.834000 => Loss: 292.183119\n",
      "Iteration  835 => Weight: 0.835000 => Loss: 291.741497\n",
      "Iteration  836 => Weight: 0.836000 => Loss: 291.300311\n",
      "Iteration  837 => Weight: 0.837000 => Loss: 290.859564\n",
      "Iteration  838 => Weight: 0.838000 => Loss: 290.419254\n",
      "Iteration  839 => Weight: 0.839000 => Loss: 289.979381\n",
      "Iteration  840 => Weight: 0.840000 => Loss: 289.539947\n",
      "Iteration  841 => Weight: 0.841000 => Loss: 289.100949\n",
      "Iteration  842 => Weight: 0.842000 => Loss: 288.662390\n",
      "Iteration  843 => Weight: 0.843000 => Loss: 288.224268\n",
      "Iteration  844 => Weight: 0.844000 => Loss: 287.786583\n",
      "Iteration  845 => Weight: 0.845000 => Loss: 287.349337\n",
      "Iteration  846 => Weight: 0.846000 => Loss: 286.912527\n",
      "Iteration  847 => Weight: 0.847000 => Loss: 286.476156\n",
      "Iteration  848 => Weight: 0.848000 => Loss: 286.040222\n",
      "Iteration  849 => Weight: 0.849000 => Loss: 285.604725\n",
      "Iteration  850 => Weight: 0.850000 => Loss: 285.169667\n",
      "Iteration  851 => Weight: 0.851000 => Loss: 284.735045\n",
      "Iteration  852 => Weight: 0.852000 => Loss: 284.300862\n",
      "Iteration  853 => Weight: 0.853000 => Loss: 283.867116\n",
      "Iteration  854 => Weight: 0.854000 => Loss: 283.433807\n",
      "Iteration  855 => Weight: 0.855000 => Loss: 283.000937\n",
      "Iteration  856 => Weight: 0.856000 => Loss: 282.568503\n",
      "Iteration  857 => Weight: 0.857000 => Loss: 282.136508\n",
      "Iteration  858 => Weight: 0.858000 => Loss: 281.704950\n",
      "Iteration  859 => Weight: 0.859000 => Loss: 281.273829\n",
      "Iteration  860 => Weight: 0.860000 => Loss: 280.843147\n",
      "Iteration  861 => Weight: 0.861000 => Loss: 280.412901\n",
      "Iteration  862 => Weight: 0.862000 => Loss: 279.983094\n",
      "Iteration  863 => Weight: 0.863000 => Loss: 279.553724\n",
      "Iteration  864 => Weight: 0.864000 => Loss: 279.124791\n",
      "Iteration  865 => Weight: 0.865000 => Loss: 278.696297\n",
      "Iteration  866 => Weight: 0.866000 => Loss: 278.268239\n",
      "Iteration  867 => Weight: 0.867000 => Loss: 277.840620\n",
      "Iteration  868 => Weight: 0.868000 => Loss: 277.413438\n",
      "Iteration  869 => Weight: 0.869000 => Loss: 276.986693\n",
      "Iteration  870 => Weight: 0.870000 => Loss: 276.560387\n",
      "Iteration  871 => Weight: 0.871000 => Loss: 276.134517\n",
      "Iteration  872 => Weight: 0.872000 => Loss: 275.709086\n",
      "Iteration  873 => Weight: 0.873000 => Loss: 275.284092\n",
      "Iteration  874 => Weight: 0.874000 => Loss: 274.859535\n",
      "Iteration  875 => Weight: 0.875000 => Loss: 274.435417\n",
      "Iteration  876 => Weight: 0.876000 => Loss: 274.011735\n",
      "Iteration  877 => Weight: 0.877000 => Loss: 273.588492\n",
      "Iteration  878 => Weight: 0.878000 => Loss: 273.165686\n",
      "Iteration  879 => Weight: 0.879000 => Loss: 272.743317\n",
      "Iteration  880 => Weight: 0.880000 => Loss: 272.321387\n",
      "Iteration  881 => Weight: 0.881000 => Loss: 271.899893\n",
      "Iteration  882 => Weight: 0.882000 => Loss: 271.478838\n",
      "Iteration  883 => Weight: 0.883000 => Loss: 271.058220\n",
      "Iteration  884 => Weight: 0.884000 => Loss: 270.638039\n",
      "Iteration  885 => Weight: 0.885000 => Loss: 270.218297\n",
      "Iteration  886 => Weight: 0.886000 => Loss: 269.798991\n",
      "Iteration  887 => Weight: 0.887000 => Loss: 269.380124\n",
      "Iteration  888 => Weight: 0.888000 => Loss: 268.961694\n",
      "Iteration  889 => Weight: 0.889000 => Loss: 268.543701\n",
      "Iteration  890 => Weight: 0.890000 => Loss: 268.126147\n",
      "Iteration  891 => Weight: 0.891000 => Loss: 267.709029\n",
      "Iteration  892 => Weight: 0.892000 => Loss: 267.292350\n",
      "Iteration  893 => Weight: 0.893000 => Loss: 266.876108\n",
      "Iteration  894 => Weight: 0.894000 => Loss: 266.460303\n",
      "Iteration  895 => Weight: 0.895000 => Loss: 266.044937\n",
      "Iteration  896 => Weight: 0.896000 => Loss: 265.630007\n",
      "Iteration  897 => Weight: 0.897000 => Loss: 265.215516\n",
      "Iteration  898 => Weight: 0.898000 => Loss: 264.801462\n",
      "Iteration  899 => Weight: 0.899000 => Loss: 264.387845\n",
      "Iteration  900 => Weight: 0.900000 => Loss: 263.974667\n",
      "Iteration  901 => Weight: 0.901000 => Loss: 263.561925\n",
      "Iteration  902 => Weight: 0.902000 => Loss: 263.149622\n",
      "Iteration  903 => Weight: 0.903000 => Loss: 262.737756\n",
      "Iteration  904 => Weight: 0.904000 => Loss: 262.326327\n",
      "Iteration  905 => Weight: 0.905000 => Loss: 261.915337\n",
      "Iteration  906 => Weight: 0.906000 => Loss: 261.504783\n",
      "Iteration  907 => Weight: 0.907000 => Loss: 261.094668\n",
      "Iteration  908 => Weight: 0.908000 => Loss: 260.684990\n",
      "Iteration  909 => Weight: 0.909000 => Loss: 260.275749\n",
      "Iteration  910 => Weight: 0.910000 => Loss: 259.866947\n",
      "Iteration  911 => Weight: 0.911000 => Loss: 259.458581\n",
      "Iteration  912 => Weight: 0.912000 => Loss: 259.050654\n",
      "Iteration  913 => Weight: 0.913000 => Loss: 258.643164\n",
      "Iteration  914 => Weight: 0.914000 => Loss: 258.236111\n",
      "Iteration  915 => Weight: 0.915000 => Loss: 257.829497\n",
      "Iteration  916 => Weight: 0.916000 => Loss: 257.423319\n",
      "Iteration  917 => Weight: 0.917000 => Loss: 257.017580\n",
      "Iteration  918 => Weight: 0.918000 => Loss: 256.612278\n",
      "Iteration  919 => Weight: 0.919000 => Loss: 256.207413\n",
      "Iteration  920 => Weight: 0.920000 => Loss: 255.802987\n",
      "Iteration  921 => Weight: 0.921000 => Loss: 255.398997\n",
      "Iteration  922 => Weight: 0.922000 => Loss: 254.995446\n",
      "Iteration  923 => Weight: 0.923000 => Loss: 254.592332\n",
      "Iteration  924 => Weight: 0.924000 => Loss: 254.189655\n",
      "Iteration  925 => Weight: 0.925000 => Loss: 253.787417\n",
      "Iteration  926 => Weight: 0.926000 => Loss: 253.385615\n",
      "Iteration  927 => Weight: 0.927000 => Loss: 252.984252\n",
      "Iteration  928 => Weight: 0.928000 => Loss: 252.583326\n",
      "Iteration  929 => Weight: 0.929000 => Loss: 252.182837\n",
      "Iteration  930 => Weight: 0.930000 => Loss: 251.782787\n",
      "Iteration  931 => Weight: 0.931000 => Loss: 251.383173\n",
      "Iteration  932 => Weight: 0.932000 => Loss: 250.983998\n",
      "Iteration  933 => Weight: 0.933000 => Loss: 250.585260\n",
      "Iteration  934 => Weight: 0.934000 => Loss: 250.186959\n",
      "Iteration  935 => Weight: 0.935000 => Loss: 249.789097\n",
      "Iteration  936 => Weight: 0.936000 => Loss: 249.391671\n",
      "Iteration  937 => Weight: 0.937000 => Loss: 248.994684\n",
      "Iteration  938 => Weight: 0.938000 => Loss: 248.598134\n",
      "Iteration  939 => Weight: 0.939000 => Loss: 248.202021\n",
      "Iteration  940 => Weight: 0.940000 => Loss: 247.806347\n",
      "Iteration  941 => Weight: 0.941000 => Loss: 247.411109\n",
      "Iteration  942 => Weight: 0.942000 => Loss: 247.016310\n",
      "Iteration  943 => Weight: 0.943000 => Loss: 246.621948\n",
      "Iteration  944 => Weight: 0.944000 => Loss: 246.228023\n",
      "Iteration  945 => Weight: 0.945000 => Loss: 245.834537\n",
      "Iteration  946 => Weight: 0.946000 => Loss: 245.441487\n",
      "Iteration  947 => Weight: 0.947000 => Loss: 245.048876\n",
      "Iteration  948 => Weight: 0.948000 => Loss: 244.656702\n",
      "Iteration  949 => Weight: 0.949000 => Loss: 244.264965\n",
      "Iteration  950 => Weight: 0.950000 => Loss: 243.873667\n",
      "Iteration  951 => Weight: 0.951000 => Loss: 243.482805\n",
      "Iteration  952 => Weight: 0.952000 => Loss: 243.092382\n",
      "Iteration  953 => Weight: 0.953000 => Loss: 242.702396\n",
      "Iteration  954 => Weight: 0.954000 => Loss: 242.312847\n",
      "Iteration  955 => Weight: 0.955000 => Loss: 241.923737\n",
      "Iteration  956 => Weight: 0.956000 => Loss: 241.535063\n",
      "Iteration  957 => Weight: 0.957000 => Loss: 241.146828\n",
      "Iteration  958 => Weight: 0.958000 => Loss: 240.759030\n",
      "Iteration  959 => Weight: 0.959000 => Loss: 240.371669\n",
      "Iteration  960 => Weight: 0.960000 => Loss: 239.984747\n",
      "Iteration  961 => Weight: 0.961000 => Loss: 239.598261\n",
      "Iteration  962 => Weight: 0.962000 => Loss: 239.212214\n",
      "Iteration  963 => Weight: 0.963000 => Loss: 238.826604\n",
      "Iteration  964 => Weight: 0.964000 => Loss: 238.441431\n",
      "Iteration  965 => Weight: 0.965000 => Loss: 238.056697\n",
      "Iteration  966 => Weight: 0.966000 => Loss: 237.672399\n",
      "Iteration  967 => Weight: 0.967000 => Loss: 237.288540\n",
      "Iteration  968 => Weight: 0.968000 => Loss: 236.905118\n",
      "Iteration  969 => Weight: 0.969000 => Loss: 236.522133\n",
      "Iteration  970 => Weight: 0.970000 => Loss: 236.139587\n",
      "Iteration  971 => Weight: 0.971000 => Loss: 235.757477\n",
      "Iteration  972 => Weight: 0.972000 => Loss: 235.375806\n",
      "Iteration  973 => Weight: 0.973000 => Loss: 234.994572\n",
      "Iteration  974 => Weight: 0.974000 => Loss: 234.613775\n",
      "Iteration  975 => Weight: 0.975000 => Loss: 234.233417\n",
      "Iteration  976 => Weight: 0.976000 => Loss: 233.853495\n",
      "Iteration  977 => Weight: 0.977000 => Loss: 233.474012\n",
      "Iteration  978 => Weight: 0.978000 => Loss: 233.094966\n",
      "Iteration  979 => Weight: 0.979000 => Loss: 232.716357\n",
      "Iteration  980 => Weight: 0.980000 => Loss: 232.338187\n",
      "Iteration  981 => Weight: 0.981000 => Loss: 231.960453\n",
      "Iteration  982 => Weight: 0.982000 => Loss: 231.583158\n",
      "Iteration  983 => Weight: 0.983000 => Loss: 231.206300\n",
      "Iteration  984 => Weight: 0.984000 => Loss: 230.829879\n",
      "Iteration  985 => Weight: 0.985000 => Loss: 230.453897\n",
      "Iteration  986 => Weight: 0.986000 => Loss: 230.078351\n",
      "Iteration  987 => Weight: 0.987000 => Loss: 229.703244\n",
      "Iteration  988 => Weight: 0.988000 => Loss: 229.328574\n",
      "Iteration  989 => Weight: 0.989000 => Loss: 228.954341\n",
      "Iteration  990 => Weight: 0.990000 => Loss: 228.580547\n",
      "Iteration  991 => Weight: 0.991000 => Loss: 228.207189\n",
      "Iteration  992 => Weight: 0.992000 => Loss: 227.834270\n",
      "Iteration  993 => Weight: 0.993000 => Loss: 227.461788\n",
      "Iteration  994 => Weight: 0.994000 => Loss: 227.089743\n",
      "Iteration  995 => Weight: 0.995000 => Loss: 226.718137\n",
      "Iteration  996 => Weight: 0.996000 => Loss: 226.346967\n",
      "Iteration  997 => Weight: 0.997000 => Loss: 225.976236\n",
      "Iteration  998 => Weight: 0.998000 => Loss: 225.605942\n",
      "Iteration  999 => Weight: 0.999000 => Loss: 225.236085\n",
      "Iteration 1000 => Weight: 1.000000 => Loss: 224.866667\n",
      "Iteration 1001 => Weight: 1.001000 => Loss: 224.497685\n",
      "Iteration 1002 => Weight: 1.002000 => Loss: 224.129142\n",
      "Iteration 1003 => Weight: 1.003000 => Loss: 223.761036\n",
      "Iteration 1004 => Weight: 1.004000 => Loss: 223.393367\n",
      "Iteration 1005 => Weight: 1.005000 => Loss: 223.026137\n",
      "Iteration 1006 => Weight: 1.006000 => Loss: 222.659343\n",
      "Iteration 1007 => Weight: 1.007000 => Loss: 222.292988\n",
      "Iteration 1008 => Weight: 1.008000 => Loss: 221.927070\n",
      "Iteration 1009 => Weight: 1.009000 => Loss: 221.561589\n",
      "Iteration 1010 => Weight: 1.010000 => Loss: 221.196547\n",
      "Iteration 1011 => Weight: 1.011000 => Loss: 220.831941\n",
      "Iteration 1012 => Weight: 1.012000 => Loss: 220.467774\n",
      "Iteration 1013 => Weight: 1.013000 => Loss: 220.104044\n",
      "Iteration 1014 => Weight: 1.014000 => Loss: 219.740751\n",
      "Iteration 1015 => Weight: 1.015000 => Loss: 219.377897\n",
      "Iteration 1016 => Weight: 1.016000 => Loss: 219.015479\n",
      "Iteration 1017 => Weight: 1.017000 => Loss: 218.653500\n",
      "Iteration 1018 => Weight: 1.018000 => Loss: 218.291958\n",
      "Iteration 1019 => Weight: 1.019000 => Loss: 217.930853\n",
      "Iteration 1020 => Weight: 1.020000 => Loss: 217.570187\n",
      "Iteration 1021 => Weight: 1.021000 => Loss: 217.209957\n",
      "Iteration 1022 => Weight: 1.022000 => Loss: 216.850166\n",
      "Iteration 1023 => Weight: 1.023000 => Loss: 216.490812\n",
      "Iteration 1024 => Weight: 1.024000 => Loss: 216.131895\n",
      "Iteration 1025 => Weight: 1.025000 => Loss: 215.773417\n",
      "Iteration 1026 => Weight: 1.026000 => Loss: 215.415375\n",
      "Iteration 1027 => Weight: 1.027000 => Loss: 215.057772\n",
      "Iteration 1028 => Weight: 1.028000 => Loss: 214.700606\n",
      "Iteration 1029 => Weight: 1.029000 => Loss: 214.343877\n",
      "Iteration 1030 => Weight: 1.030000 => Loss: 213.987587\n",
      "Iteration 1031 => Weight: 1.031000 => Loss: 213.631733\n",
      "Iteration 1032 => Weight: 1.032000 => Loss: 213.276318\n",
      "Iteration 1033 => Weight: 1.033000 => Loss: 212.921340\n",
      "Iteration 1034 => Weight: 1.034000 => Loss: 212.566799\n",
      "Iteration 1035 => Weight: 1.035000 => Loss: 212.212697\n",
      "Iteration 1036 => Weight: 1.036000 => Loss: 211.859031\n",
      "Iteration 1037 => Weight: 1.037000 => Loss: 211.505804\n",
      "Iteration 1038 => Weight: 1.038000 => Loss: 211.153014\n",
      "Iteration 1039 => Weight: 1.039000 => Loss: 210.800661\n",
      "Iteration 1040 => Weight: 1.040000 => Loss: 210.448747\n",
      "Iteration 1041 => Weight: 1.041000 => Loss: 210.097269\n",
      "Iteration 1042 => Weight: 1.042000 => Loss: 209.746230\n",
      "Iteration 1043 => Weight: 1.043000 => Loss: 209.395628\n",
      "Iteration 1044 => Weight: 1.044000 => Loss: 209.045463\n",
      "Iteration 1045 => Weight: 1.045000 => Loss: 208.695737\n",
      "Iteration 1046 => Weight: 1.046000 => Loss: 208.346447\n",
      "Iteration 1047 => Weight: 1.047000 => Loss: 207.997596\n",
      "Iteration 1048 => Weight: 1.048000 => Loss: 207.649182\n",
      "Iteration 1049 => Weight: 1.049000 => Loss: 207.301205\n",
      "Iteration 1050 => Weight: 1.050000 => Loss: 206.953667\n",
      "Iteration 1051 => Weight: 1.051000 => Loss: 206.606565\n",
      "Iteration 1052 => Weight: 1.052000 => Loss: 206.259902\n",
      "Iteration 1053 => Weight: 1.053000 => Loss: 205.913676\n",
      "Iteration 1054 => Weight: 1.054000 => Loss: 205.567887\n",
      "Iteration 1055 => Weight: 1.055000 => Loss: 205.222537\n",
      "Iteration 1056 => Weight: 1.056000 => Loss: 204.877623\n",
      "Iteration 1057 => Weight: 1.057000 => Loss: 204.533148\n",
      "Iteration 1058 => Weight: 1.058000 => Loss: 204.189110\n",
      "Iteration 1059 => Weight: 1.059000 => Loss: 203.845509\n",
      "Iteration 1060 => Weight: 1.060000 => Loss: 203.502347\n",
      "Iteration 1061 => Weight: 1.061000 => Loss: 203.159621\n",
      "Iteration 1062 => Weight: 1.062000 => Loss: 202.817334\n",
      "Iteration 1063 => Weight: 1.063000 => Loss: 202.475484\n",
      "Iteration 1064 => Weight: 1.064000 => Loss: 202.134071\n",
      "Iteration 1065 => Weight: 1.065000 => Loss: 201.793097\n",
      "Iteration 1066 => Weight: 1.066000 => Loss: 201.452559\n",
      "Iteration 1067 => Weight: 1.067000 => Loss: 201.112460\n",
      "Iteration 1068 => Weight: 1.068000 => Loss: 200.772798\n",
      "Iteration 1069 => Weight: 1.069000 => Loss: 200.433573\n",
      "Iteration 1070 => Weight: 1.070000 => Loss: 200.094787\n",
      "Iteration 1071 => Weight: 1.071000 => Loss: 199.756437\n",
      "Iteration 1072 => Weight: 1.072000 => Loss: 199.418526\n",
      "Iteration 1073 => Weight: 1.073000 => Loss: 199.081052\n",
      "Iteration 1074 => Weight: 1.074000 => Loss: 198.744015\n",
      "Iteration 1075 => Weight: 1.075000 => Loss: 198.407417\n",
      "Iteration 1076 => Weight: 1.076000 => Loss: 198.071255\n",
      "Iteration 1077 => Weight: 1.077000 => Loss: 197.735532\n",
      "Iteration 1078 => Weight: 1.078000 => Loss: 197.400246\n",
      "Iteration 1079 => Weight: 1.079000 => Loss: 197.065397\n",
      "Iteration 1080 => Weight: 1.080000 => Loss: 196.730987\n",
      "Iteration 1081 => Weight: 1.081000 => Loss: 196.397013\n",
      "Iteration 1082 => Weight: 1.082000 => Loss: 196.063478\n",
      "Iteration 1083 => Weight: 1.083000 => Loss: 195.730380\n",
      "Iteration 1084 => Weight: 1.084000 => Loss: 195.397719\n",
      "Iteration 1085 => Weight: 1.085000 => Loss: 195.065497\n",
      "Iteration 1086 => Weight: 1.086000 => Loss: 194.733711\n",
      "Iteration 1087 => Weight: 1.087000 => Loss: 194.402364\n",
      "Iteration 1088 => Weight: 1.088000 => Loss: 194.071454\n",
      "Iteration 1089 => Weight: 1.089000 => Loss: 193.740981\n",
      "Iteration 1090 => Weight: 1.090000 => Loss: 193.410947\n",
      "Iteration 1091 => Weight: 1.091000 => Loss: 193.081349\n",
      "Iteration 1092 => Weight: 1.092000 => Loss: 192.752190\n",
      "Iteration 1093 => Weight: 1.093000 => Loss: 192.423468\n",
      "Iteration 1094 => Weight: 1.094000 => Loss: 192.095183\n",
      "Iteration 1095 => Weight: 1.095000 => Loss: 191.767337\n",
      "Iteration 1096 => Weight: 1.096000 => Loss: 191.439927\n",
      "Iteration 1097 => Weight: 1.097000 => Loss: 191.112956\n",
      "Iteration 1098 => Weight: 1.098000 => Loss: 190.786422\n",
      "Iteration 1099 => Weight: 1.099000 => Loss: 190.460325\n",
      "Iteration 1100 => Weight: 1.100000 => Loss: 190.134667\n",
      "Iteration 1101 => Weight: 1.101000 => Loss: 189.809445\n",
      "Iteration 1102 => Weight: 1.102000 => Loss: 189.484662\n",
      "Iteration 1103 => Weight: 1.103000 => Loss: 189.160316\n",
      "Iteration 1104 => Weight: 1.104000 => Loss: 188.836407\n",
      "Iteration 1105 => Weight: 1.105000 => Loss: 188.512937\n",
      "Iteration 1106 => Weight: 1.106000 => Loss: 188.189903\n",
      "Iteration 1107 => Weight: 1.107000 => Loss: 187.867308\n",
      "Iteration 1108 => Weight: 1.108000 => Loss: 187.545150\n",
      "Iteration 1109 => Weight: 1.109000 => Loss: 187.223429\n",
      "Iteration 1110 => Weight: 1.110000 => Loss: 186.902147\n",
      "Iteration 1111 => Weight: 1.111000 => Loss: 186.581301\n",
      "Iteration 1112 => Weight: 1.112000 => Loss: 186.260894\n",
      "Iteration 1113 => Weight: 1.113000 => Loss: 185.940924\n",
      "Iteration 1114 => Weight: 1.114000 => Loss: 185.621391\n",
      "Iteration 1115 => Weight: 1.115000 => Loss: 185.302297\n",
      "Iteration 1116 => Weight: 1.116000 => Loss: 184.983639\n",
      "Iteration 1117 => Weight: 1.117000 => Loss: 184.665420\n",
      "Iteration 1118 => Weight: 1.118000 => Loss: 184.347638\n",
      "Iteration 1119 => Weight: 1.119000 => Loss: 184.030293\n",
      "Iteration 1120 => Weight: 1.120000 => Loss: 183.713387\n",
      "Iteration 1121 => Weight: 1.121000 => Loss: 183.396917\n",
      "Iteration 1122 => Weight: 1.122000 => Loss: 183.080886\n",
      "Iteration 1123 => Weight: 1.123000 => Loss: 182.765292\n",
      "Iteration 1124 => Weight: 1.124000 => Loss: 182.450135\n",
      "Iteration 1125 => Weight: 1.125000 => Loss: 182.135417\n",
      "Iteration 1126 => Weight: 1.126000 => Loss: 181.821135\n",
      "Iteration 1127 => Weight: 1.127000 => Loss: 181.507292\n",
      "Iteration 1128 => Weight: 1.128000 => Loss: 181.193886\n",
      "Iteration 1129 => Weight: 1.129000 => Loss: 180.880917\n",
      "Iteration 1130 => Weight: 1.130000 => Loss: 180.568387\n",
      "Iteration 1131 => Weight: 1.131000 => Loss: 180.256293\n",
      "Iteration 1132 => Weight: 1.132000 => Loss: 179.944638\n",
      "Iteration 1133 => Weight: 1.133000 => Loss: 179.633420\n",
      "Iteration 1134 => Weight: 1.134000 => Loss: 179.322639\n",
      "Iteration 1135 => Weight: 1.135000 => Loss: 179.012297\n",
      "Iteration 1136 => Weight: 1.136000 => Loss: 178.702391\n",
      "Iteration 1137 => Weight: 1.137000 => Loss: 178.392924\n",
      "Iteration 1138 => Weight: 1.138000 => Loss: 178.083894\n",
      "Iteration 1139 => Weight: 1.139000 => Loss: 177.775301\n",
      "Iteration 1140 => Weight: 1.140000 => Loss: 177.467147\n",
      "Iteration 1141 => Weight: 1.141000 => Loss: 177.159429\n",
      "Iteration 1142 => Weight: 1.142000 => Loss: 176.852150\n",
      "Iteration 1143 => Weight: 1.143000 => Loss: 176.545308\n",
      "Iteration 1144 => Weight: 1.144000 => Loss: 176.238903\n",
      "Iteration 1145 => Weight: 1.145000 => Loss: 175.932937\n",
      "Iteration 1146 => Weight: 1.146000 => Loss: 175.627407\n",
      "Iteration 1147 => Weight: 1.147000 => Loss: 175.322316\n",
      "Iteration 1148 => Weight: 1.148000 => Loss: 175.017662\n",
      "Iteration 1149 => Weight: 1.149000 => Loss: 174.713445\n",
      "Iteration 1150 => Weight: 1.150000 => Loss: 174.409667\n",
      "Iteration 1151 => Weight: 1.151000 => Loss: 174.106325\n",
      "Iteration 1152 => Weight: 1.152000 => Loss: 173.803422\n",
      "Iteration 1153 => Weight: 1.153000 => Loss: 173.500956\n",
      "Iteration 1154 => Weight: 1.154000 => Loss: 173.198927\n",
      "Iteration 1155 => Weight: 1.155000 => Loss: 172.897337\n",
      "Iteration 1156 => Weight: 1.156000 => Loss: 172.596183\n",
      "Iteration 1157 => Weight: 1.157000 => Loss: 172.295468\n",
      "Iteration 1158 => Weight: 1.158000 => Loss: 171.995190\n",
      "Iteration 1159 => Weight: 1.159000 => Loss: 171.695349\n",
      "Iteration 1160 => Weight: 1.160000 => Loss: 171.395947\n",
      "Iteration 1161 => Weight: 1.161000 => Loss: 171.096981\n",
      "Iteration 1162 => Weight: 1.162000 => Loss: 170.798454\n",
      "Iteration 1163 => Weight: 1.163000 => Loss: 170.500364\n",
      "Iteration 1164 => Weight: 1.164000 => Loss: 170.202711\n",
      "Iteration 1165 => Weight: 1.165000 => Loss: 169.905497\n",
      "Iteration 1166 => Weight: 1.166000 => Loss: 169.608719\n",
      "Iteration 1167 => Weight: 1.167000 => Loss: 169.312380\n",
      "Iteration 1168 => Weight: 1.168000 => Loss: 169.016478\n",
      "Iteration 1169 => Weight: 1.169000 => Loss: 168.721013\n",
      "Iteration 1170 => Weight: 1.170000 => Loss: 168.425987\n",
      "Iteration 1171 => Weight: 1.171000 => Loss: 168.131397\n",
      "Iteration 1172 => Weight: 1.172000 => Loss: 167.837246\n",
      "Iteration 1173 => Weight: 1.173000 => Loss: 167.543532\n",
      "Iteration 1174 => Weight: 1.174000 => Loss: 167.250255\n",
      "Iteration 1175 => Weight: 1.175000 => Loss: 166.957417\n",
      "Iteration 1176 => Weight: 1.176000 => Loss: 166.665015\n",
      "Iteration 1177 => Weight: 1.177000 => Loss: 166.373052\n",
      "Iteration 1178 => Weight: 1.178000 => Loss: 166.081526\n",
      "Iteration 1179 => Weight: 1.179000 => Loss: 165.790437\n",
      "Iteration 1180 => Weight: 1.180000 => Loss: 165.499787\n",
      "Iteration 1181 => Weight: 1.181000 => Loss: 165.209573\n",
      "Iteration 1182 => Weight: 1.182000 => Loss: 164.919798\n",
      "Iteration 1183 => Weight: 1.183000 => Loss: 164.630460\n",
      "Iteration 1184 => Weight: 1.184000 => Loss: 164.341559\n",
      "Iteration 1185 => Weight: 1.185000 => Loss: 164.053097\n",
      "Iteration 1186 => Weight: 1.186000 => Loss: 163.765071\n",
      "Iteration 1187 => Weight: 1.187000 => Loss: 163.477484\n",
      "Iteration 1188 => Weight: 1.188000 => Loss: 163.190334\n",
      "Iteration 1189 => Weight: 1.189000 => Loss: 162.903621\n",
      "Iteration 1190 => Weight: 1.190000 => Loss: 162.617347\n",
      "Iteration 1191 => Weight: 1.191000 => Loss: 162.331509\n",
      "Iteration 1192 => Weight: 1.192000 => Loss: 162.046110\n",
      "Iteration 1193 => Weight: 1.193000 => Loss: 161.761148\n",
      "Iteration 1194 => Weight: 1.194000 => Loss: 161.476623\n",
      "Iteration 1195 => Weight: 1.195000 => Loss: 161.192537\n",
      "Iteration 1196 => Weight: 1.196000 => Loss: 160.908887\n",
      "Iteration 1197 => Weight: 1.197000 => Loss: 160.625676\n",
      "Iteration 1198 => Weight: 1.198000 => Loss: 160.342902\n",
      "Iteration 1199 => Weight: 1.199000 => Loss: 160.060565\n",
      "Iteration 1200 => Weight: 1.200000 => Loss: 159.778667\n",
      "Iteration 1201 => Weight: 1.201000 => Loss: 159.497205\n",
      "Iteration 1202 => Weight: 1.202000 => Loss: 159.216182\n",
      "Iteration 1203 => Weight: 1.203000 => Loss: 158.935596\n",
      "Iteration 1204 => Weight: 1.204000 => Loss: 158.655447\n",
      "Iteration 1205 => Weight: 1.205000 => Loss: 158.375737\n",
      "Iteration 1206 => Weight: 1.206000 => Loss: 158.096463\n",
      "Iteration 1207 => Weight: 1.207000 => Loss: 157.817628\n",
      "Iteration 1208 => Weight: 1.208000 => Loss: 157.539230\n",
      "Iteration 1209 => Weight: 1.209000 => Loss: 157.261269\n",
      "Iteration 1210 => Weight: 1.210000 => Loss: 156.983747\n",
      "Iteration 1211 => Weight: 1.211000 => Loss: 156.706661\n",
      "Iteration 1212 => Weight: 1.212000 => Loss: 156.430014\n",
      "Iteration 1213 => Weight: 1.213000 => Loss: 156.153804\n",
      "Iteration 1214 => Weight: 1.214000 => Loss: 155.878031\n",
      "Iteration 1215 => Weight: 1.215000 => Loss: 155.602697\n",
      "Iteration 1216 => Weight: 1.216000 => Loss: 155.327799\n",
      "Iteration 1217 => Weight: 1.217000 => Loss: 155.053340\n",
      "Iteration 1218 => Weight: 1.218000 => Loss: 154.779318\n",
      "Iteration 1219 => Weight: 1.219000 => Loss: 154.505733\n",
      "Iteration 1220 => Weight: 1.220000 => Loss: 154.232587\n",
      "Iteration 1221 => Weight: 1.221000 => Loss: 153.959877\n",
      "Iteration 1222 => Weight: 1.222000 => Loss: 153.687606\n",
      "Iteration 1223 => Weight: 1.223000 => Loss: 153.415772\n",
      "Iteration 1224 => Weight: 1.224000 => Loss: 153.144375\n",
      "Iteration 1225 => Weight: 1.225000 => Loss: 152.873417\n",
      "Iteration 1226 => Weight: 1.226000 => Loss: 152.602895\n",
      "Iteration 1227 => Weight: 1.227000 => Loss: 152.332812\n",
      "Iteration 1228 => Weight: 1.228000 => Loss: 152.063166\n",
      "Iteration 1229 => Weight: 1.229000 => Loss: 151.793957\n",
      "Iteration 1230 => Weight: 1.230000 => Loss: 151.525187\n",
      "Iteration 1231 => Weight: 1.231000 => Loss: 151.256853\n",
      "Iteration 1232 => Weight: 1.232000 => Loss: 150.988958\n",
      "Iteration 1233 => Weight: 1.233000 => Loss: 150.721500\n",
      "Iteration 1234 => Weight: 1.234000 => Loss: 150.454479\n",
      "Iteration 1235 => Weight: 1.235000 => Loss: 150.187897\n",
      "Iteration 1236 => Weight: 1.236000 => Loss: 149.921751\n",
      "Iteration 1237 => Weight: 1.237000 => Loss: 149.656044\n",
      "Iteration 1238 => Weight: 1.238000 => Loss: 149.390774\n",
      "Iteration 1239 => Weight: 1.239000 => Loss: 149.125941\n",
      "Iteration 1240 => Weight: 1.240000 => Loss: 148.861547\n",
      "Iteration 1241 => Weight: 1.241000 => Loss: 148.597589\n",
      "Iteration 1242 => Weight: 1.242000 => Loss: 148.334070\n",
      "Iteration 1243 => Weight: 1.243000 => Loss: 148.070988\n",
      "Iteration 1244 => Weight: 1.244000 => Loss: 147.808343\n",
      "Iteration 1245 => Weight: 1.245000 => Loss: 147.546137\n",
      "Iteration 1246 => Weight: 1.246000 => Loss: 147.284367\n",
      "Iteration 1247 => Weight: 1.247000 => Loss: 147.023036\n",
      "Iteration 1248 => Weight: 1.248000 => Loss: 146.762142\n",
      "Iteration 1249 => Weight: 1.249000 => Loss: 146.501685\n",
      "Iteration 1250 => Weight: 1.250000 => Loss: 146.241667\n",
      "Iteration 1251 => Weight: 1.251000 => Loss: 145.982085\n",
      "Iteration 1252 => Weight: 1.252000 => Loss: 145.722942\n",
      "Iteration 1253 => Weight: 1.253000 => Loss: 145.464236\n",
      "Iteration 1254 => Weight: 1.254000 => Loss: 145.205967\n",
      "Iteration 1255 => Weight: 1.255000 => Loss: 144.948137\n",
      "Iteration 1256 => Weight: 1.256000 => Loss: 144.690743\n",
      "Iteration 1257 => Weight: 1.257000 => Loss: 144.433788\n",
      "Iteration 1258 => Weight: 1.258000 => Loss: 144.177270\n",
      "Iteration 1259 => Weight: 1.259000 => Loss: 143.921189\n",
      "Iteration 1260 => Weight: 1.260000 => Loss: 143.665547\n",
      "Iteration 1261 => Weight: 1.261000 => Loss: 143.410341\n",
      "Iteration 1262 => Weight: 1.262000 => Loss: 143.155574\n",
      "Iteration 1263 => Weight: 1.263000 => Loss: 142.901244\n",
      "Iteration 1264 => Weight: 1.264000 => Loss: 142.647351\n",
      "Iteration 1265 => Weight: 1.265000 => Loss: 142.393897\n",
      "Iteration 1266 => Weight: 1.266000 => Loss: 142.140879\n",
      "Iteration 1267 => Weight: 1.267000 => Loss: 141.888300\n",
      "Iteration 1268 => Weight: 1.268000 => Loss: 141.636158\n",
      "Iteration 1269 => Weight: 1.269000 => Loss: 141.384453\n",
      "Iteration 1270 => Weight: 1.270000 => Loss: 141.133187\n",
      "Iteration 1271 => Weight: 1.271000 => Loss: 140.882357\n",
      "Iteration 1272 => Weight: 1.272000 => Loss: 140.631966\n",
      "Iteration 1273 => Weight: 1.273000 => Loss: 140.382012\n",
      "Iteration 1274 => Weight: 1.274000 => Loss: 140.132495\n",
      "Iteration 1275 => Weight: 1.275000 => Loss: 139.883417\n",
      "Iteration 1276 => Weight: 1.276000 => Loss: 139.634775\n",
      "Iteration 1277 => Weight: 1.277000 => Loss: 139.386572\n",
      "Iteration 1278 => Weight: 1.278000 => Loss: 139.138806\n",
      "Iteration 1279 => Weight: 1.279000 => Loss: 138.891477\n",
      "Iteration 1280 => Weight: 1.280000 => Loss: 138.644587\n",
      "Iteration 1281 => Weight: 1.281000 => Loss: 138.398133\n",
      "Iteration 1282 => Weight: 1.282000 => Loss: 138.152118\n",
      "Iteration 1283 => Weight: 1.283000 => Loss: 137.906540\n",
      "Iteration 1284 => Weight: 1.284000 => Loss: 137.661399\n",
      "Iteration 1285 => Weight: 1.285000 => Loss: 137.416697\n",
      "Iteration 1286 => Weight: 1.286000 => Loss: 137.172431\n",
      "Iteration 1287 => Weight: 1.287000 => Loss: 136.928604\n",
      "Iteration 1288 => Weight: 1.288000 => Loss: 136.685214\n",
      "Iteration 1289 => Weight: 1.289000 => Loss: 136.442261\n",
      "Iteration 1290 => Weight: 1.290000 => Loss: 136.199747\n",
      "Iteration 1291 => Weight: 1.291000 => Loss: 135.957669\n",
      "Iteration 1292 => Weight: 1.292000 => Loss: 135.716030\n",
      "Iteration 1293 => Weight: 1.293000 => Loss: 135.474828\n",
      "Iteration 1294 => Weight: 1.294000 => Loss: 135.234063\n",
      "Iteration 1295 => Weight: 1.295000 => Loss: 134.993737\n",
      "Iteration 1296 => Weight: 1.296000 => Loss: 134.753847\n",
      "Iteration 1297 => Weight: 1.297000 => Loss: 134.514396\n",
      "Iteration 1298 => Weight: 1.298000 => Loss: 134.275382\n",
      "Iteration 1299 => Weight: 1.299000 => Loss: 134.036805\n",
      "Iteration 1300 => Weight: 1.300000 => Loss: 133.798667\n",
      "Iteration 1301 => Weight: 1.301000 => Loss: 133.560965\n",
      "Iteration 1302 => Weight: 1.302000 => Loss: 133.323702\n",
      "Iteration 1303 => Weight: 1.303000 => Loss: 133.086876\n",
      "Iteration 1304 => Weight: 1.304000 => Loss: 132.850487\n",
      "Iteration 1305 => Weight: 1.305000 => Loss: 132.614537\n",
      "Iteration 1306 => Weight: 1.306000 => Loss: 132.379023\n",
      "Iteration 1307 => Weight: 1.307000 => Loss: 132.143948\n",
      "Iteration 1308 => Weight: 1.308000 => Loss: 131.909310\n",
      "Iteration 1309 => Weight: 1.309000 => Loss: 131.675109\n",
      "Iteration 1310 => Weight: 1.310000 => Loss: 131.441347\n",
      "Iteration 1311 => Weight: 1.311000 => Loss: 131.208021\n",
      "Iteration 1312 => Weight: 1.312000 => Loss: 130.975134\n",
      "Iteration 1313 => Weight: 1.313000 => Loss: 130.742684\n",
      "Iteration 1314 => Weight: 1.314000 => Loss: 130.510671\n",
      "Iteration 1315 => Weight: 1.315000 => Loss: 130.279097\n",
      "Iteration 1316 => Weight: 1.316000 => Loss: 130.047959\n",
      "Iteration 1317 => Weight: 1.317000 => Loss: 129.817260\n",
      "Iteration 1318 => Weight: 1.318000 => Loss: 129.586998\n",
      "Iteration 1319 => Weight: 1.319000 => Loss: 129.357173\n",
      "Iteration 1320 => Weight: 1.320000 => Loss: 129.127787\n",
      "Iteration 1321 => Weight: 1.321000 => Loss: 128.898837\n",
      "Iteration 1322 => Weight: 1.322000 => Loss: 128.670326\n",
      "Iteration 1323 => Weight: 1.323000 => Loss: 128.442252\n",
      "Iteration 1324 => Weight: 1.324000 => Loss: 128.214615\n",
      "Iteration 1325 => Weight: 1.325000 => Loss: 127.987417\n",
      "Iteration 1326 => Weight: 1.326000 => Loss: 127.760655\n",
      "Iteration 1327 => Weight: 1.327000 => Loss: 127.534332\n",
      "Iteration 1328 => Weight: 1.328000 => Loss: 127.308446\n",
      "Iteration 1329 => Weight: 1.329000 => Loss: 127.082997\n",
      "Iteration 1330 => Weight: 1.330000 => Loss: 126.857987\n",
      "Iteration 1331 => Weight: 1.331000 => Loss: 126.633413\n",
      "Iteration 1332 => Weight: 1.332000 => Loss: 126.409278\n",
      "Iteration 1333 => Weight: 1.333000 => Loss: 126.185580\n",
      "Iteration 1334 => Weight: 1.334000 => Loss: 125.962319\n",
      "Iteration 1335 => Weight: 1.335000 => Loss: 125.739497\n",
      "Iteration 1336 => Weight: 1.336000 => Loss: 125.517111\n",
      "Iteration 1337 => Weight: 1.337000 => Loss: 125.295164\n",
      "Iteration 1338 => Weight: 1.338000 => Loss: 125.073654\n",
      "Iteration 1339 => Weight: 1.339000 => Loss: 124.852581\n",
      "Iteration 1340 => Weight: 1.340000 => Loss: 124.631947\n",
      "Iteration 1341 => Weight: 1.341000 => Loss: 124.411749\n",
      "Iteration 1342 => Weight: 1.342000 => Loss: 124.191990\n",
      "Iteration 1343 => Weight: 1.343000 => Loss: 123.972668\n",
      "Iteration 1344 => Weight: 1.344000 => Loss: 123.753783\n",
      "Iteration 1345 => Weight: 1.345000 => Loss: 123.535337\n",
      "Iteration 1346 => Weight: 1.346000 => Loss: 123.317327\n",
      "Iteration 1347 => Weight: 1.347000 => Loss: 123.099756\n",
      "Iteration 1348 => Weight: 1.348000 => Loss: 122.882622\n",
      "Iteration 1349 => Weight: 1.349000 => Loss: 122.665925\n",
      "Iteration 1350 => Weight: 1.350000 => Loss: 122.449667\n",
      "Iteration 1351 => Weight: 1.351000 => Loss: 122.233845\n",
      "Iteration 1352 => Weight: 1.352000 => Loss: 122.018462\n",
      "Iteration 1353 => Weight: 1.353000 => Loss: 121.803516\n",
      "Iteration 1354 => Weight: 1.354000 => Loss: 121.589007\n",
      "Iteration 1355 => Weight: 1.355000 => Loss: 121.374937\n",
      "Iteration 1356 => Weight: 1.356000 => Loss: 121.161303\n",
      "Iteration 1357 => Weight: 1.357000 => Loss: 120.948108\n",
      "Iteration 1358 => Weight: 1.358000 => Loss: 120.735350\n",
      "Iteration 1359 => Weight: 1.359000 => Loss: 120.523029\n",
      "Iteration 1360 => Weight: 1.360000 => Loss: 120.311147\n",
      "Iteration 1361 => Weight: 1.361000 => Loss: 120.099701\n",
      "Iteration 1362 => Weight: 1.362000 => Loss: 119.888694\n",
      "Iteration 1363 => Weight: 1.363000 => Loss: 119.678124\n",
      "Iteration 1364 => Weight: 1.364000 => Loss: 119.467991\n",
      "Iteration 1365 => Weight: 1.365000 => Loss: 119.258297\n",
      "Iteration 1366 => Weight: 1.366000 => Loss: 119.049039\n",
      "Iteration 1367 => Weight: 1.367000 => Loss: 118.840220\n",
      "Iteration 1368 => Weight: 1.368000 => Loss: 118.631838\n",
      "Iteration 1369 => Weight: 1.369000 => Loss: 118.423893\n",
      "Iteration 1370 => Weight: 1.370000 => Loss: 118.216387\n",
      "Iteration 1371 => Weight: 1.371000 => Loss: 118.009317\n",
      "Iteration 1372 => Weight: 1.372000 => Loss: 117.802686\n",
      "Iteration 1373 => Weight: 1.373000 => Loss: 117.596492\n",
      "Iteration 1374 => Weight: 1.374000 => Loss: 117.390735\n",
      "Iteration 1375 => Weight: 1.375000 => Loss: 117.185417\n",
      "Iteration 1376 => Weight: 1.376000 => Loss: 116.980535\n",
      "Iteration 1377 => Weight: 1.377000 => Loss: 116.776092\n",
      "Iteration 1378 => Weight: 1.378000 => Loss: 116.572086\n",
      "Iteration 1379 => Weight: 1.379000 => Loss: 116.368517\n",
      "Iteration 1380 => Weight: 1.380000 => Loss: 116.165387\n",
      "Iteration 1381 => Weight: 1.381000 => Loss: 115.962693\n",
      "Iteration 1382 => Weight: 1.382000 => Loss: 115.760438\n",
      "Iteration 1383 => Weight: 1.383000 => Loss: 115.558620\n",
      "Iteration 1384 => Weight: 1.384000 => Loss: 115.357239\n",
      "Iteration 1385 => Weight: 1.385000 => Loss: 115.156297\n",
      "Iteration 1386 => Weight: 1.386000 => Loss: 114.955791\n",
      "Iteration 1387 => Weight: 1.387000 => Loss: 114.755724\n",
      "Iteration 1388 => Weight: 1.388000 => Loss: 114.556094\n",
      "Iteration 1389 => Weight: 1.389000 => Loss: 114.356901\n",
      "Iteration 1390 => Weight: 1.390000 => Loss: 114.158147\n",
      "Iteration 1391 => Weight: 1.391000 => Loss: 113.959829\n",
      "Iteration 1392 => Weight: 1.392000 => Loss: 113.761950\n",
      "Iteration 1393 => Weight: 1.393000 => Loss: 113.564508\n",
      "Iteration 1394 => Weight: 1.394000 => Loss: 113.367503\n",
      "Iteration 1395 => Weight: 1.395000 => Loss: 113.170937\n",
      "Iteration 1396 => Weight: 1.396000 => Loss: 112.974807\n",
      "Iteration 1397 => Weight: 1.397000 => Loss: 112.779116\n",
      "Iteration 1398 => Weight: 1.398000 => Loss: 112.583862\n",
      "Iteration 1399 => Weight: 1.399000 => Loss: 112.389045\n",
      "Iteration 1400 => Weight: 1.400000 => Loss: 112.194667\n",
      "Iteration 1401 => Weight: 1.401000 => Loss: 112.000725\n",
      "Iteration 1402 => Weight: 1.402000 => Loss: 111.807222\n",
      "Iteration 1403 => Weight: 1.403000 => Loss: 111.614156\n",
      "Iteration 1404 => Weight: 1.404000 => Loss: 111.421527\n",
      "Iteration 1405 => Weight: 1.405000 => Loss: 111.229337\n",
      "Iteration 1406 => Weight: 1.406000 => Loss: 111.037583\n",
      "Iteration 1407 => Weight: 1.407000 => Loss: 110.846268\n",
      "Iteration 1408 => Weight: 1.408000 => Loss: 110.655390\n",
      "Iteration 1409 => Weight: 1.409000 => Loss: 110.464949\n",
      "Iteration 1410 => Weight: 1.410000 => Loss: 110.274947\n",
      "Iteration 1411 => Weight: 1.411000 => Loss: 110.085381\n",
      "Iteration 1412 => Weight: 1.412000 => Loss: 109.896254\n",
      "Iteration 1413 => Weight: 1.413000 => Loss: 109.707564\n",
      "Iteration 1414 => Weight: 1.414000 => Loss: 109.519311\n",
      "Iteration 1415 => Weight: 1.415000 => Loss: 109.331497\n",
      "Iteration 1416 => Weight: 1.416000 => Loss: 109.144119\n",
      "Iteration 1417 => Weight: 1.417000 => Loss: 108.957180\n",
      "Iteration 1418 => Weight: 1.418000 => Loss: 108.770678\n",
      "Iteration 1419 => Weight: 1.419000 => Loss: 108.584613\n",
      "Iteration 1420 => Weight: 1.420000 => Loss: 108.398987\n",
      "Iteration 1421 => Weight: 1.421000 => Loss: 108.213797\n",
      "Iteration 1422 => Weight: 1.422000 => Loss: 108.029046\n",
      "Iteration 1423 => Weight: 1.423000 => Loss: 107.844732\n",
      "Iteration 1424 => Weight: 1.424000 => Loss: 107.660855\n",
      "Iteration 1425 => Weight: 1.425000 => Loss: 107.477417\n",
      "Iteration 1426 => Weight: 1.426000 => Loss: 107.294415\n",
      "Iteration 1427 => Weight: 1.427000 => Loss: 107.111852\n",
      "Iteration 1428 => Weight: 1.428000 => Loss: 106.929726\n",
      "Iteration 1429 => Weight: 1.429000 => Loss: 106.748037\n",
      "Iteration 1430 => Weight: 1.430000 => Loss: 106.566787\n",
      "Iteration 1431 => Weight: 1.431000 => Loss: 106.385973\n",
      "Iteration 1432 => Weight: 1.432000 => Loss: 106.205598\n",
      "Iteration 1433 => Weight: 1.433000 => Loss: 106.025660\n",
      "Iteration 1434 => Weight: 1.434000 => Loss: 105.846159\n",
      "Iteration 1435 => Weight: 1.435000 => Loss: 105.667097\n",
      "Iteration 1436 => Weight: 1.436000 => Loss: 105.488471\n",
      "Iteration 1437 => Weight: 1.437000 => Loss: 105.310284\n",
      "Iteration 1438 => Weight: 1.438000 => Loss: 105.132534\n",
      "Iteration 1439 => Weight: 1.439000 => Loss: 104.955221\n",
      "Iteration 1440 => Weight: 1.440000 => Loss: 104.778347\n",
      "Iteration 1441 => Weight: 1.441000 => Loss: 104.601909\n",
      "Iteration 1442 => Weight: 1.442000 => Loss: 104.425910\n",
      "Iteration 1443 => Weight: 1.443000 => Loss: 104.250348\n",
      "Iteration 1444 => Weight: 1.444000 => Loss: 104.075223\n",
      "Iteration 1445 => Weight: 1.445000 => Loss: 103.900537\n",
      "Iteration 1446 => Weight: 1.446000 => Loss: 103.726287\n",
      "Iteration 1447 => Weight: 1.447000 => Loss: 103.552476\n",
      "Iteration 1448 => Weight: 1.448000 => Loss: 103.379102\n",
      "Iteration 1449 => Weight: 1.449000 => Loss: 103.206165\n",
      "Iteration 1450 => Weight: 1.450000 => Loss: 103.033667\n",
      "Iteration 1451 => Weight: 1.451000 => Loss: 102.861605\n",
      "Iteration 1452 => Weight: 1.452000 => Loss: 102.689982\n",
      "Iteration 1453 => Weight: 1.453000 => Loss: 102.518796\n",
      "Iteration 1454 => Weight: 1.454000 => Loss: 102.348047\n",
      "Iteration 1455 => Weight: 1.455000 => Loss: 102.177737\n",
      "Iteration 1456 => Weight: 1.456000 => Loss: 102.007863\n",
      "Iteration 1457 => Weight: 1.457000 => Loss: 101.838428\n",
      "Iteration 1458 => Weight: 1.458000 => Loss: 101.669430\n",
      "Iteration 1459 => Weight: 1.459000 => Loss: 101.500869\n",
      "Iteration 1460 => Weight: 1.460000 => Loss: 101.332747\n",
      "Iteration 1461 => Weight: 1.461000 => Loss: 101.165061\n",
      "Iteration 1462 => Weight: 1.462000 => Loss: 100.997814\n",
      "Iteration 1463 => Weight: 1.463000 => Loss: 100.831004\n",
      "Iteration 1464 => Weight: 1.464000 => Loss: 100.664631\n",
      "Iteration 1465 => Weight: 1.465000 => Loss: 100.498697\n",
      "Iteration 1466 => Weight: 1.466000 => Loss: 100.333199\n",
      "Iteration 1467 => Weight: 1.467000 => Loss: 100.168140\n",
      "Iteration 1468 => Weight: 1.468000 => Loss: 100.003518\n",
      "Iteration 1469 => Weight: 1.469000 => Loss: 99.839333\n",
      "Iteration 1470 => Weight: 1.470000 => Loss: 99.675587\n",
      "Iteration 1471 => Weight: 1.471000 => Loss: 99.512277\n",
      "Iteration 1472 => Weight: 1.472000 => Loss: 99.349406\n",
      "Iteration 1473 => Weight: 1.473000 => Loss: 99.186972\n",
      "Iteration 1474 => Weight: 1.474000 => Loss: 99.024975\n",
      "Iteration 1475 => Weight: 1.475000 => Loss: 98.863417\n",
      "Iteration 1476 => Weight: 1.476000 => Loss: 98.702295\n",
      "Iteration 1477 => Weight: 1.477000 => Loss: 98.541612\n",
      "Iteration 1478 => Weight: 1.478000 => Loss: 98.381366\n",
      "Iteration 1479 => Weight: 1.479000 => Loss: 98.221557\n",
      "Iteration 1480 => Weight: 1.480000 => Loss: 98.062187\n",
      "Iteration 1481 => Weight: 1.481000 => Loss: 97.903253\n",
      "Iteration 1482 => Weight: 1.482000 => Loss: 97.744758\n",
      "Iteration 1483 => Weight: 1.483000 => Loss: 97.586700\n",
      "Iteration 1484 => Weight: 1.484000 => Loss: 97.429079\n",
      "Iteration 1485 => Weight: 1.485000 => Loss: 97.271897\n",
      "Iteration 1486 => Weight: 1.486000 => Loss: 97.115151\n",
      "Iteration 1487 => Weight: 1.487000 => Loss: 96.958844\n",
      "Iteration 1488 => Weight: 1.488000 => Loss: 96.802974\n",
      "Iteration 1489 => Weight: 1.489000 => Loss: 96.647541\n",
      "Iteration 1490 => Weight: 1.490000 => Loss: 96.492547\n",
      "Iteration 1491 => Weight: 1.491000 => Loss: 96.337989\n",
      "Iteration 1492 => Weight: 1.492000 => Loss: 96.183870\n",
      "Iteration 1493 => Weight: 1.493000 => Loss: 96.030188\n",
      "Iteration 1494 => Weight: 1.494000 => Loss: 95.876943\n",
      "Iteration 1495 => Weight: 1.495000 => Loss: 95.724137\n",
      "Iteration 1496 => Weight: 1.496000 => Loss: 95.571767\n",
      "Iteration 1497 => Weight: 1.497000 => Loss: 95.419836\n",
      "Iteration 1498 => Weight: 1.498000 => Loss: 95.268342\n",
      "Iteration 1499 => Weight: 1.499000 => Loss: 95.117285\n",
      "Iteration 1500 => Weight: 1.500000 => Loss: 94.966667\n",
      "Iteration 1501 => Weight: 1.501000 => Loss: 94.816485\n",
      "Iteration 1502 => Weight: 1.502000 => Loss: 94.666742\n",
      "Iteration 1503 => Weight: 1.503000 => Loss: 94.517436\n",
      "Iteration 1504 => Weight: 1.504000 => Loss: 94.368567\n",
      "Iteration 1505 => Weight: 1.505000 => Loss: 94.220137\n",
      "Iteration 1506 => Weight: 1.506000 => Loss: 94.072143\n",
      "Iteration 1507 => Weight: 1.507000 => Loss: 93.924588\n",
      "Iteration 1508 => Weight: 1.508000 => Loss: 93.777470\n",
      "Iteration 1509 => Weight: 1.509000 => Loss: 93.630789\n",
      "Iteration 1510 => Weight: 1.510000 => Loss: 93.484547\n",
      "Iteration 1511 => Weight: 1.511000 => Loss: 93.338741\n",
      "Iteration 1512 => Weight: 1.512000 => Loss: 93.193374\n",
      "Iteration 1513 => Weight: 1.513000 => Loss: 93.048444\n",
      "Iteration 1514 => Weight: 1.514000 => Loss: 92.903951\n",
      "Iteration 1515 => Weight: 1.515000 => Loss: 92.759897\n",
      "Iteration 1516 => Weight: 1.516000 => Loss: 92.616279\n",
      "Iteration 1517 => Weight: 1.517000 => Loss: 92.473100\n",
      "Iteration 1518 => Weight: 1.518000 => Loss: 92.330358\n",
      "Iteration 1519 => Weight: 1.519000 => Loss: 92.188053\n",
      "Iteration 1520 => Weight: 1.520000 => Loss: 92.046187\n",
      "Iteration 1521 => Weight: 1.521000 => Loss: 91.904757\n",
      "Iteration 1522 => Weight: 1.522000 => Loss: 91.763766\n",
      "Iteration 1523 => Weight: 1.523000 => Loss: 91.623212\n",
      "Iteration 1524 => Weight: 1.524000 => Loss: 91.483095\n",
      "Iteration 1525 => Weight: 1.525000 => Loss: 91.343417\n",
      "Iteration 1526 => Weight: 1.526000 => Loss: 91.204175\n",
      "Iteration 1527 => Weight: 1.527000 => Loss: 91.065372\n",
      "Iteration 1528 => Weight: 1.528000 => Loss: 90.927006\n",
      "Iteration 1529 => Weight: 1.529000 => Loss: 90.789077\n",
      "Iteration 1530 => Weight: 1.530000 => Loss: 90.651587\n",
      "Iteration 1531 => Weight: 1.531000 => Loss: 90.514533\n",
      "Iteration 1532 => Weight: 1.532000 => Loss: 90.377918\n",
      "Iteration 1533 => Weight: 1.533000 => Loss: 90.241740\n",
      "Iteration 1534 => Weight: 1.534000 => Loss: 90.105999\n",
      "Iteration 1535 => Weight: 1.535000 => Loss: 89.970697\n",
      "Iteration 1536 => Weight: 1.536000 => Loss: 89.835831\n",
      "Iteration 1537 => Weight: 1.537000 => Loss: 89.701404\n",
      "Iteration 1538 => Weight: 1.538000 => Loss: 89.567414\n",
      "Iteration 1539 => Weight: 1.539000 => Loss: 89.433861\n",
      "Iteration 1540 => Weight: 1.540000 => Loss: 89.300747\n",
      "Iteration 1541 => Weight: 1.541000 => Loss: 89.168069\n",
      "Iteration 1542 => Weight: 1.542000 => Loss: 89.035830\n",
      "Iteration 1543 => Weight: 1.543000 => Loss: 88.904028\n",
      "Iteration 1544 => Weight: 1.544000 => Loss: 88.772663\n",
      "Iteration 1545 => Weight: 1.545000 => Loss: 88.641737\n",
      "Iteration 1546 => Weight: 1.546000 => Loss: 88.511247\n",
      "Iteration 1547 => Weight: 1.547000 => Loss: 88.381196\n",
      "Iteration 1548 => Weight: 1.548000 => Loss: 88.251582\n",
      "Iteration 1549 => Weight: 1.549000 => Loss: 88.122405\n",
      "Iteration 1550 => Weight: 1.550000 => Loss: 87.993667\n",
      "Iteration 1551 => Weight: 1.551000 => Loss: 87.865365\n",
      "Iteration 1552 => Weight: 1.552000 => Loss: 87.737502\n",
      "Iteration 1553 => Weight: 1.553000 => Loss: 87.610076\n",
      "Iteration 1554 => Weight: 1.554000 => Loss: 87.483087\n",
      "Iteration 1555 => Weight: 1.555000 => Loss: 87.356537\n",
      "Iteration 1556 => Weight: 1.556000 => Loss: 87.230423\n",
      "Iteration 1557 => Weight: 1.557000 => Loss: 87.104748\n",
      "Iteration 1558 => Weight: 1.558000 => Loss: 86.979510\n",
      "Iteration 1559 => Weight: 1.559000 => Loss: 86.854709\n",
      "Iteration 1560 => Weight: 1.560000 => Loss: 86.730347\n",
      "Iteration 1561 => Weight: 1.561000 => Loss: 86.606421\n",
      "Iteration 1562 => Weight: 1.562000 => Loss: 86.482934\n",
      "Iteration 1563 => Weight: 1.563000 => Loss: 86.359884\n",
      "Iteration 1564 => Weight: 1.564000 => Loss: 86.237271\n",
      "Iteration 1565 => Weight: 1.565000 => Loss: 86.115097\n",
      "Iteration 1566 => Weight: 1.566000 => Loss: 85.993359\n",
      "Iteration 1567 => Weight: 1.567000 => Loss: 85.872060\n",
      "Iteration 1568 => Weight: 1.568000 => Loss: 85.751198\n",
      "Iteration 1569 => Weight: 1.569000 => Loss: 85.630773\n",
      "Iteration 1570 => Weight: 1.570000 => Loss: 85.510787\n",
      "Iteration 1571 => Weight: 1.571000 => Loss: 85.391237\n",
      "Iteration 1572 => Weight: 1.572000 => Loss: 85.272126\n",
      "Iteration 1573 => Weight: 1.573000 => Loss: 85.153452\n",
      "Iteration 1574 => Weight: 1.574000 => Loss: 85.035215\n",
      "Iteration 1575 => Weight: 1.575000 => Loss: 84.917417\n",
      "Iteration 1576 => Weight: 1.576000 => Loss: 84.800055\n",
      "Iteration 1577 => Weight: 1.577000 => Loss: 84.683132\n",
      "Iteration 1578 => Weight: 1.578000 => Loss: 84.566646\n",
      "Iteration 1579 => Weight: 1.579000 => Loss: 84.450597\n",
      "Iteration 1580 => Weight: 1.580000 => Loss: 84.334987\n",
      "Iteration 1581 => Weight: 1.581000 => Loss: 84.219813\n",
      "Iteration 1582 => Weight: 1.582000 => Loss: 84.105078\n",
      "Iteration 1583 => Weight: 1.583000 => Loss: 83.990780\n",
      "Iteration 1584 => Weight: 1.584000 => Loss: 83.876919\n",
      "Iteration 1585 => Weight: 1.585000 => Loss: 83.763497\n",
      "Iteration 1586 => Weight: 1.586000 => Loss: 83.650511\n",
      "Iteration 1587 => Weight: 1.587000 => Loss: 83.537964\n",
      "Iteration 1588 => Weight: 1.588000 => Loss: 83.425854\n",
      "Iteration 1589 => Weight: 1.589000 => Loss: 83.314181\n",
      "Iteration 1590 => Weight: 1.590000 => Loss: 83.202947\n",
      "Iteration 1591 => Weight: 1.591000 => Loss: 83.092149\n",
      "Iteration 1592 => Weight: 1.592000 => Loss: 82.981790\n",
      "Iteration 1593 => Weight: 1.593000 => Loss: 82.871868\n",
      "Iteration 1594 => Weight: 1.594000 => Loss: 82.762383\n",
      "Iteration 1595 => Weight: 1.595000 => Loss: 82.653337\n",
      "Iteration 1596 => Weight: 1.596000 => Loss: 82.544727\n",
      "Iteration 1597 => Weight: 1.597000 => Loss: 82.436556\n",
      "Iteration 1598 => Weight: 1.598000 => Loss: 82.328822\n",
      "Iteration 1599 => Weight: 1.599000 => Loss: 82.221525\n",
      "Iteration 1600 => Weight: 1.600000 => Loss: 82.114667\n",
      "Iteration 1601 => Weight: 1.601000 => Loss: 82.008245\n",
      "Iteration 1602 => Weight: 1.602000 => Loss: 81.902262\n",
      "Iteration 1603 => Weight: 1.603000 => Loss: 81.796716\n",
      "Iteration 1604 => Weight: 1.604000 => Loss: 81.691607\n",
      "Iteration 1605 => Weight: 1.605000 => Loss: 81.586937\n",
      "Iteration 1606 => Weight: 1.606000 => Loss: 81.482703\n",
      "Iteration 1607 => Weight: 1.607000 => Loss: 81.378908\n",
      "Iteration 1608 => Weight: 1.608000 => Loss: 81.275550\n",
      "Iteration 1609 => Weight: 1.609000 => Loss: 81.172629\n",
      "Iteration 1610 => Weight: 1.610000 => Loss: 81.070147\n",
      "Iteration 1611 => Weight: 1.611000 => Loss: 80.968101\n",
      "Iteration 1612 => Weight: 1.612000 => Loss: 80.866494\n",
      "Iteration 1613 => Weight: 1.613000 => Loss: 80.765324\n",
      "Iteration 1614 => Weight: 1.614000 => Loss: 80.664591\n",
      "Iteration 1615 => Weight: 1.615000 => Loss: 80.564297\n",
      "Iteration 1616 => Weight: 1.616000 => Loss: 80.464439\n",
      "Iteration 1617 => Weight: 1.617000 => Loss: 80.365020\n",
      "Iteration 1618 => Weight: 1.618000 => Loss: 80.266038\n",
      "Iteration 1619 => Weight: 1.619000 => Loss: 80.167493\n",
      "Iteration 1620 => Weight: 1.620000 => Loss: 80.069387\n",
      "Iteration 1621 => Weight: 1.621000 => Loss: 79.971717\n",
      "Iteration 1622 => Weight: 1.622000 => Loss: 79.874486\n",
      "Iteration 1623 => Weight: 1.623000 => Loss: 79.777692\n",
      "Iteration 1624 => Weight: 1.624000 => Loss: 79.681335\n",
      "Iteration 1625 => Weight: 1.625000 => Loss: 79.585417\n",
      "Iteration 1626 => Weight: 1.626000 => Loss: 79.489935\n",
      "Iteration 1627 => Weight: 1.627000 => Loss: 79.394892\n",
      "Iteration 1628 => Weight: 1.628000 => Loss: 79.300286\n",
      "Iteration 1629 => Weight: 1.629000 => Loss: 79.206117\n",
      "Iteration 1630 => Weight: 1.630000 => Loss: 79.112387\n",
      "Iteration 1631 => Weight: 1.631000 => Loss: 79.019093\n",
      "Iteration 1632 => Weight: 1.632000 => Loss: 78.926238\n",
      "Iteration 1633 => Weight: 1.633000 => Loss: 78.833820\n",
      "Iteration 1634 => Weight: 1.634000 => Loss: 78.741839\n",
      "Iteration 1635 => Weight: 1.635000 => Loss: 78.650297\n",
      "Iteration 1636 => Weight: 1.636000 => Loss: 78.559191\n",
      "Iteration 1637 => Weight: 1.637000 => Loss: 78.468524\n",
      "Iteration 1638 => Weight: 1.638000 => Loss: 78.378294\n",
      "Iteration 1639 => Weight: 1.639000 => Loss: 78.288501\n",
      "Iteration 1640 => Weight: 1.640000 => Loss: 78.199147\n",
      "Iteration 1641 => Weight: 1.641000 => Loss: 78.110229\n",
      "Iteration 1642 => Weight: 1.642000 => Loss: 78.021750\n",
      "Iteration 1643 => Weight: 1.643000 => Loss: 77.933708\n",
      "Iteration 1644 => Weight: 1.644000 => Loss: 77.846103\n",
      "Iteration 1645 => Weight: 1.645000 => Loss: 77.758937\n",
      "Iteration 1646 => Weight: 1.646000 => Loss: 77.672207\n",
      "Iteration 1647 => Weight: 1.647000 => Loss: 77.585916\n",
      "Iteration 1648 => Weight: 1.648000 => Loss: 77.500062\n",
      "Iteration 1649 => Weight: 1.649000 => Loss: 77.414645\n",
      "Iteration 1650 => Weight: 1.650000 => Loss: 77.329667\n",
      "Iteration 1651 => Weight: 1.651000 => Loss: 77.245125\n",
      "Iteration 1652 => Weight: 1.652000 => Loss: 77.161022\n",
      "Iteration 1653 => Weight: 1.653000 => Loss: 77.077356\n",
      "Iteration 1654 => Weight: 1.654000 => Loss: 76.994127\n",
      "Iteration 1655 => Weight: 1.655000 => Loss: 76.911337\n",
      "Iteration 1656 => Weight: 1.656000 => Loss: 76.828983\n",
      "Iteration 1657 => Weight: 1.657000 => Loss: 76.747068\n",
      "Iteration 1658 => Weight: 1.658000 => Loss: 76.665590\n",
      "Iteration 1659 => Weight: 1.659000 => Loss: 76.584549\n",
      "Iteration 1660 => Weight: 1.660000 => Loss: 76.503947\n",
      "Iteration 1661 => Weight: 1.661000 => Loss: 76.423781\n",
      "Iteration 1662 => Weight: 1.662000 => Loss: 76.344054\n",
      "Iteration 1663 => Weight: 1.663000 => Loss: 76.264764\n",
      "Iteration 1664 => Weight: 1.664000 => Loss: 76.185911\n",
      "Iteration 1665 => Weight: 1.665000 => Loss: 76.107497\n",
      "Iteration 1666 => Weight: 1.666000 => Loss: 76.029519\n",
      "Iteration 1667 => Weight: 1.667000 => Loss: 75.951980\n",
      "Iteration 1668 => Weight: 1.668000 => Loss: 75.874878\n",
      "Iteration 1669 => Weight: 1.669000 => Loss: 75.798213\n",
      "Iteration 1670 => Weight: 1.670000 => Loss: 75.721987\n",
      "Iteration 1671 => Weight: 1.671000 => Loss: 75.646197\n",
      "Iteration 1672 => Weight: 1.672000 => Loss: 75.570846\n",
      "Iteration 1673 => Weight: 1.673000 => Loss: 75.495932\n",
      "Iteration 1674 => Weight: 1.674000 => Loss: 75.421455\n",
      "Iteration 1675 => Weight: 1.675000 => Loss: 75.347417\n",
      "Iteration 1676 => Weight: 1.676000 => Loss: 75.273815\n",
      "Iteration 1677 => Weight: 1.677000 => Loss: 75.200652\n",
      "Iteration 1678 => Weight: 1.678000 => Loss: 75.127926\n",
      "Iteration 1679 => Weight: 1.679000 => Loss: 75.055637\n",
      "Iteration 1680 => Weight: 1.680000 => Loss: 74.983787\n",
      "Iteration 1681 => Weight: 1.681000 => Loss: 74.912373\n",
      "Iteration 1682 => Weight: 1.682000 => Loss: 74.841398\n",
      "Iteration 1683 => Weight: 1.683000 => Loss: 74.770860\n",
      "Iteration 1684 => Weight: 1.684000 => Loss: 74.700759\n",
      "Iteration 1685 => Weight: 1.685000 => Loss: 74.631097\n",
      "Iteration 1686 => Weight: 1.686000 => Loss: 74.561871\n",
      "Iteration 1687 => Weight: 1.687000 => Loss: 74.493084\n",
      "Iteration 1688 => Weight: 1.688000 => Loss: 74.424734\n",
      "Iteration 1689 => Weight: 1.689000 => Loss: 74.356821\n",
      "Iteration 1690 => Weight: 1.690000 => Loss: 74.289347\n",
      "Iteration 1691 => Weight: 1.691000 => Loss: 74.222309\n",
      "Iteration 1692 => Weight: 1.692000 => Loss: 74.155710\n",
      "Iteration 1693 => Weight: 1.693000 => Loss: 74.089548\n",
      "Iteration 1694 => Weight: 1.694000 => Loss: 74.023823\n",
      "Iteration 1695 => Weight: 1.695000 => Loss: 73.958537\n",
      "Iteration 1696 => Weight: 1.696000 => Loss: 73.893687\n",
      "Iteration 1697 => Weight: 1.697000 => Loss: 73.829276\n",
      "Iteration 1698 => Weight: 1.698000 => Loss: 73.765302\n",
      "Iteration 1699 => Weight: 1.699000 => Loss: 73.701765\n",
      "Iteration 1700 => Weight: 1.700000 => Loss: 73.638667\n",
      "Iteration 1701 => Weight: 1.701000 => Loss: 73.576005\n",
      "Iteration 1702 => Weight: 1.702000 => Loss: 73.513782\n",
      "Iteration 1703 => Weight: 1.703000 => Loss: 73.451996\n",
      "Iteration 1704 => Weight: 1.704000 => Loss: 73.390647\n",
      "Iteration 1705 => Weight: 1.705000 => Loss: 73.329737\n",
      "Iteration 1706 => Weight: 1.706000 => Loss: 73.269263\n",
      "Iteration 1707 => Weight: 1.707000 => Loss: 73.209228\n",
      "Iteration 1708 => Weight: 1.708000 => Loss: 73.149630\n",
      "Iteration 1709 => Weight: 1.709000 => Loss: 73.090469\n",
      "Iteration 1710 => Weight: 1.710000 => Loss: 73.031747\n",
      "Iteration 1711 => Weight: 1.711000 => Loss: 72.973461\n",
      "Iteration 1712 => Weight: 1.712000 => Loss: 72.915614\n",
      "Iteration 1713 => Weight: 1.713000 => Loss: 72.858204\n",
      "Iteration 1714 => Weight: 1.714000 => Loss: 72.801231\n",
      "Iteration 1715 => Weight: 1.715000 => Loss: 72.744697\n",
      "Iteration 1716 => Weight: 1.716000 => Loss: 72.688599\n",
      "Iteration 1717 => Weight: 1.717000 => Loss: 72.632940\n",
      "Iteration 1718 => Weight: 1.718000 => Loss: 72.577718\n",
      "Iteration 1719 => Weight: 1.719000 => Loss: 72.522933\n",
      "Iteration 1720 => Weight: 1.720000 => Loss: 72.468587\n",
      "Iteration 1721 => Weight: 1.721000 => Loss: 72.414677\n",
      "Iteration 1722 => Weight: 1.722000 => Loss: 72.361206\n",
      "Iteration 1723 => Weight: 1.723000 => Loss: 72.308172\n",
      "Iteration 1724 => Weight: 1.724000 => Loss: 72.255575\n",
      "Iteration 1725 => Weight: 1.725000 => Loss: 72.203417\n",
      "Iteration 1726 => Weight: 1.726000 => Loss: 72.151695\n",
      "Iteration 1727 => Weight: 1.727000 => Loss: 72.100412\n",
      "Iteration 1728 => Weight: 1.728000 => Loss: 72.049566\n",
      "Iteration 1729 => Weight: 1.729000 => Loss: 71.999157\n",
      "Iteration 1730 => Weight: 1.730000 => Loss: 71.949187\n",
      "Iteration 1731 => Weight: 1.731000 => Loss: 71.899653\n",
      "Iteration 1732 => Weight: 1.732000 => Loss: 71.850558\n",
      "Iteration 1733 => Weight: 1.733000 => Loss: 71.801900\n",
      "Iteration 1734 => Weight: 1.734000 => Loss: 71.753679\n",
      "Iteration 1735 => Weight: 1.735000 => Loss: 71.705897\n",
      "Iteration 1736 => Weight: 1.736000 => Loss: 71.658551\n",
      "Iteration 1737 => Weight: 1.737000 => Loss: 71.611644\n",
      "Iteration 1738 => Weight: 1.738000 => Loss: 71.565174\n",
      "Iteration 1739 => Weight: 1.739000 => Loss: 71.519141\n",
      "Iteration 1740 => Weight: 1.740000 => Loss: 71.473547\n",
      "Iteration 1741 => Weight: 1.741000 => Loss: 71.428389\n",
      "Iteration 1742 => Weight: 1.742000 => Loss: 71.383670\n",
      "Iteration 1743 => Weight: 1.743000 => Loss: 71.339388\n",
      "Iteration 1744 => Weight: 1.744000 => Loss: 71.295543\n",
      "Iteration 1745 => Weight: 1.745000 => Loss: 71.252137\n",
      "Iteration 1746 => Weight: 1.746000 => Loss: 71.209167\n",
      "Iteration 1747 => Weight: 1.747000 => Loss: 71.166636\n",
      "Iteration 1748 => Weight: 1.748000 => Loss: 71.124542\n",
      "Iteration 1749 => Weight: 1.749000 => Loss: 71.082885\n",
      "Iteration 1750 => Weight: 1.750000 => Loss: 71.041667\n",
      "Iteration 1751 => Weight: 1.751000 => Loss: 71.000885\n",
      "Iteration 1752 => Weight: 1.752000 => Loss: 70.960542\n",
      "Iteration 1753 => Weight: 1.753000 => Loss: 70.920636\n",
      "Iteration 1754 => Weight: 1.754000 => Loss: 70.881167\n",
      "Iteration 1755 => Weight: 1.755000 => Loss: 70.842137\n",
      "Iteration 1756 => Weight: 1.756000 => Loss: 70.803543\n",
      "Iteration 1757 => Weight: 1.757000 => Loss: 70.765388\n",
      "Iteration 1758 => Weight: 1.758000 => Loss: 70.727670\n",
      "Iteration 1759 => Weight: 1.759000 => Loss: 70.690389\n",
      "Iteration 1760 => Weight: 1.760000 => Loss: 70.653547\n",
      "Iteration 1761 => Weight: 1.761000 => Loss: 70.617141\n",
      "Iteration 1762 => Weight: 1.762000 => Loss: 70.581174\n",
      "Iteration 1763 => Weight: 1.763000 => Loss: 70.545644\n",
      "Iteration 1764 => Weight: 1.764000 => Loss: 70.510551\n",
      "Iteration 1765 => Weight: 1.765000 => Loss: 70.475897\n",
      "Iteration 1766 => Weight: 1.766000 => Loss: 70.441679\n",
      "Iteration 1767 => Weight: 1.767000 => Loss: 70.407900\n",
      "Iteration 1768 => Weight: 1.768000 => Loss: 70.374558\n",
      "Iteration 1769 => Weight: 1.769000 => Loss: 70.341653\n",
      "Iteration 1770 => Weight: 1.770000 => Loss: 70.309187\n",
      "Iteration 1771 => Weight: 1.771000 => Loss: 70.277157\n",
      "Iteration 1772 => Weight: 1.772000 => Loss: 70.245566\n",
      "Iteration 1773 => Weight: 1.773000 => Loss: 70.214412\n",
      "Iteration 1774 => Weight: 1.774000 => Loss: 70.183695\n",
      "Iteration 1775 => Weight: 1.775000 => Loss: 70.153417\n",
      "Iteration 1776 => Weight: 1.776000 => Loss: 70.123575\n",
      "Iteration 1777 => Weight: 1.777000 => Loss: 70.094172\n",
      "Iteration 1778 => Weight: 1.778000 => Loss: 70.065206\n",
      "Iteration 1779 => Weight: 1.779000 => Loss: 70.036677\n",
      "Iteration 1780 => Weight: 1.780000 => Loss: 70.008587\n",
      "Iteration 1781 => Weight: 1.781000 => Loss: 69.980933\n",
      "Iteration 1782 => Weight: 1.782000 => Loss: 69.953718\n",
      "Iteration 1783 => Weight: 1.783000 => Loss: 69.926940\n",
      "Iteration 1784 => Weight: 1.784000 => Loss: 69.900599\n",
      "Iteration 1785 => Weight: 1.785000 => Loss: 69.874697\n",
      "Iteration 1786 => Weight: 1.786000 => Loss: 69.849231\n",
      "Iteration 1787 => Weight: 1.787000 => Loss: 69.824204\n",
      "Iteration 1788 => Weight: 1.788000 => Loss: 69.799614\n",
      "Iteration 1789 => Weight: 1.789000 => Loss: 69.775461\n",
      "Iteration 1790 => Weight: 1.790000 => Loss: 69.751747\n",
      "Iteration 1791 => Weight: 1.791000 => Loss: 69.728469\n",
      "Iteration 1792 => Weight: 1.792000 => Loss: 69.705630\n",
      "Iteration 1793 => Weight: 1.793000 => Loss: 69.683228\n",
      "Iteration 1794 => Weight: 1.794000 => Loss: 69.661263\n",
      "Iteration 1795 => Weight: 1.795000 => Loss: 69.639737\n",
      "Iteration 1796 => Weight: 1.796000 => Loss: 69.618647\n",
      "Iteration 1797 => Weight: 1.797000 => Loss: 69.597996\n",
      "Iteration 1798 => Weight: 1.798000 => Loss: 69.577782\n",
      "Iteration 1799 => Weight: 1.799000 => Loss: 69.558005\n",
      "Iteration 1800 => Weight: 1.800000 => Loss: 69.538667\n",
      "Iteration 1801 => Weight: 1.801000 => Loss: 69.519765\n",
      "Iteration 1802 => Weight: 1.802000 => Loss: 69.501302\n",
      "Iteration 1803 => Weight: 1.803000 => Loss: 69.483276\n",
      "Iteration 1804 => Weight: 1.804000 => Loss: 69.465687\n",
      "Iteration 1805 => Weight: 1.805000 => Loss: 69.448537\n",
      "Iteration 1806 => Weight: 1.806000 => Loss: 69.431823\n",
      "Iteration 1807 => Weight: 1.807000 => Loss: 69.415548\n",
      "Iteration 1808 => Weight: 1.808000 => Loss: 69.399710\n",
      "Iteration 1809 => Weight: 1.809000 => Loss: 69.384309\n",
      "Iteration 1810 => Weight: 1.810000 => Loss: 69.369347\n",
      "Iteration 1811 => Weight: 1.811000 => Loss: 69.354821\n",
      "Iteration 1812 => Weight: 1.812000 => Loss: 69.340734\n",
      "Iteration 1813 => Weight: 1.813000 => Loss: 69.327084\n",
      "Iteration 1814 => Weight: 1.814000 => Loss: 69.313871\n",
      "Iteration 1815 => Weight: 1.815000 => Loss: 69.301097\n",
      "Iteration 1816 => Weight: 1.816000 => Loss: 69.288759\n",
      "Iteration 1817 => Weight: 1.817000 => Loss: 69.276860\n",
      "Iteration 1818 => Weight: 1.818000 => Loss: 69.265398\n",
      "Iteration 1819 => Weight: 1.819000 => Loss: 69.254373\n",
      "Iteration 1820 => Weight: 1.820000 => Loss: 69.243787\n",
      "Iteration 1821 => Weight: 1.821000 => Loss: 69.233637\n",
      "Iteration 1822 => Weight: 1.822000 => Loss: 69.223926\n",
      "Iteration 1823 => Weight: 1.823000 => Loss: 69.214652\n",
      "Iteration 1824 => Weight: 1.824000 => Loss: 69.205815\n",
      "Iteration 1825 => Weight: 1.825000 => Loss: 69.197417\n",
      "Iteration 1826 => Weight: 1.826000 => Loss: 69.189455\n",
      "Iteration 1827 => Weight: 1.827000 => Loss: 69.181932\n",
      "Iteration 1828 => Weight: 1.828000 => Loss: 69.174846\n",
      "Iteration 1829 => Weight: 1.829000 => Loss: 69.168197\n",
      "Iteration 1830 => Weight: 1.830000 => Loss: 69.161987\n",
      "Iteration 1831 => Weight: 1.831000 => Loss: 69.156213\n",
      "Iteration 1832 => Weight: 1.832000 => Loss: 69.150878\n",
      "Iteration 1833 => Weight: 1.833000 => Loss: 69.145980\n",
      "Iteration 1834 => Weight: 1.834000 => Loss: 69.141519\n",
      "Iteration 1835 => Weight: 1.835000 => Loss: 69.137497\n",
      "Iteration 1836 => Weight: 1.836000 => Loss: 69.133911\n",
      "Iteration 1837 => Weight: 1.837000 => Loss: 69.130764\n",
      "Iteration 1838 => Weight: 1.838000 => Loss: 69.128054\n",
      "Iteration 1839 => Weight: 1.839000 => Loss: 69.125781\n",
      "Iteration 1840 => Weight: 1.840000 => Loss: 69.123947\n",
      "Iteration 1841 => Weight: 1.841000 => Loss: 69.122549\n",
      "Iteration 1842 => Weight: 1.842000 => Loss: 69.121590\n",
      "Iteration 1843 => Weight: 1.843000 => Loss: 69.121068\n",
      "Iteration 1844 => Weight: 1.844000 => Loss: 69.120983\n",
      "Resulting weight: 1.8439999999999077\n"
     ]
    }
   ],
   "source": [
    "# Train the system\n",
    "max_iterations = 10_000\n",
    "learning_rate = 0.001\n",
    "got_weight = train(X, Y, max_iterations, learning_rate)\n",
    "print(\"Resulting weight:\", got_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "199b0564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: x=20 => y=36.88\n"
     ]
    }
   ],
   "source": [
    "# Predict the number of pizzas\n",
    "reservations = 20\n",
    "print(\"Prediction: x=%d => y=%.2f\" % (reservations, predict(20, got_weight)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "12ba15b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the graph with the learned weight\n",
    "sea.set_theme()\n",
    "plt.axis([0, 50, 0, 50])\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.xlabel(\"Reservations\", fontsize=14)\n",
    "plt.ylabel(\"Pizzas\", fontsize=14)\n",
    "plt.plot(X, Y, \"ro\")\n",
    "plt.plot(X, predict(X, got_weight))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf0076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8d3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4swes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
