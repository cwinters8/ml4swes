{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59506e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and define required functions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sea\n",
    "\n",
    "# w = weight (i.e. how much weight does X have on the predicted result)\n",
    "# b = bias (accounts for baseline values in the data that aren't explained solely by input values)\n",
    "def predict(X, w, b):\n",
    "  return X * w + b\n",
    "\n",
    "def loss(X, Y, w, b):\n",
    "  return np.average((predict(X, w, b) - Y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4babf168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from pizza.txt\n",
    "X, Y = np.loadtxt(\"pizza.txt\", skiprows=1, unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63d1bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "# calculate the correct (or closest possible?) weight from the values in X and Y\n",
    "\n",
    "# iterations = max number of iterations to determine the weight\n",
    "# lr = learning rate (step size)\n",
    "def train(X, Y, iterations, lr) -> tuple[float, float]:\n",
    "  # starting weight and bias\n",
    "  w = b = 0\n",
    "  for i in range(iterations):\n",
    "    current_loss = loss(X, Y, w, b)\n",
    "    print(\"Iteration %4d => Weight: %.6f => Bias: %.6f => Loss: %.6f\" % (i, w, b, current_loss))\n",
    "\n",
    "    # does adding a step to w (weight) reduce the loss?\n",
    "    if loss(X, Y, w + lr, b) < current_loss:\n",
    "      w += lr\n",
    "    # does subtracting a step from w reduce the loss?\n",
    "    elif loss(X, Y, w - lr, b) < current_loss:\n",
    "      w -= lr\n",
    "\n",
    "    # does adding a step to b (bias) reduce the loss?\n",
    "    if loss(X, Y, w, b + lr) < current_loss:\n",
    "      b += lr\n",
    "    # does subtracting a step from b reduce the loss?\n",
    "    elif loss(X, Y, w, b - lr) < current_loss:\n",
    "      b -= lr\n",
    "    # if a step in either direction doesn't improve the loss, return the closest weight and bias we can\n",
    "    else:\n",
    "      return w, b\n",
    "  \n",
    "  raise Exception(\"Couldn't converge within %4d iterations\" % iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d463944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 => Weight: 0.000000 => Bias: 0.000000 => Loss: 812.866667\n",
      "Iteration    1 => Weight: 0.010000 => Bias: 0.010000 => Loss: 804.285847\n",
      "Iteration    2 => Weight: 0.020000 => Bias: 0.020000 => Loss: 795.754053\n",
      "Iteration    3 => Weight: 0.030000 => Bias: 0.030000 => Loss: 787.271287\n",
      "Iteration    4 => Weight: 0.040000 => Bias: 0.040000 => Loss: 778.837547\n",
      "Iteration    5 => Weight: 0.050000 => Bias: 0.050000 => Loss: 770.452833\n",
      "Iteration    6 => Weight: 0.060000 => Bias: 0.060000 => Loss: 762.117147\n",
      "Iteration    7 => Weight: 0.070000 => Bias: 0.070000 => Loss: 753.830487\n",
      "Iteration    8 => Weight: 0.080000 => Bias: 0.080000 => Loss: 745.592853\n",
      "Iteration    9 => Weight: 0.090000 => Bias: 0.090000 => Loss: 737.404247\n",
      "Iteration   10 => Weight: 0.100000 => Bias: 0.100000 => Loss: 729.264667\n",
      "Iteration   11 => Weight: 0.110000 => Bias: 0.110000 => Loss: 721.174113\n",
      "Iteration   12 => Weight: 0.120000 => Bias: 0.120000 => Loss: 713.132587\n",
      "Iteration   13 => Weight: 0.130000 => Bias: 0.130000 => Loss: 705.140087\n",
      "Iteration   14 => Weight: 0.140000 => Bias: 0.140000 => Loss: 697.196613\n",
      "Iteration   15 => Weight: 0.150000 => Bias: 0.150000 => Loss: 689.302167\n",
      "Iteration   16 => Weight: 0.160000 => Bias: 0.160000 => Loss: 681.456747\n",
      "Iteration   17 => Weight: 0.170000 => Bias: 0.170000 => Loss: 673.660353\n",
      "Iteration   18 => Weight: 0.180000 => Bias: 0.180000 => Loss: 665.912987\n",
      "Iteration   19 => Weight: 0.190000 => Bias: 0.190000 => Loss: 658.214647\n",
      "Iteration   20 => Weight: 0.200000 => Bias: 0.200000 => Loss: 650.565333\n",
      "Iteration   21 => Weight: 0.210000 => Bias: 0.210000 => Loss: 642.965047\n",
      "Iteration   22 => Weight: 0.220000 => Bias: 0.220000 => Loss: 635.413787\n",
      "Iteration   23 => Weight: 0.230000 => Bias: 0.230000 => Loss: 627.911553\n",
      "Iteration   24 => Weight: 0.240000 => Bias: 0.240000 => Loss: 620.458347\n",
      "Iteration   25 => Weight: 0.250000 => Bias: 0.250000 => Loss: 613.054167\n",
      "Iteration   26 => Weight: 0.260000 => Bias: 0.260000 => Loss: 605.699013\n",
      "Iteration   27 => Weight: 0.270000 => Bias: 0.270000 => Loss: 598.392887\n",
      "Iteration   28 => Weight: 0.280000 => Bias: 0.280000 => Loss: 591.135787\n",
      "Iteration   29 => Weight: 0.290000 => Bias: 0.290000 => Loss: 583.927713\n",
      "Iteration   30 => Weight: 0.300000 => Bias: 0.300000 => Loss: 576.768667\n",
      "Iteration   31 => Weight: 0.310000 => Bias: 0.310000 => Loss: 569.658647\n",
      "Iteration   32 => Weight: 0.320000 => Bias: 0.320000 => Loss: 562.597653\n",
      "Iteration   33 => Weight: 0.330000 => Bias: 0.330000 => Loss: 555.585687\n",
      "Iteration   34 => Weight: 0.340000 => Bias: 0.340000 => Loss: 548.622747\n",
      "Iteration   35 => Weight: 0.350000 => Bias: 0.350000 => Loss: 541.708833\n",
      "Iteration   36 => Weight: 0.360000 => Bias: 0.360000 => Loss: 534.843947\n",
      "Iteration   37 => Weight: 0.370000 => Bias: 0.370000 => Loss: 528.028087\n",
      "Iteration   38 => Weight: 0.380000 => Bias: 0.380000 => Loss: 521.261253\n",
      "Iteration   39 => Weight: 0.390000 => Bias: 0.390000 => Loss: 514.543447\n",
      "Iteration   40 => Weight: 0.400000 => Bias: 0.400000 => Loss: 507.874667\n",
      "Iteration   41 => Weight: 0.410000 => Bias: 0.410000 => Loss: 501.254913\n",
      "Iteration   42 => Weight: 0.420000 => Bias: 0.420000 => Loss: 494.684187\n",
      "Iteration   43 => Weight: 0.430000 => Bias: 0.430000 => Loss: 488.162487\n",
      "Iteration   44 => Weight: 0.440000 => Bias: 0.440000 => Loss: 481.689813\n",
      "Iteration   45 => Weight: 0.450000 => Bias: 0.450000 => Loss: 475.266167\n",
      "Iteration   46 => Weight: 0.460000 => Bias: 0.460000 => Loss: 468.891547\n",
      "Iteration   47 => Weight: 0.470000 => Bias: 0.470000 => Loss: 462.565953\n",
      "Iteration   48 => Weight: 0.480000 => Bias: 0.480000 => Loss: 456.289387\n",
      "Iteration   49 => Weight: 0.490000 => Bias: 0.490000 => Loss: 450.061847\n",
      "Iteration   50 => Weight: 0.500000 => Bias: 0.500000 => Loss: 443.883333\n",
      "Iteration   51 => Weight: 0.510000 => Bias: 0.510000 => Loss: 437.753847\n",
      "Iteration   52 => Weight: 0.520000 => Bias: 0.520000 => Loss: 431.673387\n",
      "Iteration   53 => Weight: 0.530000 => Bias: 0.530000 => Loss: 425.641953\n",
      "Iteration   54 => Weight: 0.540000 => Bias: 0.540000 => Loss: 419.659547\n",
      "Iteration   55 => Weight: 0.550000 => Bias: 0.550000 => Loss: 413.726167\n",
      "Iteration   56 => Weight: 0.560000 => Bias: 0.560000 => Loss: 407.841813\n",
      "Iteration   57 => Weight: 0.570000 => Bias: 0.570000 => Loss: 402.006487\n",
      "Iteration   58 => Weight: 0.580000 => Bias: 0.580000 => Loss: 396.220187\n",
      "Iteration   59 => Weight: 0.590000 => Bias: 0.590000 => Loss: 390.482913\n",
      "Iteration   60 => Weight: 0.600000 => Bias: 0.600000 => Loss: 384.794667\n",
      "Iteration   61 => Weight: 0.610000 => Bias: 0.610000 => Loss: 379.155447\n",
      "Iteration   62 => Weight: 0.620000 => Bias: 0.620000 => Loss: 373.565253\n",
      "Iteration   63 => Weight: 0.630000 => Bias: 0.630000 => Loss: 368.024087\n",
      "Iteration   64 => Weight: 0.640000 => Bias: 0.640000 => Loss: 362.531947\n",
      "Iteration   65 => Weight: 0.650000 => Bias: 0.650000 => Loss: 357.088833\n",
      "Iteration   66 => Weight: 0.660000 => Bias: 0.660000 => Loss: 351.694747\n",
      "Iteration   67 => Weight: 0.670000 => Bias: 0.670000 => Loss: 346.349687\n",
      "Iteration   68 => Weight: 0.680000 => Bias: 0.680000 => Loss: 341.053653\n",
      "Iteration   69 => Weight: 0.690000 => Bias: 0.690000 => Loss: 335.806647\n",
      "Iteration   70 => Weight: 0.700000 => Bias: 0.700000 => Loss: 330.608667\n",
      "Iteration   71 => Weight: 0.710000 => Bias: 0.710000 => Loss: 325.459713\n",
      "Iteration   72 => Weight: 0.720000 => Bias: 0.720000 => Loss: 320.359787\n",
      "Iteration   73 => Weight: 0.730000 => Bias: 0.730000 => Loss: 315.308887\n",
      "Iteration   74 => Weight: 0.740000 => Bias: 0.740000 => Loss: 310.307013\n",
      "Iteration   75 => Weight: 0.750000 => Bias: 0.750000 => Loss: 305.354167\n",
      "Iteration   76 => Weight: 0.760000 => Bias: 0.760000 => Loss: 300.450347\n",
      "Iteration   77 => Weight: 0.770000 => Bias: 0.770000 => Loss: 295.595553\n",
      "Iteration   78 => Weight: 0.780000 => Bias: 0.780000 => Loss: 290.789787\n",
      "Iteration   79 => Weight: 0.790000 => Bias: 0.790000 => Loss: 286.033047\n",
      "Iteration   80 => Weight: 0.800000 => Bias: 0.800000 => Loss: 281.325333\n",
      "Iteration   81 => Weight: 0.810000 => Bias: 0.810000 => Loss: 276.666647\n",
      "Iteration   82 => Weight: 0.820000 => Bias: 0.820000 => Loss: 272.056987\n",
      "Iteration   83 => Weight: 0.830000 => Bias: 0.830000 => Loss: 267.496353\n",
      "Iteration   84 => Weight: 0.840000 => Bias: 0.840000 => Loss: 262.984747\n",
      "Iteration   85 => Weight: 0.850000 => Bias: 0.850000 => Loss: 258.522167\n",
      "Iteration   86 => Weight: 0.860000 => Bias: 0.860000 => Loss: 254.108613\n",
      "Iteration   87 => Weight: 0.870000 => Bias: 0.870000 => Loss: 249.744087\n",
      "Iteration   88 => Weight: 0.880000 => Bias: 0.880000 => Loss: 245.428587\n",
      "Iteration   89 => Weight: 0.890000 => Bias: 0.890000 => Loss: 241.162113\n",
      "Iteration   90 => Weight: 0.900000 => Bias: 0.900000 => Loss: 236.944667\n",
      "Iteration   91 => Weight: 0.910000 => Bias: 0.910000 => Loss: 232.776247\n",
      "Iteration   92 => Weight: 0.920000 => Bias: 0.920000 => Loss: 228.656853\n",
      "Iteration   93 => Weight: 0.930000 => Bias: 0.930000 => Loss: 224.586487\n",
      "Iteration   94 => Weight: 0.940000 => Bias: 0.940000 => Loss: 220.565147\n",
      "Iteration   95 => Weight: 0.950000 => Bias: 0.950000 => Loss: 216.592833\n",
      "Iteration   96 => Weight: 0.960000 => Bias: 0.960000 => Loss: 212.669547\n",
      "Iteration   97 => Weight: 0.970000 => Bias: 0.970000 => Loss: 208.795287\n",
      "Iteration   98 => Weight: 0.980000 => Bias: 0.980000 => Loss: 204.970053\n",
      "Iteration   99 => Weight: 0.990000 => Bias: 0.990000 => Loss: 201.193847\n",
      "Iteration  100 => Weight: 1.000000 => Bias: 1.000000 => Loss: 197.466667\n",
      "Iteration  101 => Weight: 1.010000 => Bias: 1.010000 => Loss: 193.788513\n",
      "Iteration  102 => Weight: 1.020000 => Bias: 1.020000 => Loss: 190.159387\n",
      "Iteration  103 => Weight: 1.030000 => Bias: 1.030000 => Loss: 186.579287\n",
      "Iteration  104 => Weight: 1.040000 => Bias: 1.040000 => Loss: 183.048213\n",
      "Iteration  105 => Weight: 1.050000 => Bias: 1.050000 => Loss: 179.566167\n",
      "Iteration  106 => Weight: 1.060000 => Bias: 1.060000 => Loss: 176.133147\n",
      "Iteration  107 => Weight: 1.070000 => Bias: 1.070000 => Loss: 172.749153\n",
      "Iteration  108 => Weight: 1.080000 => Bias: 1.080000 => Loss: 169.414187\n",
      "Iteration  109 => Weight: 1.090000 => Bias: 1.090000 => Loss: 166.128247\n",
      "Iteration  110 => Weight: 1.100000 => Bias: 1.100000 => Loss: 162.891333\n",
      "Iteration  111 => Weight: 1.110000 => Bias: 1.110000 => Loss: 159.703447\n",
      "Iteration  112 => Weight: 1.120000 => Bias: 1.120000 => Loss: 156.564587\n",
      "Iteration  113 => Weight: 1.130000 => Bias: 1.130000 => Loss: 153.474753\n",
      "Iteration  114 => Weight: 1.140000 => Bias: 1.140000 => Loss: 150.433947\n",
      "Iteration  115 => Weight: 1.150000 => Bias: 1.150000 => Loss: 147.442167\n",
      "Iteration  116 => Weight: 1.160000 => Bias: 1.160000 => Loss: 144.499413\n",
      "Iteration  117 => Weight: 1.170000 => Bias: 1.170000 => Loss: 141.605687\n",
      "Iteration  118 => Weight: 1.180000 => Bias: 1.180000 => Loss: 138.760987\n",
      "Iteration  119 => Weight: 1.190000 => Bias: 1.190000 => Loss: 135.965313\n",
      "Iteration  120 => Weight: 1.200000 => Bias: 1.200000 => Loss: 133.218667\n",
      "Iteration  121 => Weight: 1.210000 => Bias: 1.210000 => Loss: 130.521047\n",
      "Iteration  122 => Weight: 1.220000 => Bias: 1.220000 => Loss: 127.872453\n",
      "Iteration  123 => Weight: 1.230000 => Bias: 1.230000 => Loss: 125.272887\n",
      "Iteration  124 => Weight: 1.240000 => Bias: 1.240000 => Loss: 122.722347\n",
      "Iteration  125 => Weight: 1.250000 => Bias: 1.250000 => Loss: 120.220833\n",
      "Iteration  126 => Weight: 1.260000 => Bias: 1.260000 => Loss: 117.768347\n",
      "Iteration  127 => Weight: 1.270000 => Bias: 1.270000 => Loss: 115.364887\n",
      "Iteration  128 => Weight: 1.280000 => Bias: 1.280000 => Loss: 113.010453\n",
      "Iteration  129 => Weight: 1.290000 => Bias: 1.290000 => Loss: 110.705047\n",
      "Iteration  130 => Weight: 1.300000 => Bias: 1.300000 => Loss: 108.448667\n",
      "Iteration  131 => Weight: 1.310000 => Bias: 1.310000 => Loss: 106.241313\n",
      "Iteration  132 => Weight: 1.320000 => Bias: 1.320000 => Loss: 104.082987\n",
      "Iteration  133 => Weight: 1.330000 => Bias: 1.330000 => Loss: 101.973687\n",
      "Iteration  134 => Weight: 1.340000 => Bias: 1.340000 => Loss: 99.913413\n",
      "Iteration  135 => Weight: 1.350000 => Bias: 1.350000 => Loss: 97.902167\n",
      "Iteration  136 => Weight: 1.360000 => Bias: 1.360000 => Loss: 95.939947\n",
      "Iteration  137 => Weight: 1.370000 => Bias: 1.370000 => Loss: 94.026753\n",
      "Iteration  138 => Weight: 1.380000 => Bias: 1.380000 => Loss: 92.162587\n",
      "Iteration  139 => Weight: 1.390000 => Bias: 1.390000 => Loss: 90.347447\n",
      "Iteration  140 => Weight: 1.400000 => Bias: 1.400000 => Loss: 88.581333\n",
      "Iteration  141 => Weight: 1.410000 => Bias: 1.410000 => Loss: 86.864247\n",
      "Iteration  142 => Weight: 1.420000 => Bias: 1.420000 => Loss: 85.196187\n",
      "Iteration  143 => Weight: 1.430000 => Bias: 1.430000 => Loss: 83.577153\n",
      "Iteration  144 => Weight: 1.440000 => Bias: 1.440000 => Loss: 82.007147\n",
      "Iteration  145 => Weight: 1.450000 => Bias: 1.450000 => Loss: 80.486167\n",
      "Iteration  146 => Weight: 1.460000 => Bias: 1.460000 => Loss: 79.014213\n",
      "Iteration  147 => Weight: 1.470000 => Bias: 1.470000 => Loss: 77.591287\n",
      "Iteration  148 => Weight: 1.480000 => Bias: 1.480000 => Loss: 76.217387\n",
      "Iteration  149 => Weight: 1.490000 => Bias: 1.490000 => Loss: 74.892513\n",
      "Iteration  150 => Weight: 1.500000 => Bias: 1.500000 => Loss: 73.616667\n",
      "Iteration  151 => Weight: 1.510000 => Bias: 1.510000 => Loss: 72.389847\n",
      "Iteration  152 => Weight: 1.520000 => Bias: 1.520000 => Loss: 71.212053\n",
      "Iteration  153 => Weight: 1.530000 => Bias: 1.530000 => Loss: 70.083287\n",
      "Iteration  154 => Weight: 1.540000 => Bias: 1.540000 => Loss: 69.003547\n",
      "Iteration  155 => Weight: 1.550000 => Bias: 1.550000 => Loss: 67.972833\n",
      "Iteration  156 => Weight: 1.560000 => Bias: 1.560000 => Loss: 66.991147\n",
      "Iteration  157 => Weight: 1.570000 => Bias: 1.570000 => Loss: 66.058487\n",
      "Iteration  158 => Weight: 1.580000 => Bias: 1.580000 => Loss: 65.174853\n",
      "Iteration  159 => Weight: 1.590000 => Bias: 1.590000 => Loss: 64.340247\n",
      "Iteration  160 => Weight: 1.600000 => Bias: 1.600000 => Loss: 63.554667\n",
      "Iteration  161 => Weight: 1.610000 => Bias: 1.610000 => Loss: 62.818113\n",
      "Iteration  162 => Weight: 1.620000 => Bias: 1.620000 => Loss: 62.130587\n",
      "Iteration  163 => Weight: 1.630000 => Bias: 1.630000 => Loss: 61.492087\n",
      "Iteration  164 => Weight: 1.640000 => Bias: 1.640000 => Loss: 60.902613\n",
      "Iteration  165 => Weight: 1.650000 => Bias: 1.650000 => Loss: 60.362167\n",
      "Iteration  166 => Weight: 1.660000 => Bias: 1.660000 => Loss: 59.870747\n",
      "Iteration  167 => Weight: 1.670000 => Bias: 1.670000 => Loss: 59.428353\n",
      "Iteration  168 => Weight: 1.680000 => Bias: 1.680000 => Loss: 59.034987\n",
      "Iteration  169 => Weight: 1.690000 => Bias: 1.690000 => Loss: 58.690647\n",
      "Iteration  170 => Weight: 1.700000 => Bias: 1.700000 => Loss: 58.395333\n",
      "Iteration  171 => Weight: 1.710000 => Bias: 1.710000 => Loss: 58.149047\n",
      "Iteration  172 => Weight: 1.720000 => Bias: 1.720000 => Loss: 57.951787\n",
      "Iteration  173 => Weight: 1.730000 => Bias: 1.730000 => Loss: 57.803553\n",
      "Iteration  174 => Weight: 1.740000 => Bias: 1.740000 => Loss: 57.704347\n",
      "Iteration  175 => Weight: 1.740000 => Bias: 1.750000 => Loss: 57.642713\n",
      "Iteration  176 => Weight: 1.740000 => Bias: 1.760000 => Loss: 57.581280\n",
      "Iteration  177 => Weight: 1.740000 => Bias: 1.770000 => Loss: 57.520047\n",
      "Iteration  178 => Weight: 1.740000 => Bias: 1.780000 => Loss: 57.459013\n",
      "Iteration  179 => Weight: 1.740000 => Bias: 1.790000 => Loss: 57.398180\n",
      "Iteration  180 => Weight: 1.740000 => Bias: 1.800000 => Loss: 57.337547\n",
      "Iteration  181 => Weight: 1.740000 => Bias: 1.810000 => Loss: 57.277113\n",
      "Iteration  182 => Weight: 1.740000 => Bias: 1.820000 => Loss: 57.216880\n",
      "Iteration  183 => Weight: 1.740000 => Bias: 1.830000 => Loss: 57.156847\n",
      "Iteration  184 => Weight: 1.740000 => Bias: 1.840000 => Loss: 57.097013\n",
      "Iteration  185 => Weight: 1.740000 => Bias: 1.850000 => Loss: 57.037380\n",
      "Iteration  186 => Weight: 1.740000 => Bias: 1.860000 => Loss: 56.977947\n",
      "Iteration  187 => Weight: 1.740000 => Bias: 1.870000 => Loss: 56.918713\n",
      "Iteration  188 => Weight: 1.740000 => Bias: 1.880000 => Loss: 56.859680\n",
      "Iteration  189 => Weight: 1.730000 => Bias: 1.890000 => Loss: 56.797687\n",
      "Iteration  190 => Weight: 1.730000 => Bias: 1.900000 => Loss: 56.736520\n",
      "Iteration  191 => Weight: 1.730000 => Bias: 1.910000 => Loss: 56.675553\n",
      "Iteration  192 => Weight: 1.730000 => Bias: 1.920000 => Loss: 56.614787\n",
      "Iteration  193 => Weight: 1.730000 => Bias: 1.930000 => Loss: 56.554220\n",
      "Iteration  194 => Weight: 1.730000 => Bias: 1.940000 => Loss: 56.493853\n",
      "Iteration  195 => Weight: 1.730000 => Bias: 1.950000 => Loss: 56.433687\n",
      "Iteration  196 => Weight: 1.730000 => Bias: 1.960000 => Loss: 56.373720\n",
      "Iteration  197 => Weight: 1.730000 => Bias: 1.970000 => Loss: 56.313953\n",
      "Iteration  198 => Weight: 1.730000 => Bias: 1.980000 => Loss: 56.254387\n",
      "Iteration  199 => Weight: 1.730000 => Bias: 1.990000 => Loss: 56.195020\n",
      "Iteration  200 => Weight: 1.730000 => Bias: 2.000000 => Loss: 56.135853\n",
      "Iteration  201 => Weight: 1.730000 => Bias: 2.010000 => Loss: 56.076887\n",
      "Iteration  202 => Weight: 1.730000 => Bias: 2.020000 => Loss: 56.018120\n",
      "Iteration  203 => Weight: 1.730000 => Bias: 2.030000 => Loss: 55.959553\n",
      "Iteration  204 => Weight: 1.730000 => Bias: 2.040000 => Loss: 55.901187\n",
      "Iteration  205 => Weight: 1.730000 => Bias: 2.050000 => Loss: 55.843020\n",
      "Iteration  206 => Weight: 1.730000 => Bias: 2.060000 => Loss: 55.785053\n",
      "Iteration  207 => Weight: 1.720000 => Bias: 2.070000 => Loss: 55.722287\n",
      "Iteration  208 => Weight: 1.720000 => Bias: 2.080000 => Loss: 55.662187\n",
      "Iteration  209 => Weight: 1.720000 => Bias: 2.090000 => Loss: 55.602287\n",
      "Iteration  210 => Weight: 1.720000 => Bias: 2.100000 => Loss: 55.542587\n",
      "Iteration  211 => Weight: 1.720000 => Bias: 2.110000 => Loss: 55.483087\n",
      "Iteration  212 => Weight: 1.720000 => Bias: 2.120000 => Loss: 55.423787\n",
      "Iteration  213 => Weight: 1.720000 => Bias: 2.130000 => Loss: 55.364687\n",
      "Iteration  214 => Weight: 1.720000 => Bias: 2.140000 => Loss: 55.305787\n",
      "Iteration  215 => Weight: 1.720000 => Bias: 2.150000 => Loss: 55.247087\n",
      "Iteration  216 => Weight: 1.720000 => Bias: 2.160000 => Loss: 55.188587\n",
      "Iteration  217 => Weight: 1.720000 => Bias: 2.170000 => Loss: 55.130287\n",
      "Iteration  218 => Weight: 1.720000 => Bias: 2.180000 => Loss: 55.072187\n",
      "Iteration  219 => Weight: 1.720000 => Bias: 2.190000 => Loss: 55.014287\n",
      "Iteration  220 => Weight: 1.720000 => Bias: 2.200000 => Loss: 54.956587\n",
      "Iteration  221 => Weight: 1.720000 => Bias: 2.210000 => Loss: 54.899087\n",
      "Iteration  222 => Weight: 1.720000 => Bias: 2.220000 => Loss: 54.841787\n",
      "Iteration  223 => Weight: 1.720000 => Bias: 2.230000 => Loss: 54.784687\n",
      "Iteration  224 => Weight: 1.710000 => Bias: 2.240000 => Loss: 54.723480\n",
      "Iteration  225 => Weight: 1.710000 => Bias: 2.250000 => Loss: 54.664247\n",
      "Iteration  226 => Weight: 1.710000 => Bias: 2.260000 => Loss: 54.605213\n",
      "Iteration  227 => Weight: 1.710000 => Bias: 2.270000 => Loss: 54.546380\n",
      "Iteration  228 => Weight: 1.710000 => Bias: 2.280000 => Loss: 54.487747\n",
      "Iteration  229 => Weight: 1.710000 => Bias: 2.290000 => Loss: 54.429313\n",
      "Iteration  230 => Weight: 1.710000 => Bias: 2.300000 => Loss: 54.371080\n",
      "Iteration  231 => Weight: 1.710000 => Bias: 2.310000 => Loss: 54.313047\n",
      "Iteration  232 => Weight: 1.710000 => Bias: 2.320000 => Loss: 54.255213\n",
      "Iteration  233 => Weight: 1.710000 => Bias: 2.330000 => Loss: 54.197580\n",
      "Iteration  234 => Weight: 1.710000 => Bias: 2.340000 => Loss: 54.140147\n",
      "Iteration  235 => Weight: 1.710000 => Bias: 2.350000 => Loss: 54.082913\n",
      "Iteration  236 => Weight: 1.710000 => Bias: 2.360000 => Loss: 54.025880\n",
      "Iteration  237 => Weight: 1.710000 => Bias: 2.370000 => Loss: 53.969047\n",
      "Iteration  238 => Weight: 1.710000 => Bias: 2.380000 => Loss: 53.912413\n",
      "Iteration  239 => Weight: 1.710000 => Bias: 2.390000 => Loss: 53.855980\n",
      "Iteration  240 => Weight: 1.710000 => Bias: 2.400000 => Loss: 53.799747\n",
      "Iteration  241 => Weight: 1.700000 => Bias: 2.410000 => Loss: 53.740100\n",
      "Iteration  242 => Weight: 1.700000 => Bias: 2.420000 => Loss: 53.681733\n",
      "Iteration  243 => Weight: 1.700000 => Bias: 2.430000 => Loss: 53.623567\n",
      "Iteration  244 => Weight: 1.700000 => Bias: 2.440000 => Loss: 53.565600\n",
      "Iteration  245 => Weight: 1.700000 => Bias: 2.450000 => Loss: 53.507833\n",
      "Iteration  246 => Weight: 1.700000 => Bias: 2.460000 => Loss: 53.450267\n",
      "Iteration  247 => Weight: 1.700000 => Bias: 2.470000 => Loss: 53.392900\n",
      "Iteration  248 => Weight: 1.700000 => Bias: 2.480000 => Loss: 53.335733\n",
      "Iteration  249 => Weight: 1.700000 => Bias: 2.490000 => Loss: 53.278767\n",
      "Iteration  250 => Weight: 1.700000 => Bias: 2.500000 => Loss: 53.222000\n",
      "Iteration  251 => Weight: 1.700000 => Bias: 2.510000 => Loss: 53.165433\n",
      "Iteration  252 => Weight: 1.700000 => Bias: 2.520000 => Loss: 53.109067\n",
      "Iteration  253 => Weight: 1.700000 => Bias: 2.530000 => Loss: 53.052900\n",
      "Iteration  254 => Weight: 1.700000 => Bias: 2.540000 => Loss: 52.996933\n",
      "Iteration  255 => Weight: 1.700000 => Bias: 2.550000 => Loss: 52.941167\n",
      "Iteration  256 => Weight: 1.700000 => Bias: 2.560000 => Loss: 52.885600\n",
      "Iteration  257 => Weight: 1.700000 => Bias: 2.570000 => Loss: 52.830233\n",
      "Iteration  258 => Weight: 1.690000 => Bias: 2.580000 => Loss: 52.772147\n",
      "Iteration  259 => Weight: 1.690000 => Bias: 2.590000 => Loss: 52.714647\n",
      "Iteration  260 => Weight: 1.690000 => Bias: 2.600000 => Loss: 52.657347\n",
      "Iteration  261 => Weight: 1.690000 => Bias: 2.610000 => Loss: 52.600247\n",
      "Iteration  262 => Weight: 1.690000 => Bias: 2.620000 => Loss: 52.543347\n",
      "Iteration  263 => Weight: 1.690000 => Bias: 2.630000 => Loss: 52.486647\n",
      "Iteration  264 => Weight: 1.690000 => Bias: 2.640000 => Loss: 52.430147\n",
      "Iteration  265 => Weight: 1.690000 => Bias: 2.650000 => Loss: 52.373847\n",
      "Iteration  266 => Weight: 1.690000 => Bias: 2.660000 => Loss: 52.317747\n",
      "Iteration  267 => Weight: 1.690000 => Bias: 2.670000 => Loss: 52.261847\n",
      "Iteration  268 => Weight: 1.690000 => Bias: 2.680000 => Loss: 52.206147\n",
      "Iteration  269 => Weight: 1.690000 => Bias: 2.690000 => Loss: 52.150647\n",
      "Iteration  270 => Weight: 1.690000 => Bias: 2.700000 => Loss: 52.095347\n",
      "Iteration  271 => Weight: 1.690000 => Bias: 2.710000 => Loss: 52.040247\n",
      "Iteration  272 => Weight: 1.690000 => Bias: 2.720000 => Loss: 51.985347\n",
      "Iteration  273 => Weight: 1.690000 => Bias: 2.730000 => Loss: 51.930647\n",
      "Iteration  274 => Weight: 1.690000 => Bias: 2.740000 => Loss: 51.876147\n",
      "Iteration  275 => Weight: 1.690000 => Bias: 2.750000 => Loss: 51.821847\n",
      "Iteration  276 => Weight: 1.680000 => Bias: 2.760000 => Loss: 51.762987\n",
      "Iteration  277 => Weight: 1.680000 => Bias: 2.770000 => Loss: 51.706553\n",
      "Iteration  278 => Weight: 1.680000 => Bias: 2.780000 => Loss: 51.650320\n",
      "Iteration  279 => Weight: 1.680000 => Bias: 2.790000 => Loss: 51.594287\n",
      "Iteration  280 => Weight: 1.680000 => Bias: 2.800000 => Loss: 51.538453\n",
      "Iteration  281 => Weight: 1.680000 => Bias: 2.810000 => Loss: 51.482820\n",
      "Iteration  282 => Weight: 1.680000 => Bias: 2.820000 => Loss: 51.427387\n",
      "Iteration  283 => Weight: 1.680000 => Bias: 2.830000 => Loss: 51.372153\n",
      "Iteration  284 => Weight: 1.680000 => Bias: 2.840000 => Loss: 51.317120\n",
      "Iteration  285 => Weight: 1.680000 => Bias: 2.850000 => Loss: 51.262287\n",
      "Iteration  286 => Weight: 1.680000 => Bias: 2.860000 => Loss: 51.207653\n",
      "Iteration  287 => Weight: 1.680000 => Bias: 2.870000 => Loss: 51.153220\n",
      "Iteration  288 => Weight: 1.680000 => Bias: 2.880000 => Loss: 51.098987\n",
      "Iteration  289 => Weight: 1.680000 => Bias: 2.890000 => Loss: 51.044953\n",
      "Iteration  290 => Weight: 1.680000 => Bias: 2.900000 => Loss: 50.991120\n",
      "Iteration  291 => Weight: 1.680000 => Bias: 2.910000 => Loss: 50.937487\n",
      "Iteration  292 => Weight: 1.680000 => Bias: 2.920000 => Loss: 50.884053\n",
      "Iteration  293 => Weight: 1.670000 => Bias: 2.930000 => Loss: 50.826753\n",
      "Iteration  294 => Weight: 1.670000 => Bias: 2.940000 => Loss: 50.771187\n",
      "Iteration  295 => Weight: 1.670000 => Bias: 2.950000 => Loss: 50.715820\n",
      "Iteration  296 => Weight: 1.670000 => Bias: 2.960000 => Loss: 50.660653\n",
      "Iteration  297 => Weight: 1.670000 => Bias: 2.970000 => Loss: 50.605687\n",
      "Iteration  298 => Weight: 1.670000 => Bias: 2.980000 => Loss: 50.550920\n",
      "Iteration  299 => Weight: 1.670000 => Bias: 2.990000 => Loss: 50.496353\n",
      "Iteration  300 => Weight: 1.670000 => Bias: 3.000000 => Loss: 50.441987\n",
      "Iteration  301 => Weight: 1.670000 => Bias: 3.010000 => Loss: 50.387820\n",
      "Iteration  302 => Weight: 1.670000 => Bias: 3.020000 => Loss: 50.333853\n",
      "Iteration  303 => Weight: 1.670000 => Bias: 3.030000 => Loss: 50.280087\n",
      "Iteration  304 => Weight: 1.670000 => Bias: 3.040000 => Loss: 50.226520\n",
      "Iteration  305 => Weight: 1.670000 => Bias: 3.050000 => Loss: 50.173153\n",
      "Iteration  306 => Weight: 1.670000 => Bias: 3.060000 => Loss: 50.119987\n",
      "Iteration  307 => Weight: 1.670000 => Bias: 3.070000 => Loss: 50.067020\n",
      "Iteration  308 => Weight: 1.670000 => Bias: 3.080000 => Loss: 50.014253\n",
      "Iteration  309 => Weight: 1.670000 => Bias: 3.090000 => Loss: 49.961687\n",
      "Iteration  310 => Weight: 1.660000 => Bias: 3.100000 => Loss: 49.905947\n",
      "Iteration  311 => Weight: 1.660000 => Bias: 3.110000 => Loss: 49.851247\n",
      "Iteration  312 => Weight: 1.660000 => Bias: 3.120000 => Loss: 49.796747\n",
      "Iteration  313 => Weight: 1.660000 => Bias: 3.130000 => Loss: 49.742447\n",
      "Iteration  314 => Weight: 1.660000 => Bias: 3.140000 => Loss: 49.688347\n",
      "Iteration  315 => Weight: 1.660000 => Bias: 3.150000 => Loss: 49.634447\n",
      "Iteration  316 => Weight: 1.660000 => Bias: 3.160000 => Loss: 49.580747\n",
      "Iteration  317 => Weight: 1.660000 => Bias: 3.170000 => Loss: 49.527247\n",
      "Iteration  318 => Weight: 1.660000 => Bias: 3.180000 => Loss: 49.473947\n",
      "Iteration  319 => Weight: 1.660000 => Bias: 3.190000 => Loss: 49.420847\n",
      "Iteration  320 => Weight: 1.660000 => Bias: 3.200000 => Loss: 49.367947\n",
      "Iteration  321 => Weight: 1.660000 => Bias: 3.210000 => Loss: 49.315247\n",
      "Iteration  322 => Weight: 1.660000 => Bias: 3.220000 => Loss: 49.262747\n",
      "Iteration  323 => Weight: 1.660000 => Bias: 3.230000 => Loss: 49.210447\n",
      "Iteration  324 => Weight: 1.660000 => Bias: 3.240000 => Loss: 49.158347\n",
      "Iteration  325 => Weight: 1.660000 => Bias: 3.250000 => Loss: 49.106447\n",
      "Iteration  326 => Weight: 1.660000 => Bias: 3.260000 => Loss: 49.054747\n",
      "Iteration  327 => Weight: 1.650000 => Bias: 3.270000 => Loss: 49.000567\n",
      "Iteration  328 => Weight: 1.650000 => Bias: 3.280000 => Loss: 48.946733\n",
      "Iteration  329 => Weight: 1.650000 => Bias: 3.290000 => Loss: 48.893100\n",
      "Iteration  330 => Weight: 1.650000 => Bias: 3.300000 => Loss: 48.839667\n",
      "Iteration  331 => Weight: 1.650000 => Bias: 3.310000 => Loss: 48.786433\n",
      "Iteration  332 => Weight: 1.650000 => Bias: 3.320000 => Loss: 48.733400\n",
      "Iteration  333 => Weight: 1.650000 => Bias: 3.330000 => Loss: 48.680567\n",
      "Iteration  334 => Weight: 1.650000 => Bias: 3.340000 => Loss: 48.627933\n",
      "Iteration  335 => Weight: 1.650000 => Bias: 3.350000 => Loss: 48.575500\n",
      "Iteration  336 => Weight: 1.650000 => Bias: 3.360000 => Loss: 48.523267\n",
      "Iteration  337 => Weight: 1.650000 => Bias: 3.370000 => Loss: 48.471233\n",
      "Iteration  338 => Weight: 1.650000 => Bias: 3.380000 => Loss: 48.419400\n",
      "Iteration  339 => Weight: 1.650000 => Bias: 3.390000 => Loss: 48.367767\n",
      "Iteration  340 => Weight: 1.650000 => Bias: 3.400000 => Loss: 48.316333\n",
      "Iteration  341 => Weight: 1.650000 => Bias: 3.410000 => Loss: 48.265100\n",
      "Iteration  342 => Weight: 1.650000 => Bias: 3.420000 => Loss: 48.214067\n",
      "Iteration  343 => Weight: 1.650000 => Bias: 3.430000 => Loss: 48.163233\n",
      "Iteration  344 => Weight: 1.650000 => Bias: 3.440000 => Loss: 48.112600\n",
      "Iteration  345 => Weight: 1.640000 => Bias: 3.450000 => Loss: 48.057647\n",
      "Iteration  346 => Weight: 1.640000 => Bias: 3.460000 => Loss: 48.004880\n",
      "Iteration  347 => Weight: 1.640000 => Bias: 3.470000 => Loss: 47.952313\n",
      "Iteration  348 => Weight: 1.640000 => Bias: 3.480000 => Loss: 47.899947\n",
      "Iteration  349 => Weight: 1.640000 => Bias: 3.490000 => Loss: 47.847780\n",
      "Iteration  350 => Weight: 1.640000 => Bias: 3.500000 => Loss: 47.795813\n",
      "Iteration  351 => Weight: 1.640000 => Bias: 3.510000 => Loss: 47.744047\n",
      "Iteration  352 => Weight: 1.640000 => Bias: 3.520000 => Loss: 47.692480\n",
      "Iteration  353 => Weight: 1.640000 => Bias: 3.530000 => Loss: 47.641113\n",
      "Iteration  354 => Weight: 1.640000 => Bias: 3.540000 => Loss: 47.589947\n",
      "Iteration  355 => Weight: 1.640000 => Bias: 3.550000 => Loss: 47.538980\n",
      "Iteration  356 => Weight: 1.640000 => Bias: 3.560000 => Loss: 47.488213\n",
      "Iteration  357 => Weight: 1.640000 => Bias: 3.570000 => Loss: 47.437647\n",
      "Iteration  358 => Weight: 1.640000 => Bias: 3.580000 => Loss: 47.387280\n",
      "Iteration  359 => Weight: 1.640000 => Bias: 3.590000 => Loss: 47.337113\n",
      "Iteration  360 => Weight: 1.640000 => Bias: 3.600000 => Loss: 47.287147\n",
      "Iteration  361 => Weight: 1.640000 => Bias: 3.610000 => Loss: 47.237380\n",
      "Iteration  362 => Weight: 1.630000 => Bias: 3.620000 => Loss: 47.183987\n",
      "Iteration  363 => Weight: 1.630000 => Bias: 3.630000 => Loss: 47.132087\n",
      "Iteration  364 => Weight: 1.630000 => Bias: 3.640000 => Loss: 47.080387\n",
      "Iteration  365 => Weight: 1.630000 => Bias: 3.650000 => Loss: 47.028887\n",
      "Iteration  366 => Weight: 1.630000 => Bias: 3.660000 => Loss: 46.977587\n",
      "Iteration  367 => Weight: 1.630000 => Bias: 3.670000 => Loss: 46.926487\n",
      "Iteration  368 => Weight: 1.630000 => Bias: 3.680000 => Loss: 46.875587\n",
      "Iteration  369 => Weight: 1.630000 => Bias: 3.690000 => Loss: 46.824887\n",
      "Iteration  370 => Weight: 1.630000 => Bias: 3.700000 => Loss: 46.774387\n",
      "Iteration  371 => Weight: 1.630000 => Bias: 3.710000 => Loss: 46.724087\n",
      "Iteration  372 => Weight: 1.630000 => Bias: 3.720000 => Loss: 46.673987\n",
      "Iteration  373 => Weight: 1.630000 => Bias: 3.730000 => Loss: 46.624087\n",
      "Iteration  374 => Weight: 1.630000 => Bias: 3.740000 => Loss: 46.574387\n",
      "Iteration  375 => Weight: 1.630000 => Bias: 3.750000 => Loss: 46.524887\n",
      "Iteration  376 => Weight: 1.630000 => Bias: 3.760000 => Loss: 46.475587\n",
      "Iteration  377 => Weight: 1.630000 => Bias: 3.770000 => Loss: 46.426487\n",
      "Iteration  378 => Weight: 1.630000 => Bias: 3.780000 => Loss: 46.377587\n",
      "Iteration  379 => Weight: 1.620000 => Bias: 3.790000 => Loss: 46.325753\n",
      "Iteration  380 => Weight: 1.620000 => Bias: 3.800000 => Loss: 46.274720\n",
      "Iteration  381 => Weight: 1.620000 => Bias: 3.810000 => Loss: 46.223887\n",
      "Iteration  382 => Weight: 1.620000 => Bias: 3.820000 => Loss: 46.173253\n",
      "Iteration  383 => Weight: 1.620000 => Bias: 3.830000 => Loss: 46.122820\n",
      "Iteration  384 => Weight: 1.620000 => Bias: 3.840000 => Loss: 46.072587\n",
      "Iteration  385 => Weight: 1.620000 => Bias: 3.850000 => Loss: 46.022553\n",
      "Iteration  386 => Weight: 1.620000 => Bias: 3.860000 => Loss: 45.972720\n",
      "Iteration  387 => Weight: 1.620000 => Bias: 3.870000 => Loss: 45.923087\n",
      "Iteration  388 => Weight: 1.620000 => Bias: 3.880000 => Loss: 45.873653\n",
      "Iteration  389 => Weight: 1.620000 => Bias: 3.890000 => Loss: 45.824420\n",
      "Iteration  390 => Weight: 1.620000 => Bias: 3.900000 => Loss: 45.775387\n",
      "Iteration  391 => Weight: 1.620000 => Bias: 3.910000 => Loss: 45.726553\n",
      "Iteration  392 => Weight: 1.620000 => Bias: 3.920000 => Loss: 45.677920\n",
      "Iteration  393 => Weight: 1.620000 => Bias: 3.930000 => Loss: 45.629487\n",
      "Iteration  394 => Weight: 1.620000 => Bias: 3.940000 => Loss: 45.581253\n",
      "Iteration  395 => Weight: 1.620000 => Bias: 3.950000 => Loss: 45.533220\n",
      "Iteration  396 => Weight: 1.620000 => Bias: 3.960000 => Loss: 45.485387\n",
      "Iteration  397 => Weight: 1.610000 => Bias: 3.970000 => Loss: 45.432780\n",
      "Iteration  398 => Weight: 1.610000 => Bias: 3.980000 => Loss: 45.382813\n",
      "Iteration  399 => Weight: 1.610000 => Bias: 3.990000 => Loss: 45.333047\n",
      "Iteration  400 => Weight: 1.610000 => Bias: 4.000000 => Loss: 45.283480\n",
      "Iteration  401 => Weight: 1.610000 => Bias: 4.010000 => Loss: 45.234113\n",
      "Iteration  402 => Weight: 1.610000 => Bias: 4.020000 => Loss: 45.184947\n",
      "Iteration  403 => Weight: 1.610000 => Bias: 4.030000 => Loss: 45.135980\n",
      "Iteration  404 => Weight: 1.610000 => Bias: 4.040000 => Loss: 45.087213\n",
      "Iteration  405 => Weight: 1.610000 => Bias: 4.050000 => Loss: 45.038647\n",
      "Iteration  406 => Weight: 1.610000 => Bias: 4.060000 => Loss: 44.990280\n",
      "Iteration  407 => Weight: 1.610000 => Bias: 4.070000 => Loss: 44.942113\n",
      "Iteration  408 => Weight: 1.610000 => Bias: 4.080000 => Loss: 44.894147\n",
      "Iteration  409 => Weight: 1.610000 => Bias: 4.090000 => Loss: 44.846380\n",
      "Iteration  410 => Weight: 1.610000 => Bias: 4.100000 => Loss: 44.798813\n",
      "Iteration  411 => Weight: 1.610000 => Bias: 4.110000 => Loss: 44.751447\n",
      "Iteration  412 => Weight: 1.610000 => Bias: 4.120000 => Loss: 44.704280\n",
      "Iteration  413 => Weight: 1.610000 => Bias: 4.130000 => Loss: 44.657313\n",
      "Iteration  414 => Weight: 1.600000 => Bias: 4.140000 => Loss: 44.606267\n",
      "Iteration  415 => Weight: 1.600000 => Bias: 4.150000 => Loss: 44.557167\n",
      "Iteration  416 => Weight: 1.600000 => Bias: 4.160000 => Loss: 44.508267\n",
      "Iteration  417 => Weight: 1.600000 => Bias: 4.170000 => Loss: 44.459567\n",
      "Iteration  418 => Weight: 1.600000 => Bias: 4.180000 => Loss: 44.411067\n",
      "Iteration  419 => Weight: 1.600000 => Bias: 4.190000 => Loss: 44.362767\n",
      "Iteration  420 => Weight: 1.600000 => Bias: 4.200000 => Loss: 44.314667\n",
      "Iteration  421 => Weight: 1.600000 => Bias: 4.210000 => Loss: 44.266767\n",
      "Iteration  422 => Weight: 1.600000 => Bias: 4.220000 => Loss: 44.219067\n",
      "Iteration  423 => Weight: 1.600000 => Bias: 4.230000 => Loss: 44.171567\n",
      "Iteration  424 => Weight: 1.600000 => Bias: 4.240000 => Loss: 44.124267\n",
      "Iteration  425 => Weight: 1.600000 => Bias: 4.250000 => Loss: 44.077167\n",
      "Iteration  426 => Weight: 1.600000 => Bias: 4.260000 => Loss: 44.030267\n",
      "Iteration  427 => Weight: 1.600000 => Bias: 4.270000 => Loss: 43.983567\n",
      "Iteration  428 => Weight: 1.600000 => Bias: 4.280000 => Loss: 43.937067\n",
      "Iteration  429 => Weight: 1.600000 => Bias: 4.290000 => Loss: 43.890767\n",
      "Iteration  430 => Weight: 1.600000 => Bias: 4.300000 => Loss: 43.844667\n",
      "Iteration  431 => Weight: 1.590000 => Bias: 4.310000 => Loss: 43.795180\n",
      "Iteration  432 => Weight: 1.590000 => Bias: 4.320000 => Loss: 43.746947\n",
      "Iteration  433 => Weight: 1.590000 => Bias: 4.330000 => Loss: 43.698913\n",
      "Iteration  434 => Weight: 1.590000 => Bias: 4.340000 => Loss: 43.651080\n",
      "Iteration  435 => Weight: 1.590000 => Bias: 4.350000 => Loss: 43.603447\n",
      "Iteration  436 => Weight: 1.590000 => Bias: 4.360000 => Loss: 43.556013\n",
      "Iteration  437 => Weight: 1.590000 => Bias: 4.370000 => Loss: 43.508780\n",
      "Iteration  438 => Weight: 1.590000 => Bias: 4.380000 => Loss: 43.461747\n",
      "Iteration  439 => Weight: 1.590000 => Bias: 4.390000 => Loss: 43.414913\n",
      "Iteration  440 => Weight: 1.590000 => Bias: 4.400000 => Loss: 43.368280\n",
      "Iteration  441 => Weight: 1.590000 => Bias: 4.410000 => Loss: 43.321847\n",
      "Iteration  442 => Weight: 1.590000 => Bias: 4.420000 => Loss: 43.275613\n",
      "Iteration  443 => Weight: 1.590000 => Bias: 4.430000 => Loss: 43.229580\n",
      "Iteration  444 => Weight: 1.590000 => Bias: 4.440000 => Loss: 43.183747\n",
      "Iteration  445 => Weight: 1.590000 => Bias: 4.450000 => Loss: 43.138113\n",
      "Iteration  446 => Weight: 1.590000 => Bias: 4.460000 => Loss: 43.092680\n",
      "Iteration  447 => Weight: 1.590000 => Bias: 4.470000 => Loss: 43.047447\n",
      "Iteration  448 => Weight: 1.580000 => Bias: 4.480000 => Loss: 42.999520\n",
      "Iteration  449 => Weight: 1.580000 => Bias: 4.490000 => Loss: 42.952153\n",
      "Iteration  450 => Weight: 1.580000 => Bias: 4.500000 => Loss: 42.904987\n",
      "Iteration  451 => Weight: 1.580000 => Bias: 4.510000 => Loss: 42.858020\n",
      "Iteration  452 => Weight: 1.580000 => Bias: 4.520000 => Loss: 42.811253\n",
      "Iteration  453 => Weight: 1.580000 => Bias: 4.530000 => Loss: 42.764687\n",
      "Iteration  454 => Weight: 1.580000 => Bias: 4.540000 => Loss: 42.718320\n",
      "Iteration  455 => Weight: 1.580000 => Bias: 4.550000 => Loss: 42.672153\n",
      "Iteration  456 => Weight: 1.580000 => Bias: 4.560000 => Loss: 42.626187\n",
      "Iteration  457 => Weight: 1.580000 => Bias: 4.570000 => Loss: 42.580420\n",
      "Iteration  458 => Weight: 1.580000 => Bias: 4.580000 => Loss: 42.534853\n",
      "Iteration  459 => Weight: 1.580000 => Bias: 4.590000 => Loss: 42.489487\n",
      "Iteration  460 => Weight: 1.580000 => Bias: 4.600000 => Loss: 42.444320\n",
      "Iteration  461 => Weight: 1.580000 => Bias: 4.610000 => Loss: 42.399353\n",
      "Iteration  462 => Weight: 1.580000 => Bias: 4.620000 => Loss: 42.354587\n",
      "Iteration  463 => Weight: 1.580000 => Bias: 4.630000 => Loss: 42.310020\n",
      "Iteration  464 => Weight: 1.580000 => Bias: 4.640000 => Loss: 42.265653\n",
      "Iteration  465 => Weight: 1.580000 => Bias: 4.650000 => Loss: 42.221487\n",
      "Iteration  466 => Weight: 1.570000 => Bias: 4.660000 => Loss: 42.172787\n",
      "Iteration  467 => Weight: 1.570000 => Bias: 4.670000 => Loss: 42.126487\n",
      "Iteration  468 => Weight: 1.570000 => Bias: 4.680000 => Loss: 42.080387\n",
      "Iteration  469 => Weight: 1.570000 => Bias: 4.690000 => Loss: 42.034487\n",
      "Iteration  470 => Weight: 1.570000 => Bias: 4.700000 => Loss: 41.988787\n",
      "Iteration  471 => Weight: 1.570000 => Bias: 4.710000 => Loss: 41.943287\n",
      "Iteration  472 => Weight: 1.570000 => Bias: 4.720000 => Loss: 41.897987\n",
      "Iteration  473 => Weight: 1.570000 => Bias: 4.730000 => Loss: 41.852887\n",
      "Iteration  474 => Weight: 1.570000 => Bias: 4.740000 => Loss: 41.807987\n",
      "Iteration  475 => Weight: 1.570000 => Bias: 4.750000 => Loss: 41.763287\n",
      "Iteration  476 => Weight: 1.570000 => Bias: 4.760000 => Loss: 41.718787\n",
      "Iteration  477 => Weight: 1.570000 => Bias: 4.770000 => Loss: 41.674487\n",
      "Iteration  478 => Weight: 1.570000 => Bias: 4.780000 => Loss: 41.630387\n",
      "Iteration  479 => Weight: 1.570000 => Bias: 4.790000 => Loss: 41.586487\n",
      "Iteration  480 => Weight: 1.570000 => Bias: 4.800000 => Loss: 41.542787\n",
      "Iteration  481 => Weight: 1.570000 => Bias: 4.810000 => Loss: 41.499287\n",
      "Iteration  482 => Weight: 1.570000 => Bias: 4.820000 => Loss: 41.455987\n",
      "Iteration  483 => Weight: 1.560000 => Bias: 4.830000 => Loss: 41.408847\n",
      "Iteration  484 => Weight: 1.560000 => Bias: 4.840000 => Loss: 41.363413\n",
      "Iteration  485 => Weight: 1.560000 => Bias: 4.850000 => Loss: 41.318180\n",
      "Iteration  486 => Weight: 1.560000 => Bias: 4.860000 => Loss: 41.273147\n",
      "Iteration  487 => Weight: 1.560000 => Bias: 4.870000 => Loss: 41.228313\n",
      "Iteration  488 => Weight: 1.560000 => Bias: 4.880000 => Loss: 41.183680\n",
      "Iteration  489 => Weight: 1.560000 => Bias: 4.890000 => Loss: 41.139247\n",
      "Iteration  490 => Weight: 1.560000 => Bias: 4.900000 => Loss: 41.095013\n",
      "Iteration  491 => Weight: 1.560000 => Bias: 4.910000 => Loss: 41.050980\n",
      "Iteration  492 => Weight: 1.560000 => Bias: 4.920000 => Loss: 41.007147\n",
      "Iteration  493 => Weight: 1.560000 => Bias: 4.930000 => Loss: 40.963513\n",
      "Iteration  494 => Weight: 1.560000 => Bias: 4.940000 => Loss: 40.920080\n",
      "Iteration  495 => Weight: 1.560000 => Bias: 4.950000 => Loss: 40.876847\n",
      "Iteration  496 => Weight: 1.560000 => Bias: 4.960000 => Loss: 40.833813\n",
      "Iteration  497 => Weight: 1.560000 => Bias: 4.970000 => Loss: 40.790980\n",
      "Iteration  498 => Weight: 1.560000 => Bias: 4.980000 => Loss: 40.748347\n",
      "Iteration  499 => Weight: 1.560000 => Bias: 4.990000 => Loss: 40.705913\n",
      "Iteration  500 => Weight: 1.550000 => Bias: 5.000000 => Loss: 40.660333\n",
      "Iteration  501 => Weight: 1.550000 => Bias: 5.010000 => Loss: 40.615767\n",
      "Iteration  502 => Weight: 1.550000 => Bias: 5.020000 => Loss: 40.571400\n",
      "Iteration  503 => Weight: 1.550000 => Bias: 5.030000 => Loss: 40.527233\n",
      "Iteration  504 => Weight: 1.550000 => Bias: 5.040000 => Loss: 40.483267\n",
      "Iteration  505 => Weight: 1.550000 => Bias: 5.050000 => Loss: 40.439500\n",
      "Iteration  506 => Weight: 1.550000 => Bias: 5.060000 => Loss: 40.395933\n",
      "Iteration  507 => Weight: 1.550000 => Bias: 5.070000 => Loss: 40.352567\n",
      "Iteration  508 => Weight: 1.550000 => Bias: 5.080000 => Loss: 40.309400\n",
      "Iteration  509 => Weight: 1.550000 => Bias: 5.090000 => Loss: 40.266433\n",
      "Iteration  510 => Weight: 1.550000 => Bias: 5.100000 => Loss: 40.223667\n",
      "Iteration  511 => Weight: 1.550000 => Bias: 5.110000 => Loss: 40.181100\n",
      "Iteration  512 => Weight: 1.550000 => Bias: 5.120000 => Loss: 40.138733\n",
      "Iteration  513 => Weight: 1.550000 => Bias: 5.130000 => Loss: 40.096567\n",
      "Iteration  514 => Weight: 1.550000 => Bias: 5.140000 => Loss: 40.054600\n",
      "Iteration  515 => Weight: 1.550000 => Bias: 5.150000 => Loss: 40.012833\n",
      "Iteration  516 => Weight: 1.550000 => Bias: 5.160000 => Loss: 39.971267\n",
      "Iteration  517 => Weight: 1.540000 => Bias: 5.170000 => Loss: 39.927247\n",
      "Iteration  518 => Weight: 1.540000 => Bias: 5.180000 => Loss: 39.883547\n",
      "Iteration  519 => Weight: 1.540000 => Bias: 5.190000 => Loss: 39.840047\n",
      "Iteration  520 => Weight: 1.540000 => Bias: 5.200000 => Loss: 39.796747\n",
      "Iteration  521 => Weight: 1.540000 => Bias: 5.210000 => Loss: 39.753647\n",
      "Iteration  522 => Weight: 1.540000 => Bias: 5.220000 => Loss: 39.710747\n",
      "Iteration  523 => Weight: 1.540000 => Bias: 5.230000 => Loss: 39.668047\n",
      "Iteration  524 => Weight: 1.540000 => Bias: 5.240000 => Loss: 39.625547\n",
      "Iteration  525 => Weight: 1.540000 => Bias: 5.250000 => Loss: 39.583247\n",
      "Iteration  526 => Weight: 1.540000 => Bias: 5.260000 => Loss: 39.541147\n",
      "Iteration  527 => Weight: 1.540000 => Bias: 5.270000 => Loss: 39.499247\n",
      "Iteration  528 => Weight: 1.540000 => Bias: 5.280000 => Loss: 39.457547\n",
      "Iteration  529 => Weight: 1.540000 => Bias: 5.290000 => Loss: 39.416047\n",
      "Iteration  530 => Weight: 1.540000 => Bias: 5.300000 => Loss: 39.374747\n",
      "Iteration  531 => Weight: 1.540000 => Bias: 5.310000 => Loss: 39.333647\n",
      "Iteration  532 => Weight: 1.540000 => Bias: 5.320000 => Loss: 39.292747\n",
      "Iteration  533 => Weight: 1.540000 => Bias: 5.330000 => Loss: 39.252047\n",
      "Iteration  534 => Weight: 1.540000 => Bias: 5.340000 => Loss: 39.211547\n",
      "Iteration  535 => Weight: 1.530000 => Bias: 5.350000 => Loss: 39.166753\n",
      "Iteration  536 => Weight: 1.530000 => Bias: 5.360000 => Loss: 39.124120\n",
      "Iteration  537 => Weight: 1.530000 => Bias: 5.370000 => Loss: 39.081687\n",
      "Iteration  538 => Weight: 1.530000 => Bias: 5.380000 => Loss: 39.039453\n",
      "Iteration  539 => Weight: 1.530000 => Bias: 5.390000 => Loss: 38.997420\n",
      "Iteration  540 => Weight: 1.530000 => Bias: 5.400000 => Loss: 38.955587\n",
      "Iteration  541 => Weight: 1.530000 => Bias: 5.410000 => Loss: 38.913953\n",
      "Iteration  542 => Weight: 1.530000 => Bias: 5.420000 => Loss: 38.872520\n",
      "Iteration  543 => Weight: 1.530000 => Bias: 5.430000 => Loss: 38.831287\n",
      "Iteration  544 => Weight: 1.530000 => Bias: 5.440000 => Loss: 38.790253\n",
      "Iteration  545 => Weight: 1.530000 => Bias: 5.450000 => Loss: 38.749420\n",
      "Iteration  546 => Weight: 1.530000 => Bias: 5.460000 => Loss: 38.708787\n",
      "Iteration  547 => Weight: 1.530000 => Bias: 5.470000 => Loss: 38.668353\n",
      "Iteration  548 => Weight: 1.530000 => Bias: 5.480000 => Loss: 38.628120\n",
      "Iteration  549 => Weight: 1.530000 => Bias: 5.490000 => Loss: 38.588087\n",
      "Iteration  550 => Weight: 1.530000 => Bias: 5.500000 => Loss: 38.548253\n",
      "Iteration  551 => Weight: 1.530000 => Bias: 5.510000 => Loss: 38.508620\n",
      "Iteration  552 => Weight: 1.520000 => Bias: 5.520000 => Loss: 38.465387\n",
      "Iteration  553 => Weight: 1.520000 => Bias: 5.530000 => Loss: 38.423620\n",
      "Iteration  554 => Weight: 1.520000 => Bias: 5.540000 => Loss: 38.382053\n",
      "Iteration  555 => Weight: 1.520000 => Bias: 5.550000 => Loss: 38.340687\n",
      "Iteration  556 => Weight: 1.520000 => Bias: 5.560000 => Loss: 38.299520\n",
      "Iteration  557 => Weight: 1.520000 => Bias: 5.570000 => Loss: 38.258553\n",
      "Iteration  558 => Weight: 1.520000 => Bias: 5.580000 => Loss: 38.217787\n",
      "Iteration  559 => Weight: 1.520000 => Bias: 5.590000 => Loss: 38.177220\n",
      "Iteration  560 => Weight: 1.520000 => Bias: 5.600000 => Loss: 38.136853\n",
      "Iteration  561 => Weight: 1.520000 => Bias: 5.610000 => Loss: 38.096687\n",
      "Iteration  562 => Weight: 1.520000 => Bias: 5.620000 => Loss: 38.056720\n",
      "Iteration  563 => Weight: 1.520000 => Bias: 5.630000 => Loss: 38.016953\n",
      "Iteration  564 => Weight: 1.520000 => Bias: 5.640000 => Loss: 37.977387\n",
      "Iteration  565 => Weight: 1.520000 => Bias: 5.650000 => Loss: 37.938020\n",
      "Iteration  566 => Weight: 1.520000 => Bias: 5.660000 => Loss: 37.898853\n",
      "Iteration  567 => Weight: 1.520000 => Bias: 5.670000 => Loss: 37.859887\n",
      "Iteration  568 => Weight: 1.520000 => Bias: 5.680000 => Loss: 37.821120\n",
      "Iteration  569 => Weight: 1.510000 => Bias: 5.690000 => Loss: 37.779447\n",
      "Iteration  570 => Weight: 1.510000 => Bias: 5.700000 => Loss: 37.738547\n",
      "Iteration  571 => Weight: 1.510000 => Bias: 5.710000 => Loss: 37.697847\n",
      "Iteration  572 => Weight: 1.510000 => Bias: 5.720000 => Loss: 37.657347\n",
      "Iteration  573 => Weight: 1.510000 => Bias: 5.730000 => Loss: 37.617047\n",
      "Iteration  574 => Weight: 1.510000 => Bias: 5.740000 => Loss: 37.576947\n",
      "Iteration  575 => Weight: 1.510000 => Bias: 5.750000 => Loss: 37.537047\n",
      "Iteration  576 => Weight: 1.510000 => Bias: 5.760000 => Loss: 37.497347\n",
      "Iteration  577 => Weight: 1.510000 => Bias: 5.770000 => Loss: 37.457847\n",
      "Iteration  578 => Weight: 1.510000 => Bias: 5.780000 => Loss: 37.418547\n",
      "Iteration  579 => Weight: 1.510000 => Bias: 5.790000 => Loss: 37.379447\n",
      "Iteration  580 => Weight: 1.510000 => Bias: 5.800000 => Loss: 37.340547\n",
      "Iteration  581 => Weight: 1.510000 => Bias: 5.810000 => Loss: 37.301847\n",
      "Iteration  582 => Weight: 1.510000 => Bias: 5.820000 => Loss: 37.263347\n",
      "Iteration  583 => Weight: 1.510000 => Bias: 5.830000 => Loss: 37.225047\n",
      "Iteration  584 => Weight: 1.510000 => Bias: 5.840000 => Loss: 37.186947\n",
      "Iteration  585 => Weight: 1.510000 => Bias: 5.850000 => Loss: 37.149047\n",
      "Iteration  586 => Weight: 1.510000 => Bias: 5.860000 => Loss: 37.111347\n",
      "Iteration  587 => Weight: 1.500000 => Bias: 5.870000 => Loss: 37.068900\n",
      "Iteration  588 => Weight: 1.500000 => Bias: 5.880000 => Loss: 37.029067\n",
      "Iteration  589 => Weight: 1.500000 => Bias: 5.890000 => Loss: 36.989433\n",
      "Iteration  590 => Weight: 1.500000 => Bias: 5.900000 => Loss: 36.950000\n",
      "Iteration  591 => Weight: 1.500000 => Bias: 5.910000 => Loss: 36.910767\n",
      "Iteration  592 => Weight: 1.500000 => Bias: 5.920000 => Loss: 36.871733\n",
      "Iteration  593 => Weight: 1.500000 => Bias: 5.930000 => Loss: 36.832900\n",
      "Iteration  594 => Weight: 1.500000 => Bias: 5.940000 => Loss: 36.794267\n",
      "Iteration  595 => Weight: 1.500000 => Bias: 5.950000 => Loss: 36.755833\n",
      "Iteration  596 => Weight: 1.500000 => Bias: 5.960000 => Loss: 36.717600\n",
      "Iteration  597 => Weight: 1.500000 => Bias: 5.970000 => Loss: 36.679567\n",
      "Iteration  598 => Weight: 1.500000 => Bias: 5.980000 => Loss: 36.641733\n",
      "Iteration  599 => Weight: 1.500000 => Bias: 5.990000 => Loss: 36.604100\n",
      "Iteration  600 => Weight: 1.500000 => Bias: 6.000000 => Loss: 36.566667\n",
      "Iteration  601 => Weight: 1.500000 => Bias: 6.010000 => Loss: 36.529433\n",
      "Iteration  602 => Weight: 1.500000 => Bias: 6.020000 => Loss: 36.492400\n",
      "Iteration  603 => Weight: 1.500000 => Bias: 6.030000 => Loss: 36.455567\n",
      "Iteration  604 => Weight: 1.490000 => Bias: 6.040000 => Loss: 36.414680\n",
      "Iteration  605 => Weight: 1.490000 => Bias: 6.050000 => Loss: 36.375713\n",
      "Iteration  606 => Weight: 1.490000 => Bias: 6.060000 => Loss: 36.336947\n",
      "Iteration  607 => Weight: 1.490000 => Bias: 6.070000 => Loss: 36.298380\n",
      "Iteration  608 => Weight: 1.490000 => Bias: 6.080000 => Loss: 36.260013\n",
      "Iteration  609 => Weight: 1.490000 => Bias: 6.090000 => Loss: 36.221847\n",
      "Iteration  610 => Weight: 1.490000 => Bias: 6.100000 => Loss: 36.183880\n",
      "Iteration  611 => Weight: 1.490000 => Bias: 6.110000 => Loss: 36.146113\n",
      "Iteration  612 => Weight: 1.490000 => Bias: 6.120000 => Loss: 36.108547\n",
      "Iteration  613 => Weight: 1.490000 => Bias: 6.130000 => Loss: 36.071180\n",
      "Iteration  614 => Weight: 1.490000 => Bias: 6.140000 => Loss: 36.034013\n",
      "Iteration  615 => Weight: 1.490000 => Bias: 6.150000 => Loss: 35.997047\n",
      "Iteration  616 => Weight: 1.490000 => Bias: 6.160000 => Loss: 35.960280\n",
      "Iteration  617 => Weight: 1.490000 => Bias: 6.170000 => Loss: 35.923713\n",
      "Iteration  618 => Weight: 1.490000 => Bias: 6.180000 => Loss: 35.887347\n",
      "Iteration  619 => Weight: 1.490000 => Bias: 6.190000 => Loss: 35.851180\n",
      "Iteration  620 => Weight: 1.490000 => Bias: 6.200000 => Loss: 35.815213\n",
      "Iteration  621 => Weight: 1.480000 => Bias: 6.210000 => Loss: 35.775887\n",
      "Iteration  622 => Weight: 1.480000 => Bias: 6.220000 => Loss: 35.737787\n",
      "Iteration  623 => Weight: 1.480000 => Bias: 6.230000 => Loss: 35.699887\n",
      "Iteration  624 => Weight: 1.480000 => Bias: 6.240000 => Loss: 35.662187\n",
      "Iteration  625 => Weight: 1.480000 => Bias: 6.250000 => Loss: 35.624687\n",
      "Iteration  626 => Weight: 1.480000 => Bias: 6.260000 => Loss: 35.587387\n",
      "Iteration  627 => Weight: 1.480000 => Bias: 6.270000 => Loss: 35.550287\n",
      "Iteration  628 => Weight: 1.480000 => Bias: 6.280000 => Loss: 35.513387\n",
      "Iteration  629 => Weight: 1.480000 => Bias: 6.290000 => Loss: 35.476687\n",
      "Iteration  630 => Weight: 1.480000 => Bias: 6.300000 => Loss: 35.440187\n",
      "Iteration  631 => Weight: 1.480000 => Bias: 6.310000 => Loss: 35.403887\n",
      "Iteration  632 => Weight: 1.480000 => Bias: 6.320000 => Loss: 35.367787\n",
      "Iteration  633 => Weight: 1.480000 => Bias: 6.330000 => Loss: 35.331887\n",
      "Iteration  634 => Weight: 1.480000 => Bias: 6.340000 => Loss: 35.296187\n",
      "Iteration  635 => Weight: 1.480000 => Bias: 6.350000 => Loss: 35.260687\n",
      "Iteration  636 => Weight: 1.480000 => Bias: 6.360000 => Loss: 35.225387\n",
      "Iteration  637 => Weight: 1.480000 => Bias: 6.370000 => Loss: 35.190287\n",
      "Iteration  638 => Weight: 1.470000 => Bias: 6.380000 => Loss: 35.152520\n",
      "Iteration  639 => Weight: 1.470000 => Bias: 6.390000 => Loss: 35.115287\n",
      "Iteration  640 => Weight: 1.470000 => Bias: 6.400000 => Loss: 35.078253\n",
      "Iteration  641 => Weight: 1.470000 => Bias: 6.410000 => Loss: 35.041420\n",
      "Iteration  642 => Weight: 1.470000 => Bias: 6.420000 => Loss: 35.004787\n",
      "Iteration  643 => Weight: 1.470000 => Bias: 6.430000 => Loss: 34.968353\n",
      "Iteration  644 => Weight: 1.470000 => Bias: 6.440000 => Loss: 34.932120\n",
      "Iteration  645 => Weight: 1.470000 => Bias: 6.450000 => Loss: 34.896087\n",
      "Iteration  646 => Weight: 1.470000 => Bias: 6.460000 => Loss: 34.860253\n",
      "Iteration  647 => Weight: 1.470000 => Bias: 6.470000 => Loss: 34.824620\n",
      "Iteration  648 => Weight: 1.470000 => Bias: 6.480000 => Loss: 34.789187\n",
      "Iteration  649 => Weight: 1.470000 => Bias: 6.490000 => Loss: 34.753953\n",
      "Iteration  650 => Weight: 1.470000 => Bias: 6.500000 => Loss: 34.718920\n",
      "Iteration  651 => Weight: 1.470000 => Bias: 6.510000 => Loss: 34.684087\n",
      "Iteration  652 => Weight: 1.470000 => Bias: 6.520000 => Loss: 34.649453\n",
      "Iteration  653 => Weight: 1.470000 => Bias: 6.530000 => Loss: 34.615020\n",
      "Iteration  654 => Weight: 1.470000 => Bias: 6.540000 => Loss: 34.580787\n",
      "Iteration  655 => Weight: 1.470000 => Bias: 6.550000 => Loss: 34.546753\n",
      "Iteration  656 => Weight: 1.460000 => Bias: 6.560000 => Loss: 34.508213\n",
      "Iteration  657 => Weight: 1.460000 => Bias: 6.570000 => Loss: 34.472047\n",
      "Iteration  658 => Weight: 1.460000 => Bias: 6.580000 => Loss: 34.436080\n",
      "Iteration  659 => Weight: 1.460000 => Bias: 6.590000 => Loss: 34.400313\n",
      "Iteration  660 => Weight: 1.460000 => Bias: 6.600000 => Loss: 34.364747\n",
      "Iteration  661 => Weight: 1.460000 => Bias: 6.610000 => Loss: 34.329380\n",
      "Iteration  662 => Weight: 1.460000 => Bias: 6.620000 => Loss: 34.294213\n",
      "Iteration  663 => Weight: 1.460000 => Bias: 6.630000 => Loss: 34.259247\n",
      "Iteration  664 => Weight: 1.460000 => Bias: 6.640000 => Loss: 34.224480\n",
      "Iteration  665 => Weight: 1.460000 => Bias: 6.650000 => Loss: 34.189913\n",
      "Iteration  666 => Weight: 1.460000 => Bias: 6.660000 => Loss: 34.155547\n",
      "Iteration  667 => Weight: 1.460000 => Bias: 6.670000 => Loss: 34.121380\n",
      "Iteration  668 => Weight: 1.460000 => Bias: 6.680000 => Loss: 34.087413\n",
      "Iteration  669 => Weight: 1.460000 => Bias: 6.690000 => Loss: 34.053647\n",
      "Iteration  670 => Weight: 1.460000 => Bias: 6.700000 => Loss: 34.020080\n",
      "Iteration  671 => Weight: 1.460000 => Bias: 6.710000 => Loss: 33.986713\n",
      "Iteration  672 => Weight: 1.460000 => Bias: 6.720000 => Loss: 33.953547\n",
      "Iteration  673 => Weight: 1.450000 => Bias: 6.730000 => Loss: 33.916567\n",
      "Iteration  674 => Weight: 1.450000 => Bias: 6.740000 => Loss: 33.881267\n",
      "Iteration  675 => Weight: 1.450000 => Bias: 6.750000 => Loss: 33.846167\n",
      "Iteration  676 => Weight: 1.450000 => Bias: 6.760000 => Loss: 33.811267\n",
      "Iteration  677 => Weight: 1.450000 => Bias: 6.770000 => Loss: 33.776567\n",
      "Iteration  678 => Weight: 1.450000 => Bias: 6.780000 => Loss: 33.742067\n",
      "Iteration  679 => Weight: 1.450000 => Bias: 6.790000 => Loss: 33.707767\n",
      "Iteration  680 => Weight: 1.450000 => Bias: 6.800000 => Loss: 33.673667\n",
      "Iteration  681 => Weight: 1.450000 => Bias: 6.810000 => Loss: 33.639767\n",
      "Iteration  682 => Weight: 1.450000 => Bias: 6.820000 => Loss: 33.606067\n",
      "Iteration  683 => Weight: 1.450000 => Bias: 6.830000 => Loss: 33.572567\n",
      "Iteration  684 => Weight: 1.450000 => Bias: 6.840000 => Loss: 33.539267\n",
      "Iteration  685 => Weight: 1.450000 => Bias: 6.850000 => Loss: 33.506167\n",
      "Iteration  686 => Weight: 1.450000 => Bias: 6.860000 => Loss: 33.473267\n",
      "Iteration  687 => Weight: 1.450000 => Bias: 6.870000 => Loss: 33.440567\n",
      "Iteration  688 => Weight: 1.450000 => Bias: 6.880000 => Loss: 33.408067\n",
      "Iteration  689 => Weight: 1.450000 => Bias: 6.890000 => Loss: 33.375767\n",
      "Iteration  690 => Weight: 1.440000 => Bias: 6.900000 => Loss: 33.340347\n",
      "Iteration  691 => Weight: 1.440000 => Bias: 6.910000 => Loss: 33.305913\n",
      "Iteration  692 => Weight: 1.440000 => Bias: 6.920000 => Loss: 33.271680\n",
      "Iteration  693 => Weight: 1.440000 => Bias: 6.930000 => Loss: 33.237647\n",
      "Iteration  694 => Weight: 1.440000 => Bias: 6.940000 => Loss: 33.203813\n",
      "Iteration  695 => Weight: 1.440000 => Bias: 6.950000 => Loss: 33.170180\n",
      "Iteration  696 => Weight: 1.440000 => Bias: 6.960000 => Loss: 33.136747\n",
      "Iteration  697 => Weight: 1.440000 => Bias: 6.970000 => Loss: 33.103513\n",
      "Iteration  698 => Weight: 1.440000 => Bias: 6.980000 => Loss: 33.070480\n",
      "Iteration  699 => Weight: 1.440000 => Bias: 6.990000 => Loss: 33.037647\n",
      "Iteration  700 => Weight: 1.440000 => Bias: 7.000000 => Loss: 33.005013\n",
      "Iteration  701 => Weight: 1.440000 => Bias: 7.010000 => Loss: 32.972580\n",
      "Iteration  702 => Weight: 1.440000 => Bias: 7.020000 => Loss: 32.940347\n",
      "Iteration  703 => Weight: 1.440000 => Bias: 7.030000 => Loss: 32.908313\n",
      "Iteration  704 => Weight: 1.440000 => Bias: 7.040000 => Loss: 32.876480\n",
      "Iteration  705 => Weight: 1.440000 => Bias: 7.050000 => Loss: 32.844847\n",
      "Iteration  706 => Weight: 1.440000 => Bias: 7.060000 => Loss: 32.813413\n",
      "Iteration  707 => Weight: 1.430000 => Bias: 7.070000 => Loss: 32.779553\n",
      "Iteration  708 => Weight: 1.430000 => Bias: 7.080000 => Loss: 32.745987\n",
      "Iteration  709 => Weight: 1.430000 => Bias: 7.090000 => Loss: 32.712620\n",
      "Iteration  710 => Weight: 1.430000 => Bias: 7.100000 => Loss: 32.679453\n",
      "Iteration  711 => Weight: 1.430000 => Bias: 7.110000 => Loss: 32.646487\n",
      "Iteration  712 => Weight: 1.430000 => Bias: 7.120000 => Loss: 32.613720\n",
      "Iteration  713 => Weight: 1.430000 => Bias: 7.130000 => Loss: 32.581153\n",
      "Iteration  714 => Weight: 1.430000 => Bias: 7.140000 => Loss: 32.548787\n",
      "Iteration  715 => Weight: 1.430000 => Bias: 7.150000 => Loss: 32.516620\n",
      "Iteration  716 => Weight: 1.430000 => Bias: 7.160000 => Loss: 32.484653\n",
      "Iteration  717 => Weight: 1.430000 => Bias: 7.170000 => Loss: 32.452887\n",
      "Iteration  718 => Weight: 1.430000 => Bias: 7.180000 => Loss: 32.421320\n",
      "Iteration  719 => Weight: 1.430000 => Bias: 7.190000 => Loss: 32.389953\n",
      "Iteration  720 => Weight: 1.430000 => Bias: 7.200000 => Loss: 32.358787\n",
      "Iteration  721 => Weight: 1.430000 => Bias: 7.210000 => Loss: 32.327820\n",
      "Iteration  722 => Weight: 1.430000 => Bias: 7.220000 => Loss: 32.297053\n",
      "Iteration  723 => Weight: 1.430000 => Bias: 7.230000 => Loss: 32.266487\n",
      "Iteration  724 => Weight: 1.430000 => Bias: 7.240000 => Loss: 32.236120\n",
      "Iteration  725 => Weight: 1.420000 => Bias: 7.250000 => Loss: 32.201487\n",
      "Iteration  726 => Weight: 1.420000 => Bias: 7.260000 => Loss: 32.168987\n",
      "Iteration  727 => Weight: 1.420000 => Bias: 7.270000 => Loss: 32.136687\n",
      "Iteration  728 => Weight: 1.420000 => Bias: 7.280000 => Loss: 32.104587\n",
      "Iteration  729 => Weight: 1.420000 => Bias: 7.290000 => Loss: 32.072687\n",
      "Iteration  730 => Weight: 1.420000 => Bias: 7.300000 => Loss: 32.040987\n",
      "Iteration  731 => Weight: 1.420000 => Bias: 7.310000 => Loss: 32.009487\n",
      "Iteration  732 => Weight: 1.420000 => Bias: 7.320000 => Loss: 31.978187\n",
      "Iteration  733 => Weight: 1.420000 => Bias: 7.330000 => Loss: 31.947087\n",
      "Iteration  734 => Weight: 1.420000 => Bias: 7.340000 => Loss: 31.916187\n",
      "Iteration  735 => Weight: 1.420000 => Bias: 7.350000 => Loss: 31.885487\n",
      "Iteration  736 => Weight: 1.420000 => Bias: 7.360000 => Loss: 31.854987\n",
      "Iteration  737 => Weight: 1.420000 => Bias: 7.370000 => Loss: 31.824687\n",
      "Iteration  738 => Weight: 1.420000 => Bias: 7.380000 => Loss: 31.794587\n",
      "Iteration  739 => Weight: 1.420000 => Bias: 7.390000 => Loss: 31.764687\n",
      "Iteration  740 => Weight: 1.420000 => Bias: 7.400000 => Loss: 31.734987\n",
      "Iteration  741 => Weight: 1.420000 => Bias: 7.410000 => Loss: 31.705487\n",
      "Iteration  742 => Weight: 1.410000 => Bias: 7.420000 => Loss: 31.672413\n",
      "Iteration  743 => Weight: 1.410000 => Bias: 7.430000 => Loss: 31.640780\n",
      "Iteration  744 => Weight: 1.410000 => Bias: 7.440000 => Loss: 31.609347\n",
      "Iteration  745 => Weight: 1.410000 => Bias: 7.450000 => Loss: 31.578113\n",
      "Iteration  746 => Weight: 1.410000 => Bias: 7.460000 => Loss: 31.547080\n",
      "Iteration  747 => Weight: 1.410000 => Bias: 7.470000 => Loss: 31.516247\n",
      "Iteration  748 => Weight: 1.410000 => Bias: 7.480000 => Loss: 31.485613\n",
      "Iteration  749 => Weight: 1.410000 => Bias: 7.490000 => Loss: 31.455180\n",
      "Iteration  750 => Weight: 1.410000 => Bias: 7.500000 => Loss: 31.424947\n",
      "Iteration  751 => Weight: 1.410000 => Bias: 7.510000 => Loss: 31.394913\n",
      "Iteration  752 => Weight: 1.410000 => Bias: 7.520000 => Loss: 31.365080\n",
      "Iteration  753 => Weight: 1.410000 => Bias: 7.530000 => Loss: 31.335447\n",
      "Iteration  754 => Weight: 1.410000 => Bias: 7.540000 => Loss: 31.306013\n",
      "Iteration  755 => Weight: 1.410000 => Bias: 7.550000 => Loss: 31.276780\n",
      "Iteration  756 => Weight: 1.410000 => Bias: 7.560000 => Loss: 31.247747\n",
      "Iteration  757 => Weight: 1.410000 => Bias: 7.570000 => Loss: 31.218913\n",
      "Iteration  758 => Weight: 1.410000 => Bias: 7.580000 => Loss: 31.190280\n",
      "Iteration  759 => Weight: 1.400000 => Bias: 7.590000 => Loss: 31.158767\n",
      "Iteration  760 => Weight: 1.400000 => Bias: 7.600000 => Loss: 31.128000\n",
      "Iteration  761 => Weight: 1.400000 => Bias: 7.610000 => Loss: 31.097433\n",
      "Iteration  762 => Weight: 1.400000 => Bias: 7.620000 => Loss: 31.067067\n",
      "Iteration  763 => Weight: 1.400000 => Bias: 7.630000 => Loss: 31.036900\n",
      "Iteration  764 => Weight: 1.400000 => Bias: 7.640000 => Loss: 31.006933\n",
      "Iteration  765 => Weight: 1.400000 => Bias: 7.650000 => Loss: 30.977167\n",
      "Iteration  766 => Weight: 1.400000 => Bias: 7.660000 => Loss: 30.947600\n",
      "Iteration  767 => Weight: 1.400000 => Bias: 7.670000 => Loss: 30.918233\n",
      "Iteration  768 => Weight: 1.400000 => Bias: 7.680000 => Loss: 30.889067\n",
      "Iteration  769 => Weight: 1.400000 => Bias: 7.690000 => Loss: 30.860100\n",
      "Iteration  770 => Weight: 1.400000 => Bias: 7.700000 => Loss: 30.831333\n",
      "Iteration  771 => Weight: 1.400000 => Bias: 7.710000 => Loss: 30.802767\n",
      "Iteration  772 => Weight: 1.400000 => Bias: 7.720000 => Loss: 30.774400\n",
      "Iteration  773 => Weight: 1.400000 => Bias: 7.730000 => Loss: 30.746233\n",
      "Iteration  774 => Weight: 1.400000 => Bias: 7.740000 => Loss: 30.718267\n",
      "Iteration  775 => Weight: 1.400000 => Bias: 7.750000 => Loss: 30.690500\n",
      "Iteration  776 => Weight: 1.400000 => Bias: 7.760000 => Loss: 30.662933\n",
      "Iteration  777 => Weight: 1.390000 => Bias: 7.770000 => Loss: 30.630647\n",
      "Iteration  778 => Weight: 1.390000 => Bias: 7.780000 => Loss: 30.600947\n",
      "Iteration  779 => Weight: 1.390000 => Bias: 7.790000 => Loss: 30.571447\n",
      "Iteration  780 => Weight: 1.390000 => Bias: 7.800000 => Loss: 30.542147\n",
      "Iteration  781 => Weight: 1.390000 => Bias: 7.810000 => Loss: 30.513047\n",
      "Iteration  782 => Weight: 1.390000 => Bias: 7.820000 => Loss: 30.484147\n",
      "Iteration  783 => Weight: 1.390000 => Bias: 7.830000 => Loss: 30.455447\n",
      "Iteration  784 => Weight: 1.390000 => Bias: 7.840000 => Loss: 30.426947\n",
      "Iteration  785 => Weight: 1.390000 => Bias: 7.850000 => Loss: 30.398647\n",
      "Iteration  786 => Weight: 1.390000 => Bias: 7.860000 => Loss: 30.370547\n",
      "Iteration  787 => Weight: 1.390000 => Bias: 7.870000 => Loss: 30.342647\n",
      "Iteration  788 => Weight: 1.390000 => Bias: 7.880000 => Loss: 30.314947\n",
      "Iteration  789 => Weight: 1.390000 => Bias: 7.890000 => Loss: 30.287447\n",
      "Iteration  790 => Weight: 1.390000 => Bias: 7.900000 => Loss: 30.260147\n",
      "Iteration  791 => Weight: 1.390000 => Bias: 7.910000 => Loss: 30.233047\n",
      "Iteration  792 => Weight: 1.390000 => Bias: 7.920000 => Loss: 30.206147\n",
      "Iteration  793 => Weight: 1.390000 => Bias: 7.930000 => Loss: 30.179447\n",
      "Iteration  794 => Weight: 1.380000 => Bias: 7.940000 => Loss: 30.148720\n",
      "Iteration  795 => Weight: 1.380000 => Bias: 7.950000 => Loss: 30.119887\n",
      "Iteration  796 => Weight: 1.380000 => Bias: 7.960000 => Loss: 30.091253\n",
      "Iteration  797 => Weight: 1.380000 => Bias: 7.970000 => Loss: 30.062820\n",
      "Iteration  798 => Weight: 1.380000 => Bias: 7.980000 => Loss: 30.034587\n",
      "Iteration  799 => Weight: 1.380000 => Bias: 7.990000 => Loss: 30.006553\n",
      "Iteration  800 => Weight: 1.380000 => Bias: 8.000000 => Loss: 29.978720\n",
      "Iteration  801 => Weight: 1.380000 => Bias: 8.010000 => Loss: 29.951087\n",
      "Iteration  802 => Weight: 1.380000 => Bias: 8.020000 => Loss: 29.923653\n",
      "Iteration  803 => Weight: 1.380000 => Bias: 8.030000 => Loss: 29.896420\n",
      "Iteration  804 => Weight: 1.380000 => Bias: 8.040000 => Loss: 29.869387\n",
      "Iteration  805 => Weight: 1.380000 => Bias: 8.050000 => Loss: 29.842553\n",
      "Iteration  806 => Weight: 1.380000 => Bias: 8.060000 => Loss: 29.815920\n",
      "Iteration  807 => Weight: 1.380000 => Bias: 8.070000 => Loss: 29.789487\n",
      "Iteration  808 => Weight: 1.380000 => Bias: 8.080000 => Loss: 29.763253\n",
      "Iteration  809 => Weight: 1.380000 => Bias: 8.090000 => Loss: 29.737220\n",
      "Iteration  810 => Weight: 1.380000 => Bias: 8.100000 => Loss: 29.711387\n",
      "Iteration  811 => Weight: 1.370000 => Bias: 8.110000 => Loss: 29.682220\n",
      "Iteration  812 => Weight: 1.370000 => Bias: 8.120000 => Loss: 29.654253\n",
      "Iteration  813 => Weight: 1.370000 => Bias: 8.130000 => Loss: 29.626487\n",
      "Iteration  814 => Weight: 1.370000 => Bias: 8.140000 => Loss: 29.598920\n",
      "Iteration  815 => Weight: 1.370000 => Bias: 8.150000 => Loss: 29.571553\n",
      "Iteration  816 => Weight: 1.370000 => Bias: 8.160000 => Loss: 29.544387\n",
      "Iteration  817 => Weight: 1.370000 => Bias: 8.170000 => Loss: 29.517420\n",
      "Iteration  818 => Weight: 1.370000 => Bias: 8.180000 => Loss: 29.490653\n",
      "Iteration  819 => Weight: 1.370000 => Bias: 8.190000 => Loss: 29.464087\n",
      "Iteration  820 => Weight: 1.370000 => Bias: 8.200000 => Loss: 29.437720\n",
      "Iteration  821 => Weight: 1.370000 => Bias: 8.210000 => Loss: 29.411553\n",
      "Iteration  822 => Weight: 1.370000 => Bias: 8.220000 => Loss: 29.385587\n",
      "Iteration  823 => Weight: 1.370000 => Bias: 8.230000 => Loss: 29.359820\n",
      "Iteration  824 => Weight: 1.370000 => Bias: 8.240000 => Loss: 29.334253\n",
      "Iteration  825 => Weight: 1.370000 => Bias: 8.250000 => Loss: 29.308887\n",
      "Iteration  826 => Weight: 1.370000 => Bias: 8.260000 => Loss: 29.283720\n",
      "Iteration  827 => Weight: 1.370000 => Bias: 8.270000 => Loss: 29.258753\n",
      "Iteration  828 => Weight: 1.360000 => Bias: 8.280000 => Loss: 29.231147\n",
      "Iteration  829 => Weight: 1.360000 => Bias: 8.290000 => Loss: 29.204047\n",
      "Iteration  830 => Weight: 1.360000 => Bias: 8.300000 => Loss: 29.177147\n",
      "Iteration  831 => Weight: 1.360000 => Bias: 8.310000 => Loss: 29.150447\n",
      "Iteration  832 => Weight: 1.360000 => Bias: 8.320000 => Loss: 29.123947\n",
      "Iteration  833 => Weight: 1.360000 => Bias: 8.330000 => Loss: 29.097647\n",
      "Iteration  834 => Weight: 1.360000 => Bias: 8.340000 => Loss: 29.071547\n",
      "Iteration  835 => Weight: 1.360000 => Bias: 8.350000 => Loss: 29.045647\n",
      "Iteration  836 => Weight: 1.360000 => Bias: 8.360000 => Loss: 29.019947\n",
      "Iteration  837 => Weight: 1.360000 => Bias: 8.370000 => Loss: 28.994447\n",
      "Iteration  838 => Weight: 1.360000 => Bias: 8.380000 => Loss: 28.969147\n",
      "Iteration  839 => Weight: 1.360000 => Bias: 8.390000 => Loss: 28.944047\n",
      "Iteration  840 => Weight: 1.360000 => Bias: 8.400000 => Loss: 28.919147\n",
      "Iteration  841 => Weight: 1.360000 => Bias: 8.410000 => Loss: 28.894447\n",
      "Iteration  842 => Weight: 1.360000 => Bias: 8.420000 => Loss: 28.869947\n",
      "Iteration  843 => Weight: 1.360000 => Bias: 8.430000 => Loss: 28.845647\n",
      "Iteration  844 => Weight: 1.360000 => Bias: 8.440000 => Loss: 28.821547\n",
      "Iteration  845 => Weight: 1.360000 => Bias: 8.450000 => Loss: 28.797647\n",
      "Iteration  846 => Weight: 1.350000 => Bias: 8.460000 => Loss: 28.769267\n",
      "Iteration  847 => Weight: 1.350000 => Bias: 8.470000 => Loss: 28.743233\n",
      "Iteration  848 => Weight: 1.350000 => Bias: 8.480000 => Loss: 28.717400\n",
      "Iteration  849 => Weight: 1.350000 => Bias: 8.490000 => Loss: 28.691767\n",
      "Iteration  850 => Weight: 1.350000 => Bias: 8.500000 => Loss: 28.666333\n",
      "Iteration  851 => Weight: 1.350000 => Bias: 8.510000 => Loss: 28.641100\n",
      "Iteration  852 => Weight: 1.350000 => Bias: 8.520000 => Loss: 28.616067\n",
      "Iteration  853 => Weight: 1.350000 => Bias: 8.530000 => Loss: 28.591233\n",
      "Iteration  854 => Weight: 1.350000 => Bias: 8.540000 => Loss: 28.566600\n",
      "Iteration  855 => Weight: 1.350000 => Bias: 8.550000 => Loss: 28.542167\n",
      "Iteration  856 => Weight: 1.350000 => Bias: 8.560000 => Loss: 28.517933\n",
      "Iteration  857 => Weight: 1.350000 => Bias: 8.570000 => Loss: 28.493900\n",
      "Iteration  858 => Weight: 1.350000 => Bias: 8.580000 => Loss: 28.470067\n",
      "Iteration  859 => Weight: 1.350000 => Bias: 8.590000 => Loss: 28.446433\n",
      "Iteration  860 => Weight: 1.350000 => Bias: 8.600000 => Loss: 28.423000\n",
      "Iteration  861 => Weight: 1.350000 => Bias: 8.610000 => Loss: 28.399767\n",
      "Iteration  862 => Weight: 1.350000 => Bias: 8.620000 => Loss: 28.376733\n",
      "Iteration  863 => Weight: 1.340000 => Bias: 8.630000 => Loss: 28.349913\n",
      "Iteration  864 => Weight: 1.340000 => Bias: 8.640000 => Loss: 28.324747\n",
      "Iteration  865 => Weight: 1.340000 => Bias: 8.650000 => Loss: 28.299780\n",
      "Iteration  866 => Weight: 1.340000 => Bias: 8.660000 => Loss: 28.275013\n",
      "Iteration  867 => Weight: 1.340000 => Bias: 8.670000 => Loss: 28.250447\n",
      "Iteration  868 => Weight: 1.340000 => Bias: 8.680000 => Loss: 28.226080\n",
      "Iteration  869 => Weight: 1.340000 => Bias: 8.690000 => Loss: 28.201913\n",
      "Iteration  870 => Weight: 1.340000 => Bias: 8.700000 => Loss: 28.177947\n",
      "Iteration  871 => Weight: 1.340000 => Bias: 8.710000 => Loss: 28.154180\n",
      "Iteration  872 => Weight: 1.340000 => Bias: 8.720000 => Loss: 28.130613\n",
      "Iteration  873 => Weight: 1.340000 => Bias: 8.730000 => Loss: 28.107247\n",
      "Iteration  874 => Weight: 1.340000 => Bias: 8.740000 => Loss: 28.084080\n",
      "Iteration  875 => Weight: 1.340000 => Bias: 8.750000 => Loss: 28.061113\n",
      "Iteration  876 => Weight: 1.340000 => Bias: 8.760000 => Loss: 28.038347\n",
      "Iteration  877 => Weight: 1.340000 => Bias: 8.770000 => Loss: 28.015780\n",
      "Iteration  878 => Weight: 1.340000 => Bias: 8.780000 => Loss: 27.993413\n",
      "Iteration  879 => Weight: 1.340000 => Bias: 8.790000 => Loss: 27.971247\n",
      "Iteration  880 => Weight: 1.330000 => Bias: 8.800000 => Loss: 27.945987\n",
      "Iteration  881 => Weight: 1.330000 => Bias: 8.810000 => Loss: 27.921687\n",
      "Iteration  882 => Weight: 1.330000 => Bias: 8.820000 => Loss: 27.897587\n",
      "Iteration  883 => Weight: 1.330000 => Bias: 8.830000 => Loss: 27.873687\n",
      "Iteration  884 => Weight: 1.330000 => Bias: 8.840000 => Loss: 27.849987\n",
      "Iteration  885 => Weight: 1.330000 => Bias: 8.850000 => Loss: 27.826487\n",
      "Iteration  886 => Weight: 1.330000 => Bias: 8.860000 => Loss: 27.803187\n",
      "Iteration  887 => Weight: 1.330000 => Bias: 8.870000 => Loss: 27.780087\n",
      "Iteration  888 => Weight: 1.330000 => Bias: 8.880000 => Loss: 27.757187\n",
      "Iteration  889 => Weight: 1.330000 => Bias: 8.890000 => Loss: 27.734487\n",
      "Iteration  890 => Weight: 1.330000 => Bias: 8.900000 => Loss: 27.711987\n",
      "Iteration  891 => Weight: 1.330000 => Bias: 8.910000 => Loss: 27.689687\n",
      "Iteration  892 => Weight: 1.330000 => Bias: 8.920000 => Loss: 27.667587\n",
      "Iteration  893 => Weight: 1.330000 => Bias: 8.930000 => Loss: 27.645687\n",
      "Iteration  894 => Weight: 1.330000 => Bias: 8.940000 => Loss: 27.623987\n",
      "Iteration  895 => Weight: 1.330000 => Bias: 8.950000 => Loss: 27.602487\n",
      "Iteration  896 => Weight: 1.330000 => Bias: 8.960000 => Loss: 27.581187\n",
      "Iteration  897 => Weight: 1.320000 => Bias: 8.970000 => Loss: 27.557487\n",
      "Iteration  898 => Weight: 1.320000 => Bias: 8.980000 => Loss: 27.534053\n",
      "Iteration  899 => Weight: 1.320000 => Bias: 8.990000 => Loss: 27.510820\n",
      "Iteration  900 => Weight: 1.320000 => Bias: 9.000000 => Loss: 27.487787\n",
      "Iteration  901 => Weight: 1.320000 => Bias: 9.010000 => Loss: 27.464953\n",
      "Iteration  902 => Weight: 1.320000 => Bias: 9.020000 => Loss: 27.442320\n",
      "Iteration  903 => Weight: 1.320000 => Bias: 9.030000 => Loss: 27.419887\n",
      "Iteration  904 => Weight: 1.320000 => Bias: 9.040000 => Loss: 27.397653\n",
      "Iteration  905 => Weight: 1.320000 => Bias: 9.050000 => Loss: 27.375620\n",
      "Iteration  906 => Weight: 1.320000 => Bias: 9.060000 => Loss: 27.353787\n",
      "Iteration  907 => Weight: 1.320000 => Bias: 9.070000 => Loss: 27.332153\n",
      "Iteration  908 => Weight: 1.320000 => Bias: 9.080000 => Loss: 27.310720\n",
      "Iteration  909 => Weight: 1.320000 => Bias: 9.090000 => Loss: 27.289487\n",
      "Iteration  910 => Weight: 1.320000 => Bias: 9.100000 => Loss: 27.268453\n",
      "Iteration  911 => Weight: 1.320000 => Bias: 9.110000 => Loss: 27.247620\n",
      "Iteration  912 => Weight: 1.320000 => Bias: 9.120000 => Loss: 27.226987\n",
      "Iteration  913 => Weight: 1.320000 => Bias: 9.130000 => Loss: 27.206553\n",
      "Iteration  914 => Weight: 1.320000 => Bias: 9.140000 => Loss: 27.186320\n",
      "Iteration  915 => Weight: 1.310000 => Bias: 9.150000 => Loss: 27.161847\n",
      "Iteration  916 => Weight: 1.310000 => Bias: 9.160000 => Loss: 27.139480\n",
      "Iteration  917 => Weight: 1.310000 => Bias: 9.170000 => Loss: 27.117313\n",
      "Iteration  918 => Weight: 1.310000 => Bias: 9.180000 => Loss: 27.095347\n",
      "Iteration  919 => Weight: 1.310000 => Bias: 9.190000 => Loss: 27.073580\n",
      "Iteration  920 => Weight: 1.310000 => Bias: 9.200000 => Loss: 27.052013\n",
      "Iteration  921 => Weight: 1.310000 => Bias: 9.210000 => Loss: 27.030647\n",
      "Iteration  922 => Weight: 1.310000 => Bias: 9.220000 => Loss: 27.009480\n",
      "Iteration  923 => Weight: 1.310000 => Bias: 9.230000 => Loss: 26.988513\n",
      "Iteration  924 => Weight: 1.310000 => Bias: 9.240000 => Loss: 26.967747\n",
      "Iteration  925 => Weight: 1.310000 => Bias: 9.250000 => Loss: 26.947180\n",
      "Iteration  926 => Weight: 1.310000 => Bias: 9.260000 => Loss: 26.926813\n",
      "Iteration  927 => Weight: 1.310000 => Bias: 9.270000 => Loss: 26.906647\n",
      "Iteration  928 => Weight: 1.310000 => Bias: 9.280000 => Loss: 26.886680\n",
      "Iteration  929 => Weight: 1.310000 => Bias: 9.290000 => Loss: 26.866913\n",
      "Iteration  930 => Weight: 1.310000 => Bias: 9.300000 => Loss: 26.847347\n",
      "Iteration  931 => Weight: 1.310000 => Bias: 9.310000 => Loss: 26.827980\n",
      "Iteration  932 => Weight: 1.300000 => Bias: 9.320000 => Loss: 26.805067\n",
      "Iteration  933 => Weight: 1.300000 => Bias: 9.330000 => Loss: 26.783567\n",
      "Iteration  934 => Weight: 1.300000 => Bias: 9.340000 => Loss: 26.762267\n",
      "Iteration  935 => Weight: 1.300000 => Bias: 9.350000 => Loss: 26.741167\n",
      "Iteration  936 => Weight: 1.300000 => Bias: 9.360000 => Loss: 26.720267\n",
      "Iteration  937 => Weight: 1.300000 => Bias: 9.370000 => Loss: 26.699567\n",
      "Iteration  938 => Weight: 1.300000 => Bias: 9.380000 => Loss: 26.679067\n",
      "Iteration  939 => Weight: 1.300000 => Bias: 9.390000 => Loss: 26.658767\n",
      "Iteration  940 => Weight: 1.300000 => Bias: 9.400000 => Loss: 26.638667\n",
      "Iteration  941 => Weight: 1.300000 => Bias: 9.410000 => Loss: 26.618767\n",
      "Iteration  942 => Weight: 1.300000 => Bias: 9.420000 => Loss: 26.599067\n",
      "Iteration  943 => Weight: 1.300000 => Bias: 9.430000 => Loss: 26.579567\n",
      "Iteration  944 => Weight: 1.300000 => Bias: 9.440000 => Loss: 26.560267\n",
      "Iteration  945 => Weight: 1.300000 => Bias: 9.450000 => Loss: 26.541167\n",
      "Iteration  946 => Weight: 1.300000 => Bias: 9.460000 => Loss: 26.522267\n",
      "Iteration  947 => Weight: 1.300000 => Bias: 9.470000 => Loss: 26.503567\n",
      "Iteration  948 => Weight: 1.300000 => Bias: 9.480000 => Loss: 26.485067\n",
      "Iteration  949 => Weight: 1.290000 => Bias: 9.490000 => Loss: 26.463713\n",
      "Iteration  950 => Weight: 1.290000 => Bias: 9.500000 => Loss: 26.443080\n",
      "Iteration  951 => Weight: 1.290000 => Bias: 9.510000 => Loss: 26.422647\n",
      "Iteration  952 => Weight: 1.290000 => Bias: 9.520000 => Loss: 26.402413\n",
      "Iteration  953 => Weight: 1.290000 => Bias: 9.530000 => Loss: 26.382380\n",
      "Iteration  954 => Weight: 1.290000 => Bias: 9.540000 => Loss: 26.362547\n",
      "Iteration  955 => Weight: 1.290000 => Bias: 9.550000 => Loss: 26.342913\n",
      "Iteration  956 => Weight: 1.290000 => Bias: 9.560000 => Loss: 26.323480\n",
      "Iteration  957 => Weight: 1.290000 => Bias: 9.570000 => Loss: 26.304247\n",
      "Iteration  958 => Weight: 1.290000 => Bias: 9.580000 => Loss: 26.285213\n",
      "Iteration  959 => Weight: 1.290000 => Bias: 9.590000 => Loss: 26.266380\n",
      "Iteration  960 => Weight: 1.290000 => Bias: 9.600000 => Loss: 26.247747\n",
      "Iteration  961 => Weight: 1.290000 => Bias: 9.610000 => Loss: 26.229313\n",
      "Iteration  962 => Weight: 1.290000 => Bias: 9.620000 => Loss: 26.211080\n",
      "Iteration  963 => Weight: 1.290000 => Bias: 9.630000 => Loss: 26.193047\n",
      "Iteration  964 => Weight: 1.290000 => Bias: 9.640000 => Loss: 26.175213\n",
      "Iteration  965 => Weight: 1.290000 => Bias: 9.650000 => Loss: 26.157580\n",
      "Iteration  966 => Weight: 1.290000 => Bias: 9.660000 => Loss: 26.140147\n",
      "Iteration  967 => Weight: 1.280000 => Bias: 9.670000 => Loss: 26.118020\n",
      "Iteration  968 => Weight: 1.280000 => Bias: 9.680000 => Loss: 26.098453\n",
      "Iteration  969 => Weight: 1.280000 => Bias: 9.690000 => Loss: 26.079087\n",
      "Iteration  970 => Weight: 1.280000 => Bias: 9.700000 => Loss: 26.059920\n",
      "Iteration  971 => Weight: 1.280000 => Bias: 9.710000 => Loss: 26.040953\n",
      "Iteration  972 => Weight: 1.280000 => Bias: 9.720000 => Loss: 26.022187\n",
      "Iteration  973 => Weight: 1.280000 => Bias: 9.730000 => Loss: 26.003620\n",
      "Iteration  974 => Weight: 1.280000 => Bias: 9.740000 => Loss: 25.985253\n",
      "Iteration  975 => Weight: 1.280000 => Bias: 9.750000 => Loss: 25.967087\n",
      "Iteration  976 => Weight: 1.280000 => Bias: 9.760000 => Loss: 25.949120\n",
      "Iteration  977 => Weight: 1.280000 => Bias: 9.770000 => Loss: 25.931353\n",
      "Iteration  978 => Weight: 1.280000 => Bias: 9.780000 => Loss: 25.913787\n",
      "Iteration  979 => Weight: 1.280000 => Bias: 9.790000 => Loss: 25.896420\n",
      "Iteration  980 => Weight: 1.280000 => Bias: 9.800000 => Loss: 25.879253\n",
      "Iteration  981 => Weight: 1.280000 => Bias: 9.810000 => Loss: 25.862287\n",
      "Iteration  982 => Weight: 1.280000 => Bias: 9.820000 => Loss: 25.845520\n",
      "Iteration  983 => Weight: 1.280000 => Bias: 9.830000 => Loss: 25.828953\n",
      "Iteration  984 => Weight: 1.270000 => Bias: 9.840000 => Loss: 25.808387\n",
      "Iteration  985 => Weight: 1.270000 => Bias: 9.850000 => Loss: 25.789687\n",
      "Iteration  986 => Weight: 1.270000 => Bias: 9.860000 => Loss: 25.771187\n",
      "Iteration  987 => Weight: 1.270000 => Bias: 9.870000 => Loss: 25.752887\n",
      "Iteration  988 => Weight: 1.270000 => Bias: 9.880000 => Loss: 25.734787\n",
      "Iteration  989 => Weight: 1.270000 => Bias: 9.890000 => Loss: 25.716887\n",
      "Iteration  990 => Weight: 1.270000 => Bias: 9.900000 => Loss: 25.699187\n",
      "Iteration  991 => Weight: 1.270000 => Bias: 9.910000 => Loss: 25.681687\n",
      "Iteration  992 => Weight: 1.270000 => Bias: 9.920000 => Loss: 25.664387\n",
      "Iteration  993 => Weight: 1.270000 => Bias: 9.930000 => Loss: 25.647287\n",
      "Iteration  994 => Weight: 1.270000 => Bias: 9.940000 => Loss: 25.630387\n",
      "Iteration  995 => Weight: 1.270000 => Bias: 9.950000 => Loss: 25.613687\n",
      "Iteration  996 => Weight: 1.270000 => Bias: 9.960000 => Loss: 25.597187\n",
      "Iteration  997 => Weight: 1.270000 => Bias: 9.970000 => Loss: 25.580887\n",
      "Iteration  998 => Weight: 1.270000 => Bias: 9.980000 => Loss: 25.564787\n",
      "Iteration  999 => Weight: 1.270000 => Bias: 9.990000 => Loss: 25.548887\n",
      "Iteration 1000 => Weight: 1.270000 => Bias: 10.000000 => Loss: 25.533187\n",
      "Iteration 1001 => Weight: 1.260000 => Bias: 10.010000 => Loss: 25.514180\n",
      "Iteration 1002 => Weight: 1.260000 => Bias: 10.020000 => Loss: 25.496347\n",
      "Iteration 1003 => Weight: 1.260000 => Bias: 10.030000 => Loss: 25.478713\n",
      "Iteration 1004 => Weight: 1.260000 => Bias: 10.040000 => Loss: 25.461280\n",
      "Iteration 1005 => Weight: 1.260000 => Bias: 10.050000 => Loss: 25.444047\n",
      "Iteration 1006 => Weight: 1.260000 => Bias: 10.060000 => Loss: 25.427013\n",
      "Iteration 1007 => Weight: 1.260000 => Bias: 10.070000 => Loss: 25.410180\n",
      "Iteration 1008 => Weight: 1.260000 => Bias: 10.080000 => Loss: 25.393547\n",
      "Iteration 1009 => Weight: 1.260000 => Bias: 10.090000 => Loss: 25.377113\n",
      "Iteration 1010 => Weight: 1.260000 => Bias: 10.100000 => Loss: 25.360880\n",
      "Iteration 1011 => Weight: 1.260000 => Bias: 10.110000 => Loss: 25.344847\n",
      "Iteration 1012 => Weight: 1.260000 => Bias: 10.120000 => Loss: 25.329013\n",
      "Iteration 1013 => Weight: 1.260000 => Bias: 10.130000 => Loss: 25.313380\n",
      "Iteration 1014 => Weight: 1.260000 => Bias: 10.140000 => Loss: 25.297947\n",
      "Iteration 1015 => Weight: 1.260000 => Bias: 10.150000 => Loss: 25.282713\n",
      "Iteration 1016 => Weight: 1.260000 => Bias: 10.160000 => Loss: 25.267680\n",
      "Iteration 1017 => Weight: 1.260000 => Bias: 10.170000 => Loss: 25.252847\n",
      "Iteration 1018 => Weight: 1.250000 => Bias: 10.180000 => Loss: 25.235400\n",
      "Iteration 1019 => Weight: 1.250000 => Bias: 10.190000 => Loss: 25.218433\n",
      "Iteration 1020 => Weight: 1.250000 => Bias: 10.200000 => Loss: 25.201667\n",
      "Iteration 1021 => Weight: 1.250000 => Bias: 10.210000 => Loss: 25.185100\n",
      "Iteration 1022 => Weight: 1.250000 => Bias: 10.220000 => Loss: 25.168733\n",
      "Iteration 1023 => Weight: 1.250000 => Bias: 10.230000 => Loss: 25.152567\n",
      "Iteration 1024 => Weight: 1.250000 => Bias: 10.240000 => Loss: 25.136600\n",
      "Iteration 1025 => Weight: 1.250000 => Bias: 10.250000 => Loss: 25.120833\n",
      "Iteration 1026 => Weight: 1.250000 => Bias: 10.260000 => Loss: 25.105267\n",
      "Iteration 1027 => Weight: 1.250000 => Bias: 10.270000 => Loss: 25.089900\n",
      "Iteration 1028 => Weight: 1.250000 => Bias: 10.280000 => Loss: 25.074733\n",
      "Iteration 1029 => Weight: 1.250000 => Bias: 10.290000 => Loss: 25.059767\n",
      "Iteration 1030 => Weight: 1.250000 => Bias: 10.300000 => Loss: 25.045000\n",
      "Iteration 1031 => Weight: 1.250000 => Bias: 10.310000 => Loss: 25.030433\n",
      "Iteration 1032 => Weight: 1.250000 => Bias: 10.320000 => Loss: 25.016067\n",
      "Iteration 1033 => Weight: 1.250000 => Bias: 10.330000 => Loss: 25.001900\n",
      "Iteration 1034 => Weight: 1.250000 => Bias: 10.340000 => Loss: 24.987933\n",
      "Iteration 1035 => Weight: 1.250000 => Bias: 10.350000 => Loss: 24.974167\n",
      "Iteration 1036 => Weight: 1.240000 => Bias: 10.360000 => Loss: 24.955947\n",
      "Iteration 1037 => Weight: 1.240000 => Bias: 10.370000 => Loss: 24.940047\n",
      "Iteration 1038 => Weight: 1.240000 => Bias: 10.380000 => Loss: 24.924347\n",
      "Iteration 1039 => Weight: 1.240000 => Bias: 10.390000 => Loss: 24.908847\n",
      "Iteration 1040 => Weight: 1.240000 => Bias: 10.400000 => Loss: 24.893547\n",
      "Iteration 1041 => Weight: 1.240000 => Bias: 10.410000 => Loss: 24.878447\n",
      "Iteration 1042 => Weight: 1.240000 => Bias: 10.420000 => Loss: 24.863547\n",
      "Iteration 1043 => Weight: 1.240000 => Bias: 10.430000 => Loss: 24.848847\n",
      "Iteration 1044 => Weight: 1.240000 => Bias: 10.440000 => Loss: 24.834347\n",
      "Iteration 1045 => Weight: 1.240000 => Bias: 10.450000 => Loss: 24.820047\n",
      "Iteration 1046 => Weight: 1.240000 => Bias: 10.460000 => Loss: 24.805947\n",
      "Iteration 1047 => Weight: 1.240000 => Bias: 10.470000 => Loss: 24.792047\n",
      "Iteration 1048 => Weight: 1.240000 => Bias: 10.480000 => Loss: 24.778347\n",
      "Iteration 1049 => Weight: 1.240000 => Bias: 10.490000 => Loss: 24.764847\n",
      "Iteration 1050 => Weight: 1.240000 => Bias: 10.500000 => Loss: 24.751547\n",
      "Iteration 1051 => Weight: 1.240000 => Bias: 10.510000 => Loss: 24.738447\n",
      "Iteration 1052 => Weight: 1.240000 => Bias: 10.520000 => Loss: 24.725547\n",
      "Iteration 1053 => Weight: 1.230000 => Bias: 10.530000 => Loss: 24.708887\n",
      "Iteration 1054 => Weight: 1.230000 => Bias: 10.540000 => Loss: 24.693853\n",
      "Iteration 1055 => Weight: 1.230000 => Bias: 10.550000 => Loss: 24.679020\n",
      "Iteration 1056 => Weight: 1.230000 => Bias: 10.560000 => Loss: 24.664387\n",
      "Iteration 1057 => Weight: 1.230000 => Bias: 10.570000 => Loss: 24.649953\n",
      "Iteration 1058 => Weight: 1.230000 => Bias: 10.580000 => Loss: 24.635720\n",
      "Iteration 1059 => Weight: 1.230000 => Bias: 10.590000 => Loss: 24.621687\n",
      "Iteration 1060 => Weight: 1.230000 => Bias: 10.600000 => Loss: 24.607853\n",
      "Iteration 1061 => Weight: 1.230000 => Bias: 10.610000 => Loss: 24.594220\n",
      "Iteration 1062 => Weight: 1.230000 => Bias: 10.620000 => Loss: 24.580787\n",
      "Iteration 1063 => Weight: 1.230000 => Bias: 10.630000 => Loss: 24.567553\n",
      "Iteration 1064 => Weight: 1.230000 => Bias: 10.640000 => Loss: 24.554520\n",
      "Iteration 1065 => Weight: 1.230000 => Bias: 10.650000 => Loss: 24.541687\n",
      "Iteration 1066 => Weight: 1.230000 => Bias: 10.660000 => Loss: 24.529053\n",
      "Iteration 1067 => Weight: 1.230000 => Bias: 10.670000 => Loss: 24.516620\n",
      "Iteration 1068 => Weight: 1.230000 => Bias: 10.680000 => Loss: 24.504387\n",
      "Iteration 1069 => Weight: 1.230000 => Bias: 10.690000 => Loss: 24.492353\n",
      "Iteration 1070 => Weight: 1.220000 => Bias: 10.700000 => Loss: 24.477253\n",
      "Iteration 1071 => Weight: 1.220000 => Bias: 10.710000 => Loss: 24.463087\n",
      "Iteration 1072 => Weight: 1.220000 => Bias: 10.720000 => Loss: 24.449120\n",
      "Iteration 1073 => Weight: 1.220000 => Bias: 10.730000 => Loss: 24.435353\n",
      "Iteration 1074 => Weight: 1.220000 => Bias: 10.740000 => Loss: 24.421787\n",
      "Iteration 1075 => Weight: 1.220000 => Bias: 10.750000 => Loss: 24.408420\n",
      "Iteration 1076 => Weight: 1.220000 => Bias: 10.760000 => Loss: 24.395253\n",
      "Iteration 1077 => Weight: 1.220000 => Bias: 10.770000 => Loss: 24.382287\n",
      "Iteration 1078 => Weight: 1.220000 => Bias: 10.780000 => Loss: 24.369520\n",
      "Iteration 1079 => Weight: 1.220000 => Bias: 10.790000 => Loss: 24.356953\n",
      "Iteration 1080 => Weight: 1.220000 => Bias: 10.800000 => Loss: 24.344587\n",
      "Iteration 1081 => Weight: 1.220000 => Bias: 10.810000 => Loss: 24.332420\n",
      "Iteration 1082 => Weight: 1.220000 => Bias: 10.820000 => Loss: 24.320453\n",
      "Iteration 1083 => Weight: 1.220000 => Bias: 10.830000 => Loss: 24.308687\n",
      "Iteration 1084 => Weight: 1.220000 => Bias: 10.840000 => Loss: 24.297120\n",
      "Iteration 1085 => Weight: 1.220000 => Bias: 10.850000 => Loss: 24.285753\n",
      "Iteration 1086 => Weight: 1.220000 => Bias: 10.860000 => Loss: 24.274587\n",
      "Iteration 1087 => Weight: 1.210000 => Bias: 10.870000 => Loss: 24.261047\n",
      "Iteration 1088 => Weight: 1.210000 => Bias: 10.880000 => Loss: 24.247747\n",
      "Iteration 1089 => Weight: 1.210000 => Bias: 10.890000 => Loss: 24.234647\n",
      "Iteration 1090 => Weight: 1.210000 => Bias: 10.900000 => Loss: 24.221747\n",
      "Iteration 1091 => Weight: 1.210000 => Bias: 10.910000 => Loss: 24.209047\n",
      "Iteration 1092 => Weight: 1.210000 => Bias: 10.920000 => Loss: 24.196547\n",
      "Iteration 1093 => Weight: 1.210000 => Bias: 10.930000 => Loss: 24.184247\n",
      "Iteration 1094 => Weight: 1.210000 => Bias: 10.940000 => Loss: 24.172147\n",
      "Iteration 1095 => Weight: 1.210000 => Bias: 10.950000 => Loss: 24.160247\n",
      "Iteration 1096 => Weight: 1.210000 => Bias: 10.960000 => Loss: 24.148547\n",
      "Iteration 1097 => Weight: 1.210000 => Bias: 10.970000 => Loss: 24.137047\n",
      "Iteration 1098 => Weight: 1.210000 => Bias: 10.980000 => Loss: 24.125747\n",
      "Iteration 1099 => Weight: 1.210000 => Bias: 10.990000 => Loss: 24.114647\n",
      "Iteration 1100 => Weight: 1.210000 => Bias: 11.000000 => Loss: 24.103747\n",
      "Iteration 1101 => Weight: 1.210000 => Bias: 11.010000 => Loss: 24.093047\n",
      "Iteration 1102 => Weight: 1.210000 => Bias: 11.020000 => Loss: 24.082547\n",
      "Iteration 1103 => Weight: 1.210000 => Bias: 11.030000 => Loss: 24.072247\n",
      "Iteration 1104 => Weight: 1.210000 => Bias: 11.040000 => Loss: 24.062147\n",
      "Iteration 1105 => Weight: 1.200000 => Bias: 11.050000 => Loss: 24.047833\n",
      "Iteration 1106 => Weight: 1.200000 => Bias: 11.060000 => Loss: 24.035600\n",
      "Iteration 1107 => Weight: 1.200000 => Bias: 11.070000 => Loss: 24.023567\n",
      "Iteration 1108 => Weight: 1.200000 => Bias: 11.080000 => Loss: 24.011733\n",
      "Iteration 1109 => Weight: 1.200000 => Bias: 11.090000 => Loss: 24.000100\n",
      "Iteration 1110 => Weight: 1.200000 => Bias: 11.100000 => Loss: 23.988667\n",
      "Iteration 1111 => Weight: 1.200000 => Bias: 11.110000 => Loss: 23.977433\n",
      "Iteration 1112 => Weight: 1.200000 => Bias: 11.120000 => Loss: 23.966400\n",
      "Iteration 1113 => Weight: 1.200000 => Bias: 11.130000 => Loss: 23.955567\n",
      "Iteration 1114 => Weight: 1.200000 => Bias: 11.140000 => Loss: 23.944933\n",
      "Iteration 1115 => Weight: 1.200000 => Bias: 11.150000 => Loss: 23.934500\n",
      "Iteration 1116 => Weight: 1.200000 => Bias: 11.160000 => Loss: 23.924267\n",
      "Iteration 1117 => Weight: 1.200000 => Bias: 11.170000 => Loss: 23.914233\n",
      "Iteration 1118 => Weight: 1.200000 => Bias: 11.180000 => Loss: 23.904400\n",
      "Iteration 1119 => Weight: 1.200000 => Bias: 11.190000 => Loss: 23.894767\n",
      "Iteration 1120 => Weight: 1.200000 => Bias: 11.200000 => Loss: 23.885333\n",
      "Iteration 1121 => Weight: 1.200000 => Bias: 11.210000 => Loss: 23.876100\n",
      "Iteration 1122 => Weight: 1.190000 => Bias: 11.220000 => Loss: 23.863347\n",
      "Iteration 1123 => Weight: 1.190000 => Bias: 11.230000 => Loss: 23.851980\n",
      "Iteration 1124 => Weight: 1.190000 => Bias: 11.240000 => Loss: 23.840813\n",
      "Iteration 1125 => Weight: 1.190000 => Bias: 11.250000 => Loss: 23.829847\n",
      "Iteration 1126 => Weight: 1.190000 => Bias: 11.260000 => Loss: 23.819080\n",
      "Iteration 1127 => Weight: 1.190000 => Bias: 11.270000 => Loss: 23.808513\n",
      "Iteration 1128 => Weight: 1.190000 => Bias: 11.280000 => Loss: 23.798147\n",
      "Iteration 1129 => Weight: 1.190000 => Bias: 11.290000 => Loss: 23.787980\n",
      "Iteration 1130 => Weight: 1.190000 => Bias: 11.300000 => Loss: 23.778013\n",
      "Iteration 1131 => Weight: 1.190000 => Bias: 11.310000 => Loss: 23.768247\n",
      "Iteration 1132 => Weight: 1.190000 => Bias: 11.320000 => Loss: 23.758680\n",
      "Iteration 1133 => Weight: 1.190000 => Bias: 11.330000 => Loss: 23.749313\n",
      "Iteration 1134 => Weight: 1.190000 => Bias: 11.340000 => Loss: 23.740147\n",
      "Iteration 1135 => Weight: 1.190000 => Bias: 11.350000 => Loss: 23.731180\n",
      "Iteration 1136 => Weight: 1.190000 => Bias: 11.360000 => Loss: 23.722413\n",
      "Iteration 1137 => Weight: 1.190000 => Bias: 11.370000 => Loss: 23.713847\n",
      "Iteration 1138 => Weight: 1.190000 => Bias: 11.380000 => Loss: 23.705480\n",
      "Iteration 1139 => Weight: 1.180000 => Bias: 11.390000 => Loss: 23.694287\n",
      "Iteration 1140 => Weight: 1.180000 => Bias: 11.400000 => Loss: 23.683787\n",
      "Iteration 1141 => Weight: 1.180000 => Bias: 11.410000 => Loss: 23.673487\n",
      "Iteration 1142 => Weight: 1.180000 => Bias: 11.420000 => Loss: 23.663387\n",
      "Iteration 1143 => Weight: 1.180000 => Bias: 11.430000 => Loss: 23.653487\n",
      "Iteration 1144 => Weight: 1.180000 => Bias: 11.440000 => Loss: 23.643787\n",
      "Iteration 1145 => Weight: 1.180000 => Bias: 11.450000 => Loss: 23.634287\n",
      "Iteration 1146 => Weight: 1.180000 => Bias: 11.460000 => Loss: 23.624987\n",
      "Iteration 1147 => Weight: 1.180000 => Bias: 11.470000 => Loss: 23.615887\n",
      "Iteration 1148 => Weight: 1.180000 => Bias: 11.480000 => Loss: 23.606987\n",
      "Iteration 1149 => Weight: 1.180000 => Bias: 11.490000 => Loss: 23.598287\n",
      "Iteration 1150 => Weight: 1.180000 => Bias: 11.500000 => Loss: 23.589787\n",
      "Iteration 1151 => Weight: 1.180000 => Bias: 11.510000 => Loss: 23.581487\n",
      "Iteration 1152 => Weight: 1.180000 => Bias: 11.520000 => Loss: 23.573387\n",
      "Iteration 1153 => Weight: 1.180000 => Bias: 11.530000 => Loss: 23.565487\n",
      "Iteration 1154 => Weight: 1.180000 => Bias: 11.540000 => Loss: 23.557787\n",
      "Iteration 1155 => Weight: 1.180000 => Bias: 11.550000 => Loss: 23.550287\n",
      "Iteration 1156 => Weight: 1.180000 => Bias: 11.560000 => Loss: 23.542987\n",
      "Iteration 1157 => Weight: 1.170000 => Bias: 11.570000 => Loss: 23.531020\n",
      "Iteration 1158 => Weight: 1.170000 => Bias: 11.580000 => Loss: 23.521587\n",
      "Iteration 1159 => Weight: 1.170000 => Bias: 11.590000 => Loss: 23.512353\n",
      "Iteration 1160 => Weight: 1.170000 => Bias: 11.600000 => Loss: 23.503320\n",
      "Iteration 1161 => Weight: 1.170000 => Bias: 11.610000 => Loss: 23.494487\n",
      "Iteration 1162 => Weight: 1.170000 => Bias: 11.620000 => Loss: 23.485853\n",
      "Iteration 1163 => Weight: 1.170000 => Bias: 11.630000 => Loss: 23.477420\n",
      "Iteration 1164 => Weight: 1.170000 => Bias: 11.640000 => Loss: 23.469187\n",
      "Iteration 1165 => Weight: 1.170000 => Bias: 11.650000 => Loss: 23.461153\n",
      "Iteration 1166 => Weight: 1.170000 => Bias: 11.660000 => Loss: 23.453320\n",
      "Iteration 1167 => Weight: 1.170000 => Bias: 11.670000 => Loss: 23.445687\n",
      "Iteration 1168 => Weight: 1.170000 => Bias: 11.680000 => Loss: 23.438253\n",
      "Iteration 1169 => Weight: 1.170000 => Bias: 11.690000 => Loss: 23.431020\n",
      "Iteration 1170 => Weight: 1.170000 => Bias: 11.700000 => Loss: 23.423987\n",
      "Iteration 1171 => Weight: 1.170000 => Bias: 11.710000 => Loss: 23.417153\n",
      "Iteration 1172 => Weight: 1.170000 => Bias: 11.720000 => Loss: 23.410520\n",
      "Iteration 1173 => Weight: 1.170000 => Bias: 11.730000 => Loss: 23.404087\n",
      "Iteration 1174 => Weight: 1.160000 => Bias: 11.740000 => Loss: 23.393680\n",
      "Iteration 1175 => Weight: 1.160000 => Bias: 11.750000 => Loss: 23.385113\n",
      "Iteration 1176 => Weight: 1.160000 => Bias: 11.760000 => Loss: 23.376747\n",
      "Iteration 1177 => Weight: 1.160000 => Bias: 11.770000 => Loss: 23.368580\n",
      "Iteration 1178 => Weight: 1.160000 => Bias: 11.780000 => Loss: 23.360613\n",
      "Iteration 1179 => Weight: 1.160000 => Bias: 11.790000 => Loss: 23.352847\n",
      "Iteration 1180 => Weight: 1.160000 => Bias: 11.800000 => Loss: 23.345280\n",
      "Iteration 1181 => Weight: 1.160000 => Bias: 11.810000 => Loss: 23.337913\n",
      "Iteration 1182 => Weight: 1.160000 => Bias: 11.820000 => Loss: 23.330747\n",
      "Iteration 1183 => Weight: 1.160000 => Bias: 11.830000 => Loss: 23.323780\n",
      "Iteration 1184 => Weight: 1.160000 => Bias: 11.840000 => Loss: 23.317013\n",
      "Iteration 1185 => Weight: 1.160000 => Bias: 11.850000 => Loss: 23.310447\n",
      "Iteration 1186 => Weight: 1.160000 => Bias: 11.860000 => Loss: 23.304080\n",
      "Iteration 1187 => Weight: 1.160000 => Bias: 11.870000 => Loss: 23.297913\n",
      "Iteration 1188 => Weight: 1.160000 => Bias: 11.880000 => Loss: 23.291947\n",
      "Iteration 1189 => Weight: 1.160000 => Bias: 11.890000 => Loss: 23.286180\n",
      "Iteration 1190 => Weight: 1.160000 => Bias: 11.900000 => Loss: 23.280613\n",
      "Iteration 1191 => Weight: 1.150000 => Bias: 11.910000 => Loss: 23.271767\n",
      "Iteration 1192 => Weight: 1.150000 => Bias: 11.920000 => Loss: 23.264067\n",
      "Iteration 1193 => Weight: 1.150000 => Bias: 11.930000 => Loss: 23.256567\n",
      "Iteration 1194 => Weight: 1.150000 => Bias: 11.940000 => Loss: 23.249267\n",
      "Iteration 1195 => Weight: 1.150000 => Bias: 11.950000 => Loss: 23.242167\n",
      "Iteration 1196 => Weight: 1.150000 => Bias: 11.960000 => Loss: 23.235267\n",
      "Iteration 1197 => Weight: 1.150000 => Bias: 11.970000 => Loss: 23.228567\n",
      "Iteration 1198 => Weight: 1.150000 => Bias: 11.980000 => Loss: 23.222067\n",
      "Iteration 1199 => Weight: 1.150000 => Bias: 11.990000 => Loss: 23.215767\n",
      "Iteration 1200 => Weight: 1.150000 => Bias: 12.000000 => Loss: 23.209667\n",
      "Iteration 1201 => Weight: 1.150000 => Bias: 12.010000 => Loss: 23.203767\n",
      "Iteration 1202 => Weight: 1.150000 => Bias: 12.020000 => Loss: 23.198067\n",
      "Iteration 1203 => Weight: 1.150000 => Bias: 12.030000 => Loss: 23.192567\n",
      "Iteration 1204 => Weight: 1.150000 => Bias: 12.040000 => Loss: 23.187267\n",
      "Iteration 1205 => Weight: 1.150000 => Bias: 12.050000 => Loss: 23.182167\n",
      "Iteration 1206 => Weight: 1.150000 => Bias: 12.060000 => Loss: 23.177267\n",
      "Iteration 1207 => Weight: 1.150000 => Bias: 12.070000 => Loss: 23.172567\n",
      "Iteration 1208 => Weight: 1.140000 => Bias: 12.080000 => Loss: 23.165280\n",
      "Iteration 1209 => Weight: 1.140000 => Bias: 12.090000 => Loss: 23.158447\n",
      "Iteration 1210 => Weight: 1.140000 => Bias: 12.100000 => Loss: 23.151813\n",
      "Iteration 1211 => Weight: 1.140000 => Bias: 12.110000 => Loss: 23.145380\n",
      "Iteration 1212 => Weight: 1.140000 => Bias: 12.120000 => Loss: 23.139147\n",
      "Iteration 1213 => Weight: 1.140000 => Bias: 12.130000 => Loss: 23.133113\n",
      "Iteration 1214 => Weight: 1.140000 => Bias: 12.140000 => Loss: 23.127280\n",
      "Iteration 1215 => Weight: 1.140000 => Bias: 12.150000 => Loss: 23.121647\n",
      "Iteration 1216 => Weight: 1.140000 => Bias: 12.160000 => Loss: 23.116213\n",
      "Iteration 1217 => Weight: 1.140000 => Bias: 12.170000 => Loss: 23.110980\n",
      "Iteration 1218 => Weight: 1.140000 => Bias: 12.180000 => Loss: 23.105947\n",
      "Iteration 1219 => Weight: 1.140000 => Bias: 12.190000 => Loss: 23.101113\n",
      "Iteration 1220 => Weight: 1.140000 => Bias: 12.200000 => Loss: 23.096480\n",
      "Iteration 1221 => Weight: 1.140000 => Bias: 12.210000 => Loss: 23.092047\n",
      "Iteration 1222 => Weight: 1.140000 => Bias: 12.220000 => Loss: 23.087813\n",
      "Iteration 1223 => Weight: 1.140000 => Bias: 12.230000 => Loss: 23.083780\n",
      "Iteration 1224 => Weight: 1.140000 => Bias: 12.240000 => Loss: 23.079947\n",
      "Iteration 1225 => Weight: 1.140000 => Bias: 12.250000 => Loss: 23.076313\n",
      "Iteration 1226 => Weight: 1.130000 => Bias: 12.260000 => Loss: 23.068253\n",
      "Iteration 1227 => Weight: 1.130000 => Bias: 12.270000 => Loss: 23.062487\n",
      "Iteration 1228 => Weight: 1.130000 => Bias: 12.280000 => Loss: 23.056920\n",
      "Iteration 1229 => Weight: 1.130000 => Bias: 12.290000 => Loss: 23.051553\n",
      "Iteration 1230 => Weight: 1.130000 => Bias: 12.300000 => Loss: 23.046387\n",
      "Iteration 1231 => Weight: 1.130000 => Bias: 12.310000 => Loss: 23.041420\n",
      "Iteration 1232 => Weight: 1.130000 => Bias: 12.320000 => Loss: 23.036653\n",
      "Iteration 1233 => Weight: 1.130000 => Bias: 12.330000 => Loss: 23.032087\n",
      "Iteration 1234 => Weight: 1.130000 => Bias: 12.340000 => Loss: 23.027720\n",
      "Iteration 1235 => Weight: 1.130000 => Bias: 12.350000 => Loss: 23.023553\n",
      "Iteration 1236 => Weight: 1.130000 => Bias: 12.360000 => Loss: 23.019587\n",
      "Iteration 1237 => Weight: 1.130000 => Bias: 12.370000 => Loss: 23.015820\n",
      "Iteration 1238 => Weight: 1.130000 => Bias: 12.380000 => Loss: 23.012253\n",
      "Iteration 1239 => Weight: 1.130000 => Bias: 12.390000 => Loss: 23.008887\n",
      "Iteration 1240 => Weight: 1.130000 => Bias: 12.400000 => Loss: 23.005720\n",
      "Iteration 1241 => Weight: 1.130000 => Bias: 12.410000 => Loss: 23.002753\n",
      "Iteration 1242 => Weight: 1.130000 => Bias: 12.420000 => Loss: 22.999987\n",
      "Iteration 1243 => Weight: 1.120000 => Bias: 12.430000 => Loss: 22.993487\n",
      "Iteration 1244 => Weight: 1.120000 => Bias: 12.440000 => Loss: 22.988587\n",
      "Iteration 1245 => Weight: 1.120000 => Bias: 12.450000 => Loss: 22.983887\n",
      "Iteration 1246 => Weight: 1.120000 => Bias: 12.460000 => Loss: 22.979387\n",
      "Iteration 1247 => Weight: 1.120000 => Bias: 12.470000 => Loss: 22.975087\n",
      "Iteration 1248 => Weight: 1.120000 => Bias: 12.480000 => Loss: 22.970987\n",
      "Iteration 1249 => Weight: 1.120000 => Bias: 12.490000 => Loss: 22.967087\n",
      "Iteration 1250 => Weight: 1.120000 => Bias: 12.500000 => Loss: 22.963387\n",
      "Iteration 1251 => Weight: 1.120000 => Bias: 12.510000 => Loss: 22.959887\n",
      "Iteration 1252 => Weight: 1.120000 => Bias: 12.520000 => Loss: 22.956587\n",
      "Iteration 1253 => Weight: 1.120000 => Bias: 12.530000 => Loss: 22.953487\n",
      "Iteration 1254 => Weight: 1.120000 => Bias: 12.540000 => Loss: 22.950587\n",
      "Iteration 1255 => Weight: 1.120000 => Bias: 12.550000 => Loss: 22.947887\n",
      "Iteration 1256 => Weight: 1.120000 => Bias: 12.560000 => Loss: 22.945387\n",
      "Iteration 1257 => Weight: 1.120000 => Bias: 12.570000 => Loss: 22.943087\n",
      "Iteration 1258 => Weight: 1.120000 => Bias: 12.580000 => Loss: 22.940987\n",
      "Iteration 1259 => Weight: 1.120000 => Bias: 12.590000 => Loss: 22.939087\n",
      "Iteration 1260 => Weight: 1.110000 => Bias: 12.600000 => Loss: 22.934147\n",
      "Iteration 1261 => Weight: 1.110000 => Bias: 12.610000 => Loss: 22.930113\n",
      "Iteration 1262 => Weight: 1.110000 => Bias: 12.620000 => Loss: 22.926280\n",
      "Iteration 1263 => Weight: 1.110000 => Bias: 12.630000 => Loss: 22.922647\n",
      "Iteration 1264 => Weight: 1.110000 => Bias: 12.640000 => Loss: 22.919213\n",
      "Iteration 1265 => Weight: 1.110000 => Bias: 12.650000 => Loss: 22.915980\n",
      "Iteration 1266 => Weight: 1.110000 => Bias: 12.660000 => Loss: 22.912947\n",
      "Iteration 1267 => Weight: 1.110000 => Bias: 12.670000 => Loss: 22.910113\n",
      "Iteration 1268 => Weight: 1.110000 => Bias: 12.680000 => Loss: 22.907480\n",
      "Iteration 1269 => Weight: 1.110000 => Bias: 12.690000 => Loss: 22.905047\n",
      "Iteration 1270 => Weight: 1.110000 => Bias: 12.700000 => Loss: 22.902813\n",
      "Iteration 1271 => Weight: 1.110000 => Bias: 12.710000 => Loss: 22.900780\n",
      "Iteration 1272 => Weight: 1.110000 => Bias: 12.720000 => Loss: 22.898947\n",
      "Iteration 1273 => Weight: 1.110000 => Bias: 12.730000 => Loss: 22.897313\n",
      "Iteration 1274 => Weight: 1.110000 => Bias: 12.740000 => Loss: 22.895880\n",
      "Iteration 1275 => Weight: 1.110000 => Bias: 12.750000 => Loss: 22.894647\n",
      "Iteration 1276 => Weight: 1.110000 => Bias: 12.760000 => Loss: 22.893613\n",
      "Iteration 1277 => Weight: 1.100000 => Bias: 12.770000 => Loss: 22.890233\n",
      "Iteration 1278 => Weight: 1.100000 => Bias: 12.780000 => Loss: 22.887067\n",
      "Iteration 1279 => Weight: 1.100000 => Bias: 12.790000 => Loss: 22.884100\n",
      "Iteration 1280 => Weight: 1.100000 => Bias: 12.800000 => Loss: 22.881333\n",
      "Iteration 1281 => Weight: 1.100000 => Bias: 12.810000 => Loss: 22.878767\n",
      "Iteration 1282 => Weight: 1.100000 => Bias: 12.820000 => Loss: 22.876400\n",
      "Iteration 1283 => Weight: 1.100000 => Bias: 12.830000 => Loss: 22.874233\n",
      "Iteration 1284 => Weight: 1.100000 => Bias: 12.840000 => Loss: 22.872267\n",
      "Iteration 1285 => Weight: 1.100000 => Bias: 12.850000 => Loss: 22.870500\n",
      "Iteration 1286 => Weight: 1.100000 => Bias: 12.860000 => Loss: 22.868933\n",
      "Iteration 1287 => Weight: 1.100000 => Bias: 12.870000 => Loss: 22.867567\n",
      "Iteration 1288 => Weight: 1.100000 => Bias: 12.880000 => Loss: 22.866400\n",
      "Iteration 1289 => Weight: 1.100000 => Bias: 12.890000 => Loss: 22.865433\n",
      "Iteration 1290 => Weight: 1.100000 => Bias: 12.900000 => Loss: 22.864667\n",
      "Iteration 1291 => Weight: 1.100000 => Bias: 12.910000 => Loss: 22.864100\n",
      "Iteration 1292 => Weight: 1.100000 => Bias: 12.920000 => Loss: 22.863733\n",
      "Iteration 1293 => Weight: 1.100000 => Bias: 12.930000 => Loss: 22.863567\n",
      "Results:\n",
      "  Weight: %.6f\n",
      "  Bias: %.6f 1.1000000000000008 12.929999999999769\n"
     ]
    }
   ],
   "source": [
    "# Train the system\n",
    "max_iterations = 10_000\n",
    "learning_rate = 0.01\n",
    "got_weight, got_bias = train(X, Y, max_iterations, learning_rate)\n",
    "print(\"Results:\\n  Weight: %.6f\\n  Bias: %.6f\", got_weight, got_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "199b0564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: x=20 => y=34.93\n"
     ]
    }
   ],
   "source": [
    "# Predict the number of pizzas\n",
    "reservations = 20\n",
    "print(\"Prediction: x=%d => y=%.2f\" % (reservations, predict(20, got_weight, got_bias)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba15b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1172a4d70>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the graph with the learned weight and bias\n",
    "sea.set_theme()\n",
    "plt.axis([0, 50, 0, 50])\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.xlabel(\"Reservations\", fontsize=14)\n",
    "plt.ylabel(\"Pizzas\", fontsize=14)\n",
    "plt.plot(X, Y, \"ro\")\n",
    "plt.plot(X, predict(X, got_weight, got_bias))\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4swes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
